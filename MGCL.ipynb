{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Define the Graph Encoder\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GraphEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Define the Modality Encoder (e.g., for image or text)\n",
        "class ModalityEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ModalityEncoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Contrastive Loss Function\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        batch_size = z1.size(0)\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        # Similarity matrix\n",
        "        sim_matrix = torch.exp(torch.mm(z1, z2.t()) / self.temperature)\n",
        "        pos_sim = torch.exp(torch.sum(z1 * z2, dim=-1) / self.temperature)\n",
        "\n",
        "        loss = -torch.log(pos_sim / (sim_matrix.sum(dim=-1) + 1e-8)).mean()\n",
        "        return loss\n",
        "\n",
        "# Multimodal Graph Contrastive Model\n",
        "class MultimodalGraphCL(nn.Module):\n",
        "    def __init__(self, graph_input_dim, modality_input_dim, hidden_dim, output_dim):\n",
        "        super(MultimodalGraphCL, self).__init__()\n",
        "        self.graph_encoder = GraphEncoder(graph_input_dim, hidden_dim, output_dim)\n",
        "        self.modality_encoder = ModalityEncoder(modality_input_dim, output_dim)\n",
        "        self.contrastive_loss = ContrastiveLoss()\n",
        "\n",
        "    def forward(self, graph_data, modality_data):\n",
        "        # Graph encoding\n",
        "        z1 = self.graph_encoder(graph_data.x, graph_data.edge_index)\n",
        "\n",
        "        # Modality encoding\n",
        "        z2 = self.modality_encoder(modality_data)\n",
        "\n",
        "        # Contrastive loss\n",
        "        loss = self.contrastive_loss(z1, z2)\n",
        "        return loss\n",
        "\n",
        "# Dummy data for demonstration\n",
        "num_nodes = 10\n",
        "graph_input_dim = 5\n",
        "modality_input_dim = 100\n",
        "hidden_dim = 64\n",
        "output_dim = 32\n",
        "\n",
        "# Create dummy graph data\n",
        "x = torch.rand((num_nodes, graph_input_dim)) # Node features\n",
        "edge_index = torch.randint(0, num_nodes, (2, 20)) # Random edges\n",
        "graph_data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Dummy modality data (e.g., 10 samples with modality_input_dim features)\n",
        "modality_data = torch.rand((num_nodes, modality_input_dim))\n",
        "\n",
        "# Initialize and train the model\n",
        "model = MultimodalGraphCL(graph_input_dim, modality_input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training step\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(graph_data, modality_data)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMT4fz4NiBrI",
        "outputId": "dd1340e4-423f-40f2-eb7b-849403bd7c40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3242\n",
            "Epoch 2, Loss: 2.2935\n",
            "Epoch 3, Loss: 2.2677\n",
            "Epoch 4, Loss: 2.2422\n",
            "Epoch 5, Loss: 2.2129\n",
            "Epoch 6, Loss: 2.1772\n",
            "Epoch 7, Loss: 2.1334\n",
            "Epoch 8, Loss: 2.0800\n",
            "Epoch 9, Loss: 2.0178\n",
            "Epoch 10, Loss: 1.9521\n",
            "Epoch 11, Loss: 1.8897\n",
            "Epoch 12, Loss: 1.8338\n",
            "Epoch 13, Loss: 1.7821\n",
            "Epoch 14, Loss: 1.7330\n",
            "Epoch 15, Loss: 1.6863\n",
            "Epoch 16, Loss: 1.6390\n",
            "Epoch 17, Loss: 1.5894\n",
            "Epoch 18, Loss: 1.5390\n",
            "Epoch 19, Loss: 1.4884\n",
            "Epoch 20, Loss: 1.4370\n",
            "Epoch 21, Loss: 1.3854\n",
            "Epoch 22, Loss: 1.3352\n",
            "Epoch 23, Loss: 1.2886\n",
            "Epoch 24, Loss: 1.2479\n",
            "Epoch 25, Loss: 1.2121\n",
            "Epoch 26, Loss: 1.1798\n",
            "Epoch 27, Loss: 1.1494\n",
            "Epoch 28, Loss: 1.1203\n",
            "Epoch 29, Loss: 1.0928\n",
            "Epoch 30, Loss: 1.0703\n",
            "Epoch 31, Loss: 1.0576\n",
            "Epoch 32, Loss: 1.0543\n",
            "Epoch 33, Loss: 1.0548\n",
            "Epoch 34, Loss: 1.0532\n",
            "Epoch 35, Loss: 1.0468\n",
            "Epoch 36, Loss: 1.0365\n",
            "Epoch 37, Loss: 1.0241\n",
            "Epoch 38, Loss: 1.0120\n",
            "Epoch 39, Loss: 1.0018\n",
            "Epoch 40, Loss: 0.9946\n",
            "Epoch 41, Loss: 0.9903\n",
            "Epoch 42, Loss: 0.9886\n",
            "Epoch 43, Loss: 0.9882\n",
            "Epoch 44, Loss: 0.9871\n",
            "Epoch 45, Loss: 0.9842\n",
            "Epoch 46, Loss: 0.9794\n",
            "Epoch 47, Loss: 0.9736\n",
            "Epoch 48, Loss: 0.9677\n",
            "Epoch 49, Loss: 0.9626\n",
            "Epoch 50, Loss: 0.9590\n",
            "Epoch 51, Loss: 0.9564\n",
            "Epoch 52, Loss: 0.9542\n",
            "Epoch 53, Loss: 0.9524\n",
            "Epoch 54, Loss: 0.9501\n",
            "Epoch 55, Loss: 0.9472\n",
            "Epoch 56, Loss: 0.9437\n",
            "Epoch 57, Loss: 0.9404\n",
            "Epoch 58, Loss: 0.9377\n",
            "Epoch 59, Loss: 0.9353\n",
            "Epoch 60, Loss: 0.9329\n",
            "Epoch 61, Loss: 0.9308\n",
            "Epoch 62, Loss: 0.9289\n",
            "Epoch 63, Loss: 0.9271\n",
            "Epoch 64, Loss: 0.9250\n",
            "Epoch 65, Loss: 0.9226\n",
            "Epoch 66, Loss: 0.9200\n",
            "Epoch 67, Loss: 0.9176\n",
            "Epoch 68, Loss: 0.9156\n",
            "Epoch 69, Loss: 0.9139\n",
            "Epoch 70, Loss: 0.9120\n",
            "Epoch 71, Loss: 0.9101\n",
            "Epoch 72, Loss: 0.9081\n",
            "Epoch 73, Loss: 0.9063\n",
            "Epoch 74, Loss: 0.9044\n",
            "Epoch 75, Loss: 0.9026\n",
            "Epoch 76, Loss: 0.9008\n",
            "Epoch 77, Loss: 0.8991\n",
            "Epoch 78, Loss: 0.8974\n",
            "Epoch 79, Loss: 0.8958\n",
            "Epoch 80, Loss: 0.8941\n",
            "Epoch 81, Loss: 0.8925\n",
            "Epoch 82, Loss: 0.8910\n",
            "Epoch 83, Loss: 0.8894\n",
            "Epoch 84, Loss: 0.8879\n",
            "Epoch 85, Loss: 0.8863\n",
            "Epoch 86, Loss: 0.8848\n",
            "Epoch 87, Loss: 0.8833\n",
            "Epoch 88, Loss: 0.8818\n",
            "Epoch 89, Loss: 0.8803\n",
            "Epoch 90, Loss: 0.8788\n",
            "Epoch 91, Loss: 0.8773\n",
            "Epoch 92, Loss: 0.8758\n",
            "Epoch 93, Loss: 0.8743\n",
            "Epoch 94, Loss: 0.8729\n",
            "Epoch 95, Loss: 0.8714\n",
            "Epoch 96, Loss: 0.8699\n",
            "Epoch 97, Loss: 0.8684\n",
            "Epoch 98, Loss: 0.8669\n",
            "Epoch 99, Loss: 0.8653\n",
            "Epoch 100, Loss: 0.8638\n"
          ]
        }
      ]
    }
  ]
}