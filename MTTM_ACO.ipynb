{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ca2872-175d-403d-b71b-86651dadae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is: Facebook\n",
      "train length 1896\n",
      "validate length 475\n",
      "test length 1561\n",
      "training model\n",
      "Epoch [1/20],  Loss: 1.6930, Link Prediction Loss: 0.4285, Type Classification Loss: 1.2645, Train_Acc: 0.8823,  Validate_Acc: 0.9500.\n",
      "Epoch [2/20],  Loss: 1.5515, Link Prediction Loss: 0.3577, Type Classification Loss: 1.1939, Train_Acc: 0.9536,  Validate_Acc: 0.9350.\n",
      "Epoch [3/20],  Loss: 1.5624, Link Prediction Loss: 0.3565, Type Classification Loss: 1.2059, Train_Acc: 0.9552,  Validate_Acc: 0.9542.\n",
      "Epoch [4/20],  Loss: 1.5403, Link Prediction Loss: 0.3442, Type Classification Loss: 1.1961, Train_Acc: 0.9667,  Validate_Acc: 0.9604.\n",
      "Epoch [5/20],  Loss: 1.5343, Link Prediction Loss: 0.3416, Type Classification Loss: 1.1927, Train_Acc: 0.9698,  Validate_Acc: 0.9604.\n",
      "Epoch [6/20],  Loss: 1.5328, Link Prediction Loss: 0.3386, Type Classification Loss: 1.1942, Train_Acc: 0.9740,  Validate_Acc: 0.8860.\n",
      "Epoch [7/20],  Loss: 1.5691, Link Prediction Loss: 0.3766, Type Classification Loss: 1.1925, Train_Acc: 0.9354,  Validate_Acc: 0.9521.\n",
      "Epoch [8/20],  Loss: 1.5429, Link Prediction Loss: 0.3471, Type Classification Loss: 1.1958, Train_Acc: 0.9656,  Validate_Acc: 0.9583.\n",
      "Epoch [9/20],  Loss: 1.5418, Link Prediction Loss: 0.3492, Type Classification Loss: 1.1926, Train_Acc: 0.9625,  Validate_Acc: 0.9563.\n",
      "Epoch [10/20],  Loss: 1.5454, Link Prediction Loss: 0.3528, Type Classification Loss: 1.1926, Train_Acc: 0.9604,  Validate_Acc: 0.9604.\n",
      "Epoch [11/20],  Loss: 1.5430, Link Prediction Loss: 0.3487, Type Classification Loss: 1.1943, Train_Acc: 0.9646,  Validate_Acc: 0.9646.\n",
      "Epoch [12/20],  Loss: 1.5530, Link Prediction Loss: 0.3573, Type Classification Loss: 1.1958, Train_Acc: 0.9557,  Validate_Acc: 0.9437.\n",
      "Epoch [13/20],  Loss: 1.5481, Link Prediction Loss: 0.3525, Type Classification Loss: 1.1957, Train_Acc: 0.9609,  Validate_Acc: 0.9646.\n",
      "Epoch [14/20],  Loss: 1.5402, Link Prediction Loss: 0.3460, Type Classification Loss: 1.1942, Train_Acc: 0.9661,  Validate_Acc: 0.9646.\n",
      "Epoch [15/20],  Loss: 1.5331, Link Prediction Loss: 0.3390, Type Classification Loss: 1.1941, Train_Acc: 0.9729,  Validate_Acc: 0.9604.\n",
      "Epoch [16/20],  Loss: 1.5322, Link Prediction Loss: 0.3396, Type Classification Loss: 1.1927, Train_Acc: 0.9740,  Validate_Acc: 0.9646.\n",
      "Epoch [17/20],  Loss: 1.5334, Link Prediction Loss: 0.3392, Type Classification Loss: 1.1942, Train_Acc: 0.9734,  Validate_Acc: 0.9604.\n",
      "Epoch [18/20],  Loss: 1.5342, Link Prediction Loss: 0.3385, Type Classification Loss: 1.1957, Train_Acc: 0.9745,  Validate_Acc: 0.9513.\n",
      "Epoch [19/20],  Loss: 1.5351, Link Prediction Loss: 0.3392, Type Classification Loss: 1.1959, Train_Acc: 0.9740,  Validate_Acc: 0.9688.\n",
      "Epoch [20/20],  Loss: 1.5332, Link Prediction Loss: 0.3406, Type Classification Loss: 1.1926, Train_Acc: 0.9724,  Validate_Acc: 0.9563.\n",
      "testing model\n",
      "Final reault: AUC -- 0.9872   Precision -- 0.8942   Accuracy -- 0.9577  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uclab1\\AppData\\Local\\Temp\\ipykernel_20532\\3061154237.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_validate_dir))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_and_validate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 362\u001b[0m\n\u001b[0;32m    360\u001b[0m main(predicted_Type, dataset, output_file)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43maco_tune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_hyperparams)\n",
      "Cell \u001b[1;32mIn[4], line 330\u001b[0m, in \u001b[0;36maco_tune_hyperparameters\u001b[1;34m()\u001b[0m\n\u001b[0;32m    327\u001b[0m solutions\u001b[38;5;241m.\u001b[39mappend(solution)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Evaluate the solution by running the training and validation (you can modify this evaluation)\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate_model\u001b[49m(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate_idx, batch_size\u001b[38;5;241m=\u001b[39mbatch_size_idx, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim_idx, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs_idx)\n\u001b[0;32m    331\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Update pheromones based on the score\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_and_validate_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import time, os\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "hidden_dim = 16\n",
    "\n",
    "class edgeFeatures(object):\n",
    "    def __init__(self, label=None, type = None, embeddings = None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        return\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ', skiprows=0)\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ', skiprows=0)\n",
    "\n",
    "    train_Real_Graph = nx.Graph()\n",
    "    train_Fake_Graph = nx.Graph()\n",
    "    test_Real_Graph = nx.Graph()\n",
    "    test_Fake_Graph = nx.Graph()\n",
    "\n",
    "    real_edge_Attritube = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edge_Attritube = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    lenReal = len(real_edge_Attritube)\n",
    "    lenFake = len(fake_edge_Attritube)\n",
    "\n",
    "    # print(real_edge_Attritube)\n",
    "    #new type id according to dataset\n",
    "    if dataset.lower() == 'facebook':\n",
    "        dataNewType = [9, 8, 7, 6, 5, 4]\n",
    "    else:\n",
    "        dataNewType = [2]\n",
    "\n",
    "    for i in range(lenReal):\n",
    "        relation = real_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "\n",
    "    for i in range(lenFake):\n",
    "        relation = fake_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    realFileName = 'Datasets/' + dataset + '/realData.csv'\n",
    "    fakeFileName = 'Datasets/' + dataset + '/fakeData.csv'\n",
    "    train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph = structuralGraph(realFileName, fakeFileName, dataset)\n",
    "    node2vecReFile = \"Datasets/node2vecFeature/\" + dataset + \"Feature.txt\"\n",
    "    data = pd.read_csv(node2vecReFile, sep=' ', skiprows=1, header=None)\n",
    "    edges = np.array(data.iloc[:, 0:1]) + np.array(data.iloc[:, 1:2])\n",
    "    embeddings = np.array(data.iloc[:, 2:66])\n",
    "    nodeL = np.array(data.iloc[:, 0:1])\n",
    "    nodeR = np.array(data.iloc[:, 1:2])\n",
    "    train_data = []\n",
    "    test = []\n",
    "    for i in range(len(edges)):\n",
    "        edgeFeature = edgeFeatures(\" \")\n",
    "        nodel = int(re.sub(\"\\D\", \"\", nodeL[i][0]))\n",
    "        noder = int(re.sub(\"\\D\", \"\", nodeR[i][0]))\n",
    "        if train_Real_Graph.has_edge(nodel, noder) or train_Fake_Graph.has_edge(nodel, noder): # train set\n",
    "            if train_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = train_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = train_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_Real_Graph.has_edge(nodel, noder) or test_Fake_Graph.has_edge(nodel, noder):  # test set\n",
    "            if test_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = test_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = test_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            test.append(edgeFeature)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)  # train_test_split返回切分的数据集train/validate\n",
    "    train_dataset = []\n",
    "    validate_dataset = []\n",
    "    test_dataset = []\n",
    "    for index, element in enumerate(train):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        train_dataset.append(m)\n",
    "    for index, element in enumerate(validate):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        validate_dataset.append(m)\n",
    "    for index, element in enumerate(test):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        test_dataset.append(m)\n",
    "    print('train length', len(train_dataset))\n",
    "    print('validate length', len(validate_dataset))\n",
    "    print('test length', len(test_dataset))\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(dataset=validate_dataset, batch_size=batch_size,  shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validate_loader, test_loader\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "class re_shape(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x.reshape(len(x),len(x[0][0])))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output =  grad_output.reshape(len(grad_output),1,len(grad_output[0]))\n",
    "        return output,None\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @ staticmethod\n",
    "    def forward(ctx, x, lambd, **kwargs: None):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_output):\n",
    "        return grad_output[0] * -ctx.lambd, None\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "class adversarial_neural_networks(nn.Module):\n",
    "    def __init__(self, predicted_Type):\n",
    "        super(adversarial_neural_networks, self).__init__()\n",
    "        self.predicted_Type = predicted_Type\n",
    "\n",
    "        ##The generative predictor\n",
    "        self.predictor = nn.Sequential()\n",
    "        self.predictor.add_module('exta_Conv1',nn.Conv1d(in_channels=1, out_channels=1, kernel_size=10, stride=1, padding=0))\n",
    "        self.predictor.add_module('fully_connected_layer1', nn.Linear(55, 32))\n",
    "\n",
    "        self.predictor_classifier = nn.Sequential()\n",
    "        self.predictor_classifier.add_module('c_fc1', nn.Linear(32,24))\n",
    "        self.predictor_classifier.add_module('c_fc1_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc2', nn.Linear(24, 16))\n",
    "        self.predictor_classifier.add_module('c_fc2_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc3', nn.Linear(16, 2))\n",
    "        self.predictor_classifier.add_module('c_softmax', nn.Softmax(dim=1))  # 对每一行进行softmax\n",
    "\n",
    "        #discriminative classifier learn shared feature\n",
    "        self.discriminative_classifier = nn.Sequential()\n",
    "        self.discriminative_classifier.add_module('d_fc1', nn.Linear(32, 16))\n",
    "        self.discriminative_classifier.add_module('relu_f1', nn.ReLU())\n",
    "        self.discriminative_classifier.add_module('d_fc2', nn.Linear(16, self.predicted_Type))\n",
    "        self.discriminative_classifier.add_module('d_softmax',nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.predictor(embeddings)\n",
    "        shared_embeddings = re_shape.apply(embeddings)\n",
    "        link_output = self.predictor_classifier(shared_embeddings)\n",
    "        reverse_embeddings = GradReverse.apply(shared_embeddings, 1.0)\n",
    "        type_output = self.discriminative_classifier(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def train_adversarial_neural_networks(train_loader, validate_loader, model, output_file):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=learning_rate)\n",
    "    best_validate_acc = 0.000\n",
    "    best_validate_dir = ''\n",
    "\n",
    "    print('training model')\n",
    "    # Train of Model\n",
    "    for epoch in range(num_epochs):  # num_epochs is 50\n",
    "        p = float(epoch) / 100\n",
    "        lr = learning_rate / (1. + 10 * p) ** 0.75\n",
    "        optimizer.lr = lr\n",
    "        cost_vector = []\n",
    "        prediction_cost_vector = []\n",
    "        classification_cost_vector = []\n",
    "        acc_vector = []\n",
    "        valid_acc_vector = []\n",
    "        vali_cost_vector = []\n",
    "        train_score = []\n",
    "        train_label = []\n",
    "        for i, (train_data, train_labels, type_labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_data = to_var(train_data)\n",
    "            train_labels = to_var(train_labels),\n",
    "            type_labels = to_var(type_labels)\n",
    "            link_outputs, type_outputs = model(train_data.unsqueeze(1))\n",
    "            train_score += list(link_outputs[:, 1].cpu().detach().numpy())\n",
    "            train_label += list(train_labels[0].numpy())\n",
    "            train_labels = train_labels[0]\n",
    "            train_labels = train_labels.long()\n",
    "            type_labels = type_labels.long()\n",
    "            prediction_loss = criterion(link_outputs, train_labels)\n",
    "            classification_loss = criterion(type_outputs, type_labels)\n",
    "            loss = prediction_loss + classification_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(link_outputs, 1)\n",
    "            accuracy = (train_labels == argmax.squeeze()).float().mean()\n",
    "            prediction_cost_vector.append(prediction_loss.item())\n",
    "            classification_cost_vector.append(classification_loss.item())\n",
    "            cost_vector.append(loss.item())\n",
    "            acc_vector.append(accuracy.item())\n",
    "\n",
    "        # validate process\n",
    "        model.eval()\n",
    "        validate_acc_vector_temp = []\n",
    "        for i, (validate_data, validate_labels, type_labels) in enumerate(validate_loader):\n",
    "            validate_data = to_var(validate_data)\n",
    "            validate_labels = to_var(validate_labels)\n",
    "            type_labels = to_var(type_labels)\n",
    "            validate_outputs, type_outputs = model(validate_data.unsqueeze(1))\n",
    "            _, validate_argmax = torch.max(validate_outputs, 1)\n",
    "            validate_labels = validate_labels.long()\n",
    "            vali_loss = criterion(validate_outputs, validate_labels)\n",
    "            validate_accuracy = (validate_labels == validate_argmax.squeeze()).float().mean()\n",
    "            vali_cost_vector.append(vali_loss.item())\n",
    "            validate_acc_vector_temp.append(validate_accuracy.item())\n",
    "        validate_acc = np.mean(validate_acc_vector_temp)\n",
    "        valid_acc_vector.append(validate_acc)\n",
    "        model.train()\n",
    "        print('Epoch [%d/%d],  Loss: %.4f, Link Prediction Loss: %.4f, Type Classification Loss: %.4f, Train_Acc: %.4f,  Validate_Acc: %.4f.'\n",
    "              % (epoch + 1, num_epochs, np.mean(cost_vector), np.mean(prediction_cost_vector), np.mean(classification_cost_vector),\n",
    "                 np.mean(acc_vector), validate_acc))\n",
    "\n",
    "        if validate_acc > best_validate_acc:\n",
    "            best_validate_acc = validate_acc\n",
    "            if not os.path.exists(output_file):\n",
    "                os.mkdir(output_file)\n",
    "            best_validate_dir = output_file + str(epoch + 1) + '.pkl'\n",
    "            torch.save(model.state_dict(), best_validate_dir)\n",
    "    return best_validate_dir\n",
    "\n",
    "def test_adversarial_neural_networks(best_validate_dir, test_loader, model):\n",
    "    # Test the Model\n",
    "    print('testing model')\n",
    "    model.load_state_dict(torch.load(best_validate_dir))\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    test_score = []\n",
    "    test_pred = []\n",
    "    test_true = []\n",
    "    tes_score = []\n",
    "    tes_label = []\n",
    "\n",
    "    for i, (test_data, test_labels, type_labels) in enumerate(test_loader):\n",
    "        test_data = to_var(test_data)\n",
    "        test_labels = to_var(test_labels)\n",
    "        # type_labels = to_var(type_labels)\n",
    "        test_outputs, type_outputs = model(test_data.unsqueeze(1))\n",
    "        tes_score += list(test_outputs[:, 1].cpu().detach().numpy())\n",
    "        tes_label += list(test_labels.numpy())\n",
    "        _, test_argmax = torch.max(test_outputs, 1)\n",
    "        if i == 0:\n",
    "            test_score = to_np(test_outputs)\n",
    "            test_pred = to_np(test_argmax)\n",
    "            test_true = to_np(test_labels)\n",
    "        else:\n",
    "            test_score = np.concatenate((test_score, to_np(test_outputs)), axis=0)\n",
    "            test_pred = np.concatenate((test_pred, to_np(test_argmax)), axis=0)\n",
    "            test_true = np.concatenate((test_true, to_np(test_labels)), axis=0)\n",
    "\n",
    "    test_accuracy = metrics.accuracy_score(test_true, test_pred)\n",
    "    test_precision = metrics.precision_score(test_true, test_pred, average='macro')\n",
    "    test_aucroc = metrics.roc_auc_score(tes_label, tes_score, average='macro')\n",
    "\n",
    "    return test_aucroc, test_precision, test_accuracy\n",
    "\n",
    "# Ant Colony Optimization algorithm for hyperparameter tuning\n",
    "def aco_tune_hyperparameters():\n",
    "    # Initialize pheromone matrix (for each hyperparameter)\n",
    "    pheromone_matrix = np.ones((4, 4))  # 4 hyperparameters to tune: learning rate, batch size, hidden dimension, num_epochs\n",
    "    best_score = float('-inf')\n",
    "    best_params = None\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        solutions = []\n",
    "        scores = []\n",
    "\n",
    "        for ant in range(num_ants):\n",
    "            # Construct a solution by selecting hyperparameters\n",
    "            learning_rate_idx = np.random.choice(range(4))  # Random selection for demonstration\n",
    "            batch_size_idx = np.random.choice(range(4))  \n",
    "            hidden_dim_idx = np.random.choice(range(4))\n",
    "            num_epochs_idx = np.random.choice(range(4))\n",
    "\n",
    "            solution = [learning_rate_idx, batch_size_idx, hidden_dim_idx, num_epochs_idx]\n",
    "            solutions.append(solution)\n",
    "\n",
    "            # Evaluate the solution by running the training and validation (you can modify this evaluation)\n",
    "            score = train_and_validate_model(learning_rate=learning_rate_idx, batch_size=batch_size_idx, hidden_dim=hidden_dim_idx, num_epochs=num_epochs_idx)\n",
    "            scores.append(score)\n",
    "\n",
    "            # Update pheromones based on the score\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = solution\n",
    "\n",
    "        # Update pheromones\n",
    "        pheromone_matrix = (1 - rho) * pheromone_matrix + (Q / np.array(scores))\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "\n",
    "def main(predicted_Type, dataset, output_file):\n",
    "    train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "    model = adversarial_neural_networks(predicted_Type)\n",
    "    best_validate_dir = train_adversarial_neural_networks(train_loader, validate_loader, model, output_file)\n",
    "    auc, precision, accuracy = test_adversarial_neural_networks(best_validate_dir, test_loader, model)\n",
    "    print(\"Final reault: AUC -- %.4f  \" % (auc), \"Precision -- %.4f  \" % (precision), 'Accuracy -- %.4f  ' % (accuracy))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    datasets = ['Facebook', 'IMDB', 'YELP', 'DBLP']\n",
    "    dataset = datasets[0]\n",
    "    print('Input dataset is:', dataset)\n",
    "    predicted_Type_datasets = {'Facebook': 4, 'IMDB': 2, 'YELP': 2, 'DBLP': 2}\n",
    "    predicted_Type = predicted_Type_datasets[dataset]\n",
    "    output_file = 'trainOutput/' + dataset.lower() + '/output.txt'\n",
    "    main(predicted_Type, dataset, output_file)\n",
    "    # Example usage:\n",
    "    best_hyperparams = aco_tune_hyperparameters()\n",
    "    print(\"Best hyperparameters found:\", best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e357a3e-5196-432d-8034-ced464328d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
