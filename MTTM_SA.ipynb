{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd2abe-9787-4c1a-a832-dc8d497efbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import time, os\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 16\n",
    "\n",
    "class edgeFeatures(object):\n",
    "    def __init__(self, label=None, type = None, embeddings = None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        return\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ', skiprows=0)\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ', skiprows=0)\n",
    "\n",
    "    train_Real_Graph = nx.Graph()\n",
    "    train_Fake_Graph = nx.Graph()\n",
    "    test_Real_Graph = nx.Graph()\n",
    "    test_Fake_Graph = nx.Graph()\n",
    "\n",
    "    real_edge_Attritube = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edge_Attritube = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    lenReal = len(real_edge_Attritube)\n",
    "    lenFake = len(fake_edge_Attritube)\n",
    "\n",
    "    # print(real_edge_Attritube)\n",
    "    #new type id according to dataset\n",
    "    if dataset.lower() == 'facebook':\n",
    "        dataNewType = [9, 8, 7, 6, 5, 4]\n",
    "    else:\n",
    "        dataNewType = [2]\n",
    "\n",
    "    for i in range(lenReal):\n",
    "        relation = real_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "\n",
    "    for i in range(lenFake):\n",
    "        relation = fake_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    realFileName = 'Datasets/' + dataset + '/realData.csv'\n",
    "    fakeFileName = 'Datasets/' + dataset + '/fakeData.csv'\n",
    "    train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph = structuralGraph(realFileName, fakeFileName, dataset)\n",
    "    node2vecReFile = \"Datasets/node2vecFeature/\" + dataset + \"Feature.txt\"\n",
    "    data = pd.read_csv(node2vecReFile, sep=' ', skiprows=1, header=None)\n",
    "    edges = np.array(data.iloc[:, 0:1]) + np.array(data.iloc[:, 1:2])\n",
    "    embeddings = np.array(data.iloc[:, 2:66])\n",
    "    nodeL = np.array(data.iloc[:, 0:1])\n",
    "    nodeR = np.array(data.iloc[:, 1:2])\n",
    "    train_data = []\n",
    "    test = []\n",
    "    for i in range(len(edges)):\n",
    "        edgeFeature = edgeFeatures(\" \")\n",
    "        nodel = int(re.sub(\"\\D\", \"\", nodeL[i][0]))\n",
    "        noder = int(re.sub(\"\\D\", \"\", nodeR[i][0]))\n",
    "        if train_Real_Graph.has_edge(nodel, noder) or train_Fake_Graph.has_edge(nodel, noder): # train set\n",
    "            if train_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = train_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = train_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_Real_Graph.has_edge(nodel, noder) or test_Fake_Graph.has_edge(nodel, noder):  # test set\n",
    "            if test_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = test_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = test_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            test.append(edgeFeature)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)  # train_test_split返回切分的数据集train/validate\n",
    "    train_dataset = []\n",
    "    validate_dataset = []\n",
    "    test_dataset = []\n",
    "    for index, element in enumerate(train):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        train_dataset.append(m)\n",
    "    for index, element in enumerate(validate):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        validate_dataset.append(m)\n",
    "    for index, element in enumerate(test):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        test_dataset.append(m)\n",
    "    print('train length', len(train_dataset))\n",
    "    print('validate length', len(validate_dataset))\n",
    "    print('test length', len(test_dataset))\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(dataset=validate_dataset, batch_size=batch_size,  shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validate_loader, test_loader\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "class re_shape(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x.reshape(len(x),len(x[0][0])))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output =  grad_output.reshape(len(grad_output),1,len(grad_output[0]))\n",
    "        return output,None\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @ staticmethod\n",
    "    def forward(ctx, x, lambd, **kwargs: None):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_output):\n",
    "        return grad_output[0] * -ctx.lambd, None\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "class adversarial_neural_networks(nn.Module):\n",
    "    def __init__(self, predicted_Type):\n",
    "        super(adversarial_neural_networks, self).__init__()\n",
    "        self.predicted_Type = predicted_Type\n",
    "\n",
    "        ##The generative predictor\n",
    "        self.predictor = nn.Sequential()\n",
    "        self.predictor.add_module('exta_Conv1',nn.Conv1d(in_channels=1, out_channels=1, kernel_size=10, stride=1, padding=0))\n",
    "        self.predictor.add_module('fully_connected_layer1', nn.Linear(55, 32))\n",
    "\n",
    "        self.predictor_classifier = nn.Sequential()\n",
    "        self.predictor_classifier.add_module('c_fc1', nn.Linear(32,24))\n",
    "        self.predictor_classifier.add_module('c_fc1_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc2', nn.Linear(24, 16))\n",
    "        self.predictor_classifier.add_module('c_fc2_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc3', nn.Linear(16, 2))\n",
    "        self.predictor_classifier.add_module('c_softmax', nn.Softmax(dim=1))  # 对每一行进行softmax\n",
    "\n",
    "        #discriminative classifier learn shared feature\n",
    "        self.discriminative_classifier = nn.Sequential()\n",
    "        self.discriminative_classifier.add_module('d_fc1', nn.Linear(32, 16))\n",
    "        self.discriminative_classifier.add_module('relu_f1', nn.ReLU())\n",
    "        self.discriminative_classifier.add_module('d_fc2', nn.Linear(16, self.predicted_Type))\n",
    "        self.discriminative_classifier.add_module('d_softmax',nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.predictor(embeddings)\n",
    "        shared_embeddings = re_shape.apply(embeddings)\n",
    "        link_output = self.predictor_classifier(shared_embeddings)\n",
    "        reverse_embeddings = GradReverse.apply(shared_embeddings, 1.0)\n",
    "        type_output = self.discriminative_classifier(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def train_adversarial_neural_networks(train_loader, validate_loader, model, output_file):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=learning_rate)\n",
    "    best_validate_acc = 0.000\n",
    "    best_validate_dir = ''\n",
    "\n",
    "    print('training model')\n",
    "    # Train of Model\n",
    "    for epoch in range(num_epochs):  # num_epochs is 50\n",
    "        p = float(epoch) / 100\n",
    "        lr = learning_rate / (1. + 10 * p) ** 0.75\n",
    "        optimizer.lr = lr\n",
    "        cost_vector = []\n",
    "        prediction_cost_vector = []\n",
    "        classification_cost_vector = []\n",
    "        acc_vector = []\n",
    "        valid_acc_vector = []\n",
    "        vali_cost_vector = []\n",
    "        train_score = []\n",
    "        train_label = []\n",
    "        for i, (train_data, train_labels, type_labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_data = to_var(train_data)\n",
    "            train_labels = to_var(train_labels),\n",
    "            type_labels = to_var(type_labels)\n",
    "            link_outputs, type_outputs = model(train_data.unsqueeze(1))\n",
    "            train_score += list(link_outputs[:, 1].cpu().detach().numpy())\n",
    "            train_label += list(train_labels[0].numpy())\n",
    "            train_labels = train_labels[0]\n",
    "            train_labels = train_labels.long()\n",
    "            type_labels = type_labels.long()\n",
    "            prediction_loss = criterion(link_outputs, train_labels)\n",
    "            classification_loss = criterion(type_outputs, type_labels)\n",
    "            loss = prediction_loss + classification_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(link_outputs, 1)\n",
    "            accuracy = (train_labels == argmax.squeeze()).float().mean()\n",
    "            prediction_cost_vector.append(prediction_loss.item())\n",
    "            classification_cost_vector.append(classification_loss.item())\n",
    "            cost_vector.append(loss.item())\n",
    "            acc_vector.append(accuracy.item())\n",
    "\n",
    "        # validate process\n",
    "        model.eval()\n",
    "        validate_acc_vector_temp = []\n",
    "        for i, (validate_data, validate_labels, type_labels) in enumerate(validate_loader):\n",
    "            validate_data = to_var(validate_data)\n",
    "            validate_labels = to_var(validate_labels)\n",
    "            type_labels = to_var(type_labels)\n",
    "            validate_outputs, type_outputs = model(validate_data.unsqueeze(1))\n",
    "            _, validate_argmax = torch.max(validate_outputs, 1)\n",
    "            validate_labels = validate_labels.long()\n",
    "            vali_loss = criterion(validate_outputs, validate_labels)\n",
    "            validate_accuracy = (validate_labels == validate_argmax.squeeze()).float().mean()\n",
    "            vali_cost_vector.append(vali_loss.item())\n",
    "            validate_acc_vector_temp.append(validate_accuracy.item())\n",
    "        validate_acc = np.mean(validate_acc_vector_temp)\n",
    "        valid_acc_vector.append(validate_acc)\n",
    "        model.train()\n",
    "        print('Epoch [%d/%d],  Loss: %.4f, Link Prediction Loss: %.4f, Type Classification Loss: %.4f, Train_Acc: %.4f,  Validate_Acc: %.4f.'\n",
    "              % (epoch + 1, num_epochs, np.mean(cost_vector), np.mean(prediction_cost_vector), np.mean(classification_cost_vector),\n",
    "                 np.mean(acc_vector), validate_acc))\n",
    "\n",
    "        if validate_acc > best_validate_acc:\n",
    "            best_validate_acc = validate_acc\n",
    "            if not os.path.exists(output_file):\n",
    "                os.mkdir(output_file)\n",
    "            best_validate_dir = output_file + str(epoch + 1) + '.pkl'\n",
    "            torch.save(model.state_dict(), best_validate_dir)\n",
    "    return best_validate_dir\n",
    "\n",
    "def test_adversarial_neural_networks(best_validate_dir, test_loader, model):\n",
    "    # Test the Model\n",
    "    print('testing model')\n",
    "    model.load_state_dict(torch.load(best_validate_dir))\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    test_score = []\n",
    "    test_pred = []\n",
    "    test_true = []\n",
    "    tes_score = []\n",
    "    tes_label = []\n",
    "\n",
    "    for i, (test_data, test_labels, type_labels) in enumerate(test_loader):\n",
    "        test_data = to_var(test_data)\n",
    "        test_labels = to_var(test_labels)\n",
    "        # type_labels = to_var(type_labels)\n",
    "        test_outputs, type_outputs = model(test_data.unsqueeze(1))\n",
    "        tes_score += list(test_outputs[:, 1].cpu().detach().numpy())\n",
    "        tes_label += list(test_labels.numpy())\n",
    "        _, test_argmax = torch.max(test_outputs, 1)\n",
    "        if i == 0:\n",
    "            test_score = to_np(test_outputs)\n",
    "            test_pred = to_np(test_argmax)\n",
    "            test_true = to_np(test_labels)\n",
    "        else:\n",
    "            test_score = np.concatenate((test_score, to_np(test_outputs)), axis=0)\n",
    "            test_pred = np.concatenate((test_pred, to_np(test_argmax)), axis=0)\n",
    "            test_true = np.concatenate((test_true, to_np(test_labels)), axis=0)\n",
    "\n",
    "    test_accuracy = metrics.accuracy_score(test_true, test_pred)\n",
    "    test_precision = metrics.precision_score(test_true, test_pred, average='macro')\n",
    "    test_aucroc = metrics.roc_auc_score(tes_label, tes_score, average='macro')\n",
    "\n",
    "    return test_aucroc, test_precision, test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simulated_annealing_optimization(initial_params, max_iterations, dataset, output_file):\n",
    "    # Simulated Annealing Parameters\n",
    "    initial_temperature = 100.0\n",
    "    cooling_rate = 0.95\n",
    "    max_no_improve = 50\n",
    "    \n",
    "    def objective_function(params):\n",
    "        # Unpack parameters\n",
    "        num_epochs, batch_size, learning_rate, hidden_dim = params\n",
    "        \n",
    "        # Run the model training and validation with current parameters\n",
    "        model = adversarial_neural_networks(predicted_Type)\n",
    "        train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "        best_validate_dir = train_adversarial_neural_networks(train_loader, validate_loader, model, output_file)\n",
    "        auc, precision, accuracy = test_adversarial_neural_networks(best_validate_dir, test_loader, model)\n",
    "        \n",
    "        # Return the negative of accuracy as a \"cost\" to minimize\n",
    "        return -accuracy\n",
    "\n",
    "    def tweak_parameters(params):\n",
    "        num_epochs, batch_size, learning_rate, hidden_dim = params\n",
    "        new_num_epochs = max(1, num_epochs + random.randint(-5, 5))\n",
    "        new_batch_size = max(8, batch_size + random.randint(-8, 8))\n",
    "        new_learning_rate = max(1e-5, learning_rate + (random.random() - 0.5) * 0.0005)\n",
    "        new_hidden_dim = max(4, hidden_dim + random.randint(-4, 4))\n",
    "        return (new_num_epochs, new_batch_size, new_learning_rate, new_hidden_dim)\n",
    "\n",
    "    # Initial parameters\n",
    "    current_params = initial_params\n",
    "    current_cost = objective_function(current_params)\n",
    "    best_params = current_params\n",
    "    best_cost = current_cost\n",
    "    temperature = initial_temperature\n",
    "    no_improve_counter = 0\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        new_params = tweak_parameters(current_params)\n",
    "        new_cost = objective_function(new_params)\n",
    "        \n",
    "        # Acceptance probability\n",
    "        if new_cost < current_cost or random.random() < math.exp((current_cost - new_cost) / temperature):\n",
    "            current_params = new_params\n",
    "            current_cost = new_cost\n",
    "            if new_cost < best_cost:\n",
    "                best_params = new_params\n",
    "                best_cost = new_cost\n",
    "                no_improve_counter = 0\n",
    "            else:\n",
    "                no_improve_counter += 1\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "\n",
    "        # Cool down the temperature\n",
    "        temperature *= cooling_rate\n",
    "        \n",
    "        print(f\"Iteration {iteration + 1}: Cost={-current_cost:.4f}, Params={current_params}, Best Cost={-best_cost:.4f}\")\n",
    "\n",
    "        # Early stopping if no improvement over a number of iterations\n",
    "        if no_improve_counter >= max_no_improve:\n",
    "            break\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def main(predicted_Type, dataset, output_file):\n",
    "    train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "    model = adversarial_neural_networks(predicted_Type)\n",
    "    best_validate_dir = train_adversarial_neural_networks(train_loader, validate_loader, model, output_file)\n",
    "    auc, precision, accuracy = test_adversarial_neural_networks(best_validate_dir, test_loader, model)\n",
    "    print(\"Final reault: AUC -- %.4f  \" % (auc), \"Precision -- %.4f  \" % (precision), 'Accuracy -- %.4f  ' % (accuracy))\n",
    "\n",
    "# Main execution with simulated annealing\n",
    "if __name__ == '__main__':\n",
    "    datasets = ['Facebook', 'IMDB', 'YELP', 'DBLP']\n",
    "    dataset = datasets[0]\n",
    "    print('Input dataset is:', dataset)\n",
    "    predicted_Type_datasets = {'Facebook': 4, 'IMDB': 2, 'YELP': 2, 'DBLP': 2}\n",
    "    predicted_Type = predicted_Type_datasets[dataset]\n",
    "    output_file = 'trainOutput/' + dataset.lower() + '/output.txt'\n",
    "\n",
    "    # Initial parameters (num_epochs, batch_size, learning_rate, hidden_dim)\n",
    "    initial_params = (200, 32, 0.001, 16)\n",
    "    max_iterations = 100\n",
    "\n",
    "    # Optimize hyperparameters using simulated annealing\n",
    "    best_params = simulated_annealing_optimization(initial_params, max_iterations, dataset, output_file)\n",
    "    print(\"Optimal Parameters Found:\", best_params)\n",
    "\n",
    "    # Train the model with the optimal parameters\n",
    "    num_epochs, batch_size, learning_rate, hidden_dim = best_params\n",
    "    main(predicted_Type, dataset, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e74a656-1316-4f0a-9344-cc803ce6d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\n",
      "ERROR: Could not find a version that satisfies the requirement torchvision (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for torchvision\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ea17cb-7530-408e-a77d-854eaa43d7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17fb38-ab0c-4020-9376-bcac3fa6b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import time, os\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 16\n",
    "\n",
    "class edgeFeatures(object):\n",
    "    def __init__(self, label=None, type = None, embeddings = None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        return\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ', skiprows=0)\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ', skiprows=0)\n",
    "\n",
    "    train_Real_Graph = nx.Graph()\n",
    "    train_Fake_Graph = nx.Graph()\n",
    "    test_Real_Graph = nx.Graph()\n",
    "    test_Fake_Graph = nx.Graph()\n",
    "\n",
    "    real_edge_Attritube = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edge_Attritube = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    lenReal = len(real_edge_Attritube)\n",
    "    lenFake = len(fake_edge_Attritube)\n",
    "\n",
    "    # print(real_edge_Attritube)\n",
    "    #new type id according to dataset\n",
    "    if dataset.lower() == 'facebook':\n",
    "        dataNewType = [9, 8, 7, 6, 5, 4]\n",
    "    else:\n",
    "        dataNewType = [2]\n",
    "\n",
    "    for i in range(lenReal):\n",
    "        relation = real_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "\n",
    "    for i in range(lenFake):\n",
    "        relation = fake_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    realFileName = 'Datasets/' + dataset + '/realData.csv'\n",
    "    fakeFileName = 'Datasets/' + dataset + '/fakeData.csv'\n",
    "    train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph = structuralGraph(realFileName, fakeFileName, dataset)\n",
    "    node2vecReFile = \"Datasets/node2vecFeature/\" + dataset + \"Feature.txt\"\n",
    "    data = pd.read_csv(node2vecReFile, sep=' ', skiprows=1, header=None)\n",
    "    edges = np.array(data.iloc[:, 0:1]) + np.array(data.iloc[:, 1:2])\n",
    "    embeddings = np.array(data.iloc[:, 2:66])\n",
    "    nodeL = np.array(data.iloc[:, 0:1])\n",
    "    nodeR = np.array(data.iloc[:, 1:2])\n",
    "    train_data = []\n",
    "    test = []\n",
    "    for i in range(len(edges)):\n",
    "        edgeFeature = edgeFeatures(\" \")\n",
    "        nodel = int(re.sub(\"\\D\", \"\", nodeL[i][0]))\n",
    "        noder = int(re.sub(\"\\D\", \"\", nodeR[i][0]))\n",
    "        if train_Real_Graph.has_edge(nodel, noder) or train_Fake_Graph.has_edge(nodel, noder): # train set\n",
    "            if train_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = train_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = train_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_Real_Graph.has_edge(nodel, noder) or test_Fake_Graph.has_edge(nodel, noder):  # test set\n",
    "            if test_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = test_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = test_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            test.append(edgeFeature)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)  # train_test_split返回切分的数据集train/validate\n",
    "    train_dataset = []\n",
    "    validate_dataset = []\n",
    "    test_dataset = []\n",
    "    for index, element in enumerate(train):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        train_dataset.append(m)\n",
    "    for index, element in enumerate(validate):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        validate_dataset.append(m)\n",
    "    for index, element in enumerate(test):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        test_dataset.append(m)\n",
    "    print('train length', len(train_dataset))\n",
    "    print('validate length', len(validate_dataset))\n",
    "    print('test length', len(test_dataset))\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(dataset=validate_dataset, batch_size=batch_size,  shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validate_loader, test_loader\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "class re_shape(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x.reshape(len(x),len(x[0][0])))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output =  grad_output.reshape(len(grad_output),1,len(grad_output[0]))\n",
    "        return output,None\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @ staticmethod\n",
    "    def forward(ctx, x, lambd, **kwargs: None):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_output):\n",
    "        return grad_output[0] * -ctx.lambd, None\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "class adversarial_neural_networks(nn.Module):\n",
    "    def __init__(self, predicted_Type):\n",
    "        super(adversarial_neural_networks, self).__init__()\n",
    "        self.predicted_Type = predicted_Type\n",
    "\n",
    "        ##The generative predictor\n",
    "        self.predictor = nn.Sequential()\n",
    "        self.predictor.add_module('exta_Conv1',nn.Conv1d(in_channels=1, out_channels=1, kernel_size=10, stride=1, padding=0))\n",
    "        self.predictor.add_module('fully_connected_layer1', nn.Linear(55, 32))\n",
    "\n",
    "        self.predictor_classifier = nn.Sequential()\n",
    "        self.predictor_classifier.add_module('c_fc1', nn.Linear(32,24))\n",
    "        self.predictor_classifier.add_module('c_fc1_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc2', nn.Linear(24, 16))\n",
    "        self.predictor_classifier.add_module('c_fc2_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc3', nn.Linear(16, 2))\n",
    "        self.predictor_classifier.add_module('c_softmax', nn.Softmax(dim=1))  # 对每一行进行softmax\n",
    "\n",
    "        #discriminative classifier learn shared feature\n",
    "        self.discriminative_classifier = nn.Sequential()\n",
    "        self.discriminative_classifier.add_module('d_fc1', nn.Linear(32, 16))\n",
    "        self.discriminative_classifier.add_module('relu_f1', nn.ReLU())\n",
    "        self.discriminative_classifier.add_module('d_fc2', nn.Linear(16, self.predicted_Type))\n",
    "        self.discriminative_classifier.add_module('d_softmax',nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.predictor(embeddings)\n",
    "        shared_embeddings = re_shape.apply(embeddings)\n",
    "        link_output = self.predictor_classifier(shared_embeddings)\n",
    "        reverse_embeddings = GradReverse.apply(shared_embeddings, 1.0)\n",
    "        type_output = self.discriminative_classifier(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def train_adversarial_neural_networks(train_loader, validate_loader, model, output_file):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=learning_rate)\n",
    "    best_validate_acc = 0.000\n",
    "    best_validate_dir = ''\n",
    "\n",
    "    print('training model')\n",
    "    # Train of Model\n",
    "    for epoch in range(num_epochs):  # num_epochs is 50\n",
    "        p = float(epoch) / 100\n",
    "        lr = learning_rate / (1. + 10 * p) ** 0.75\n",
    "        optimizer.lr = lr\n",
    "        cost_vector = []\n",
    "        prediction_cost_vector = []\n",
    "        classification_cost_vector = []\n",
    "        acc_vector = []\n",
    "        valid_acc_vector = []\n",
    "        vali_cost_vector = []\n",
    "        train_score = []\n",
    "        train_label = []\n",
    "        for i, (train_data, train_labels, type_labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_data = to_var(train_data)\n",
    "            train_labels = to_var(train_labels),\n",
    "            type_labels = to_var(type_labels)\n",
    "            link_outputs, type_outputs = model(train_data.unsqueeze(1))\n",
    "            train_score += list(link_outputs[:, 1].cpu().detach().numpy())\n",
    "            train_label += list(train_labels[0].numpy())\n",
    "            train_labels = train_labels[0]\n",
    "            train_labels = train_labels.long()\n",
    "            type_labels = type_labels.long()\n",
    "            prediction_loss = criterion(link_outputs, train_labels)\n",
    "            classification_loss = criterion(type_outputs, type_labels)\n",
    "            loss = prediction_loss + classification_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(link_outputs, 1)\n",
    "            accuracy = (train_labels == argmax.squeeze()).float().mean()\n",
    "            prediction_cost_vector.append(prediction_loss.item())\n",
    "            classification_cost_vector.append(classification_loss.item())\n",
    "            cost_vector.append(loss.item())\n",
    "            acc_vector.append(accuracy.item())\n",
    "\n",
    "        # validate process\n",
    "        model.eval()\n",
    "        validate_acc_vector_temp = []\n",
    "        for i, (validate_data, validate_labels, type_labels) in enumerate(validate_loader):\n",
    "            validate_data = to_var(validate_data)\n",
    "            validate_labels = to_var(validate_labels)\n",
    "            type_labels = to_var(type_labels)\n",
    "            validate_outputs, type_outputs = model(validate_data.unsqueeze(1))\n",
    "            _, validate_argmax = torch.max(validate_outputs, 1)\n",
    "            validate_labels = validate_labels.long()\n",
    "            vali_loss = criterion(validate_outputs, validate_labels)\n",
    "            validate_accuracy = (validate_labels == validate_argmax.squeeze()).float().mean()\n",
    "            vali_cost_vector.append(vali_loss.item())\n",
    "            validate_acc_vector_temp.append(validate_accuracy.item())\n",
    "        validate_acc = np.mean(validate_acc_vector_temp)\n",
    "        valid_acc_vector.append(validate_acc)\n",
    "        model.train()\n",
    "        print('Epoch [%d/%d],  Loss: %.4f, Link Prediction Loss: %.4f, Type Classification Loss: %.4f, Train_Acc: %.4f,  Validate_Acc: %.4f.'\n",
    "              % (epoch + 1, num_epochs, np.mean(cost_vector), np.mean(prediction_cost_vector), np.mean(classification_cost_vector),\n",
    "                 np.mean(acc_vector), validate_acc))\n",
    "\n",
    "        if validate_acc > best_validate_acc:\n",
    "            best_validate_acc = validate_acc\n",
    "            if not os.path.exists(output_file):\n",
    "                os.mkdir(output_file)\n",
    "            best_validate_dir = output_file + str(epoch + 1) + '.pkl'\n",
    "            torch.save(model.state_dict(), best_validate_dir)\n",
    "    return best_validate_dir\n",
    "\n",
    "def test_adversarial_neural_networks(best_validate_dir, test_loader, model):\n",
    "    # Test the Model\n",
    "    print('testing model')\n",
    "    model.load_state_dict(torch.load(best_validate_dir))\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    test_score = []\n",
    "    test_pred = []\n",
    "    test_true = []\n",
    "    tes_score = []\n",
    "    tes_label = []\n",
    "\n",
    "    for i, (test_data, test_labels, type_labels) in enumerate(test_loader):\n",
    "        test_data = to_var(test_data)\n",
    "        test_labels = to_var(test_labels)\n",
    "        # type_labels = to_var(type_labels)\n",
    "        test_outputs, type_outputs = model(test_data.unsqueeze(1))\n",
    "        tes_score += list(test_outputs[:, 1].cpu().detach().numpy())\n",
    "        tes_label += list(test_labels.numpy())\n",
    "        _, test_argmax = torch.max(test_outputs, 1)\n",
    "        if i == 0:\n",
    "            test_score = to_np(test_outputs)\n",
    "            test_pred = to_np(test_argmax)\n",
    "            test_true = to_np(test_labels)\n",
    "        else:\n",
    "            test_score = np.concatenate((test_score, to_np(test_outputs)), axis=0)\n",
    "            test_pred = np.concatenate((test_pred, to_np(test_argmax)), axis=0)\n",
    "            test_true = np.concatenate((test_true, to_np(test_labels)), axis=0)\n",
    "\n",
    "    test_accuracy = metrics.accuracy_score(test_true, test_pred)\n",
    "    test_precision = metrics.precision_score(test_true, test_pred, average='macro')\n",
    "    test_aucroc = metrics.roc_auc_score(tes_label, tes_score, average='macro')\n",
    "\n",
    "    return test_aucroc, test_precision, test_accuracy\n",
    "\n",
    "def main(predicted_Type, dataset, output_file):\n",
    "    train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "    model = adversarial_neural_networks(predicted_Type)\n",
    "    best_validate_dir = train_adversarial_neural_networks(train_loader, validate_loader, model, output_file)\n",
    "    auc, precision, accuracy = test_adversarial_neural_networks(best_validate_dir, test_loader, model)\n",
    "    print(\"Final reault: AUC -- %.4f  \" % (auc), \"Precision -- %.4f  \" % (precision), 'Accuracy -- %.4f  ' % (accuracy))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = ['Facebook', 'IMDB', 'YELP', 'DBLP']\n",
    "    dataset = datasets[0]\n",
    "    print('Input dataset is:', dataset)\n",
    "    predicted_Type_datasets = {'Facebook': 4, 'IMDB': 2, 'YELP': 2, 'DBLP': 2}\n",
    "    predicted_Type = predicted_Type_datasets[dataset]\n",
    "    output_file = 'trainOutput/' + dataset.lower() + '/output.txt'\n",
    "    main(predicted_Type, dataset, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d975c-2c60-4210-abc9-71cce8d6f0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
