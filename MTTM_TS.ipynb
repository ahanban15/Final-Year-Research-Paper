{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c6bf1-8e1a-40ca-90b0-92096b0b18d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:92: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:93: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:92: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:93: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\NITDELHI LAB#04\\AppData\\Local\\Temp\\ipykernel_32620\\3900159450.py:92: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  nodel = int(re.sub(\"\\D\", \"\", nodeL[i][0]))\n",
      "C:\\Users\\NITDELHI LAB#04\\AppData\\Local\\Temp\\ipykernel_32620\\3900159450.py:93: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  noder = int(re.sub(\"\\D\", \"\", nodeR[i][0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length 1896\n",
      "validate length 475\n",
      "test length 1561\n",
      "training model\n",
      "Epoch [1/200],  Loss: 2.0022, Link Prediction Loss: 0.6346, Type Classification Loss: 1.3676, Train_Acc: 0.7240,  Validate_Acc: 0.7244.\n",
      "Epoch [2/200],  Loss: 1.8868, Link Prediction Loss: 0.5716, Type Classification Loss: 1.3152, Train_Acc: 0.7260,  Validate_Acc: 0.7244.\n",
      "Epoch [3/200],  Loss: 1.7881, Link Prediction Loss: 0.5013, Type Classification Loss: 1.2867, Train_Acc: 0.7276,  Validate_Acc: 0.7681.\n",
      "Epoch [4/200],  Loss: 1.6891, Link Prediction Loss: 0.4357, Type Classification Loss: 1.2535, Train_Acc: 0.9333,  Validate_Acc: 0.9542.\n",
      "Epoch [5/200],  Loss: 1.6397, Link Prediction Loss: 0.4052, Type Classification Loss: 1.2344, Train_Acc: 0.9563,  Validate_Acc: 0.9475.\n",
      "Epoch [6/200],  Loss: 1.5910, Link Prediction Loss: 0.3675, Type Classification Loss: 1.2236, Train_Acc: 0.9589,  Validate_Acc: 0.9559.\n",
      "Epoch [7/200],  Loss: 1.5542, Link Prediction Loss: 0.3529, Type Classification Loss: 1.2013, Train_Acc: 0.9656,  Validate_Acc: 0.9604.\n",
      "Epoch [8/200],  Loss: 1.5369, Link Prediction Loss: 0.3479, Type Classification Loss: 1.1890, Train_Acc: 0.9661,  Validate_Acc: 0.9625.\n",
      "Epoch [9/200],  Loss: 1.5295, Link Prediction Loss: 0.3444, Type Classification Loss: 1.1851, Train_Acc: 0.9703,  Validate_Acc: 0.9625.\n",
      "Epoch [10/200],  Loss: 1.5388, Link Prediction Loss: 0.3422, Type Classification Loss: 1.1966, Train_Acc: 0.9729,  Validate_Acc: 0.9646.\n",
      "Epoch [11/200],  Loss: 1.5286, Link Prediction Loss: 0.3410, Type Classification Loss: 1.1876, Train_Acc: 0.9734,  Validate_Acc: 0.9646.\n",
      "Epoch [12/200],  Loss: 1.5282, Link Prediction Loss: 0.3404, Type Classification Loss: 1.1878, Train_Acc: 0.9740,  Validate_Acc: 0.9646.\n",
      "Epoch [13/200],  Loss: 1.5282, Link Prediction Loss: 0.3408, Type Classification Loss: 1.1874, Train_Acc: 0.9724,  Validate_Acc: 0.9646.\n",
      "Epoch [14/200],  Loss: 1.5330, Link Prediction Loss: 0.3415, Type Classification Loss: 1.1915, Train_Acc: 0.9708,  Validate_Acc: 0.9646.\n",
      "Epoch [15/200],  Loss: 1.5286, Link Prediction Loss: 0.3396, Type Classification Loss: 1.1889, Train_Acc: 0.9740,  Validate_Acc: 0.9646.\n",
      "Epoch [16/200],  Loss: 1.5295, Link Prediction Loss: 0.3393, Type Classification Loss: 1.1902, Train_Acc: 0.9734,  Validate_Acc: 0.9646.\n",
      "Epoch [17/200],  Loss: 1.5312, Link Prediction Loss: 0.3384, Type Classification Loss: 1.1929, Train_Acc: 0.9745,  Validate_Acc: 0.9646.\n",
      "Epoch [18/200],  Loss: 1.5278, Link Prediction Loss: 0.3385, Type Classification Loss: 1.1893, Train_Acc: 0.9750,  Validate_Acc: 0.9667.\n",
      "Epoch [19/200],  Loss: 1.5282, Link Prediction Loss: 0.3375, Type Classification Loss: 1.1907, Train_Acc: 0.9760,  Validate_Acc: 0.9708.\n",
      "Epoch [20/200],  Loss: 1.5204, Link Prediction Loss: 0.3385, Type Classification Loss: 1.1819, Train_Acc: 0.9755,  Validate_Acc: 0.9625.\n",
      "Epoch [21/200],  Loss: 1.5226, Link Prediction Loss: 0.3370, Type Classification Loss: 1.1856, Train_Acc: 0.9760,  Validate_Acc: 0.9684.\n",
      "Epoch [22/200],  Loss: 1.5163, Link Prediction Loss: 0.3384, Type Classification Loss: 1.1780, Train_Acc: 0.9755,  Validate_Acc: 0.9625.\n",
      "Epoch [23/200],  Loss: 1.5188, Link Prediction Loss: 0.3390, Type Classification Loss: 1.1798, Train_Acc: 0.9745,  Validate_Acc: 0.9667.\n",
      "Epoch [24/200],  Loss: 1.5283, Link Prediction Loss: 0.3375, Type Classification Loss: 1.1909, Train_Acc: 0.9750,  Validate_Acc: 0.9688.\n",
      "Epoch [25/200],  Loss: 1.5295, Link Prediction Loss: 0.3378, Type Classification Loss: 1.1918, Train_Acc: 0.9750,  Validate_Acc: 0.9667.\n",
      "Epoch [26/200],  Loss: 1.5245, Link Prediction Loss: 0.3363, Type Classification Loss: 1.1883, Train_Acc: 0.9760,  Validate_Acc: 0.9604.\n",
      "Epoch [27/200],  Loss: 1.5295, Link Prediction Loss: 0.3367, Type Classification Loss: 1.1928, Train_Acc: 0.9760,  Validate_Acc: 0.9646.\n",
      "Epoch [28/200],  Loss: 1.5243, Link Prediction Loss: 0.3378, Type Classification Loss: 1.1865, Train_Acc: 0.9755,  Validate_Acc: 0.9708.\n",
      "Epoch [29/200],  Loss: 1.5247, Link Prediction Loss: 0.3368, Type Classification Loss: 1.1879, Train_Acc: 0.9755,  Validate_Acc: 0.9646.\n",
      "Epoch [30/200],  Loss: 1.5234, Link Prediction Loss: 0.3354, Type Classification Loss: 1.1879, Train_Acc: 0.9786,  Validate_Acc: 0.9688.\n",
      "Epoch [31/200],  Loss: 1.5202, Link Prediction Loss: 0.3371, Type Classification Loss: 1.1832, Train_Acc: 0.9771,  Validate_Acc: 0.9646.\n",
      "Epoch [32/200],  Loss: 1.5257, Link Prediction Loss: 0.3364, Type Classification Loss: 1.1893, Train_Acc: 0.9771,  Validate_Acc: 0.9708.\n",
      "Epoch [33/200],  Loss: 1.5256, Link Prediction Loss: 0.3364, Type Classification Loss: 1.1892, Train_Acc: 0.9766,  Validate_Acc: 0.9688.\n",
      "Epoch [34/200],  Loss: 1.5218, Link Prediction Loss: 0.3359, Type Classification Loss: 1.1859, Train_Acc: 0.9781,  Validate_Acc: 0.9708.\n",
      "Epoch [35/200],  Loss: 1.5237, Link Prediction Loss: 0.3368, Type Classification Loss: 1.1869, Train_Acc: 0.9766,  Validate_Acc: 0.9663.\n",
      "Epoch [36/200],  Loss: 1.5228, Link Prediction Loss: 0.3385, Type Classification Loss: 1.1842, Train_Acc: 0.9734,  Validate_Acc: 0.9708.\n",
      "Epoch [37/200],  Loss: 1.5139, Link Prediction Loss: 0.3354, Type Classification Loss: 1.1784, Train_Acc: 0.9781,  Validate_Acc: 0.9688.\n",
      "Epoch [38/200],  Loss: 1.5170, Link Prediction Loss: 0.3370, Type Classification Loss: 1.1800, Train_Acc: 0.9766,  Validate_Acc: 0.9646.\n",
      "Epoch [39/200],  Loss: 1.5158, Link Prediction Loss: 0.3362, Type Classification Loss: 1.1796, Train_Acc: 0.9766,  Validate_Acc: 0.9663.\n",
      "Epoch [40/200],  Loss: 1.5180, Link Prediction Loss: 0.3368, Type Classification Loss: 1.1812, Train_Acc: 0.9755,  Validate_Acc: 0.9646.\n",
      "Epoch [41/200],  Loss: 1.5199, Link Prediction Loss: 0.3364, Type Classification Loss: 1.1835, Train_Acc: 0.9760,  Validate_Acc: 0.9646.\n",
      "Epoch [42/200],  Loss: 1.5199, Link Prediction Loss: 0.3361, Type Classification Loss: 1.1838, Train_Acc: 0.9776,  Validate_Acc: 0.9688.\n",
      "Epoch [43/200],  Loss: 1.5222, Link Prediction Loss: 0.3347, Type Classification Loss: 1.1875, Train_Acc: 0.9786,  Validate_Acc: 0.9646.\n",
      "Epoch [44/200],  Loss: 1.5179, Link Prediction Loss: 0.3350, Type Classification Loss: 1.1828, Train_Acc: 0.9792,  Validate_Acc: 0.9646.\n",
      "Epoch [45/200],  Loss: 1.5198, Link Prediction Loss: 0.3348, Type Classification Loss: 1.1850, Train_Acc: 0.9781,  Validate_Acc: 0.9625.\n",
      "Epoch [46/200],  Loss: 1.5246, Link Prediction Loss: 0.3372, Type Classification Loss: 1.1874, Train_Acc: 0.9755,  Validate_Acc: 0.9663.\n",
      "Epoch [47/200],  Loss: 1.5209, Link Prediction Loss: 0.3351, Type Classification Loss: 1.1858, Train_Acc: 0.9776,  Validate_Acc: 0.9625.\n",
      "Epoch [48/200],  Loss: 1.5220, Link Prediction Loss: 0.3355, Type Classification Loss: 1.1864, Train_Acc: 0.9781,  Validate_Acc: 0.9646.\n",
      "Epoch [49/200],  Loss: 1.5200, Link Prediction Loss: 0.3356, Type Classification Loss: 1.1844, Train_Acc: 0.9766,  Validate_Acc: 0.9646.\n",
      "Epoch [50/200],  Loss: 1.5214, Link Prediction Loss: 0.3348, Type Classification Loss: 1.1866, Train_Acc: 0.9781,  Validate_Acc: 0.9667.\n",
      "Epoch [51/200],  Loss: 1.5226, Link Prediction Loss: 0.3356, Type Classification Loss: 1.1870, Train_Acc: 0.9786,  Validate_Acc: 0.9646.\n",
      "Epoch [52/200],  Loss: 1.5214, Link Prediction Loss: 0.3346, Type Classification Loss: 1.1867, Train_Acc: 0.9786,  Validate_Acc: 0.9604.\n",
      "Epoch [53/200],  Loss: 1.5252, Link Prediction Loss: 0.3358, Type Classification Loss: 1.1893, Train_Acc: 0.9776,  Validate_Acc: 0.9642.\n",
      "Epoch [54/200],  Loss: 1.5213, Link Prediction Loss: 0.3361, Type Classification Loss: 1.1852, Train_Acc: 0.9776,  Validate_Acc: 0.9646.\n",
      "Epoch [55/200],  Loss: 1.5195, Link Prediction Loss: 0.3343, Type Classification Loss: 1.1852, Train_Acc: 0.9786,  Validate_Acc: 0.9579.\n",
      "Epoch [56/200],  Loss: 1.5151, Link Prediction Loss: 0.3347, Type Classification Loss: 1.1804, Train_Acc: 0.9786,  Validate_Acc: 0.9646.\n",
      "Epoch [57/200],  Loss: 1.5148, Link Prediction Loss: 0.3344, Type Classification Loss: 1.1804, Train_Acc: 0.9792,  Validate_Acc: 0.9600.\n",
      "Epoch [58/200],  Loss: 1.5228, Link Prediction Loss: 0.3390, Type Classification Loss: 1.1838, Train_Acc: 0.9745,  Validate_Acc: 0.9646.\n",
      "Epoch [59/200],  Loss: 1.5206, Link Prediction Loss: 0.3353, Type Classification Loss: 1.1853, Train_Acc: 0.9781,  Validate_Acc: 0.9646.\n",
      "Epoch [60/200],  Loss: 1.5334, Link Prediction Loss: 0.3450, Type Classification Loss: 1.1885, Train_Acc: 0.9667,  Validate_Acc: 0.9559.\n",
      "Epoch [61/200],  Loss: 1.5211, Link Prediction Loss: 0.3356, Type Classification Loss: 1.1854, Train_Acc: 0.9771,  Validate_Acc: 0.9646.\n",
      "Epoch [62/200],  Loss: 1.5199, Link Prediction Loss: 0.3340, Type Classification Loss: 1.1859, Train_Acc: 0.9792,  Validate_Acc: 0.9646.\n",
      "Epoch [63/200],  Loss: 1.5210, Link Prediction Loss: 0.3337, Type Classification Loss: 1.1873, Train_Acc: 0.9797,  Validate_Acc: 0.9604.\n",
      "Epoch [64/200],  Loss: 1.5193, Link Prediction Loss: 0.3360, Type Classification Loss: 1.1833, Train_Acc: 0.9776,  Validate_Acc: 0.9600.\n",
      "Epoch [65/200],  Loss: 1.5262, Link Prediction Loss: 0.3368, Type Classification Loss: 1.1894, Train_Acc: 0.9760,  Validate_Acc: 0.9559.\n",
      "Epoch [66/200],  Loss: 1.5262, Link Prediction Loss: 0.3366, Type Classification Loss: 1.1896, Train_Acc: 0.9766,  Validate_Acc: 0.9621.\n",
      "Epoch [67/200],  Loss: 1.5277, Link Prediction Loss: 0.3360, Type Classification Loss: 1.1917, Train_Acc: 0.9776,  Validate_Acc: 0.9646.\n",
      "Epoch [68/200],  Loss: 1.5215, Link Prediction Loss: 0.3333, Type Classification Loss: 1.1882, Train_Acc: 0.9792,  Validate_Acc: 0.9621.\n",
      "Epoch [69/200],  Loss: 1.5186, Link Prediction Loss: 0.3333, Type Classification Loss: 1.1854, Train_Acc: 0.9802,  Validate_Acc: 0.9708.\n",
      "Epoch [70/200],  Loss: 1.5091, Link Prediction Loss: 0.3326, Type Classification Loss: 1.1765, Train_Acc: 0.9807,  Validate_Acc: 0.9646.\n",
      "Epoch [71/200],  Loss: 1.5031, Link Prediction Loss: 0.3340, Type Classification Loss: 1.1691, Train_Acc: 0.9797,  Validate_Acc: 0.9667.\n",
      "Epoch [72/200],  Loss: 1.5127, Link Prediction Loss: 0.3335, Type Classification Loss: 1.1792, Train_Acc: 0.9797,  Validate_Acc: 0.9646.\n",
      "Epoch [73/200],  Loss: 1.5150, Link Prediction Loss: 0.3319, Type Classification Loss: 1.1831, Train_Acc: 0.9818,  Validate_Acc: 0.9646.\n",
      "Epoch [74/200],  Loss: 1.5127, Link Prediction Loss: 0.3322, Type Classification Loss: 1.1805, Train_Acc: 0.9807,  Validate_Acc: 0.9646.\n",
      "Epoch [75/200],  Loss: 1.5149, Link Prediction Loss: 0.3321, Type Classification Loss: 1.1827, Train_Acc: 0.9812,  Validate_Acc: 0.9646.\n",
      "Epoch [76/200],  Loss: 1.5139, Link Prediction Loss: 0.3331, Type Classification Loss: 1.1808, Train_Acc: 0.9802,  Validate_Acc: 0.9667.\n",
      "Epoch [77/200],  Loss: 1.5182, Link Prediction Loss: 0.3320, Type Classification Loss: 1.1862, Train_Acc: 0.9818,  Validate_Acc: 0.9646.\n",
      "Epoch [78/200],  Loss: 1.5229, Link Prediction Loss: 0.3339, Type Classification Loss: 1.1890, Train_Acc: 0.9797,  Validate_Acc: 0.9646.\n",
      "Epoch [79/200],  Loss: 1.5212, Link Prediction Loss: 0.3323, Type Classification Loss: 1.1889, Train_Acc: 0.9812,  Validate_Acc: 0.9625.\n",
      "Epoch [80/200],  Loss: 1.5213, Link Prediction Loss: 0.3317, Type Classification Loss: 1.1896, Train_Acc: 0.9818,  Validate_Acc: 0.9667.\n",
      "Epoch [81/200],  Loss: 1.5160, Link Prediction Loss: 0.3312, Type Classification Loss: 1.1848, Train_Acc: 0.9823,  Validate_Acc: 0.9646.\n",
      "Epoch [82/200],  Loss: 1.5173, Link Prediction Loss: 0.3312, Type Classification Loss: 1.1861, Train_Acc: 0.9823,  Validate_Acc: 0.9646.\n",
      "Epoch [83/200],  Loss: 1.5162, Link Prediction Loss: 0.3311, Type Classification Loss: 1.1852, Train_Acc: 0.9823,  Validate_Acc: 0.9646.\n",
      "Epoch [84/200],  Loss: 1.5149, Link Prediction Loss: 0.3311, Type Classification Loss: 1.1838, Train_Acc: 0.9823,  Validate_Acc: 0.9625.\n",
      "Epoch [85/200],  Loss: 1.5229, Link Prediction Loss: 0.3375, Type Classification Loss: 1.1854, Train_Acc: 0.9750,  Validate_Acc: 0.9667.\n",
      "Epoch [86/200],  Loss: 1.5251, Link Prediction Loss: 0.3388, Type Classification Loss: 1.1862, Train_Acc: 0.9745,  Validate_Acc: 0.9646.\n",
      "Epoch [87/200],  Loss: 1.5269, Link Prediction Loss: 0.3387, Type Classification Loss: 1.1882, Train_Acc: 0.9734,  Validate_Acc: 0.9708.\n",
      "Epoch [88/200],  Loss: 1.5234, Link Prediction Loss: 0.3343, Type Classification Loss: 1.1891, Train_Acc: 0.9786,  Validate_Acc: 0.9646.\n",
      "Epoch [89/200],  Loss: 1.5148, Link Prediction Loss: 0.3337, Type Classification Loss: 1.1811, Train_Acc: 0.9797,  Validate_Acc: 0.9646.\n",
      "Epoch [90/200],  Loss: 1.5206, Link Prediction Loss: 0.3349, Type Classification Loss: 1.1857, Train_Acc: 0.9776,  Validate_Acc: 0.9667.\n",
      "Epoch [91/200],  Loss: 1.5183, Link Prediction Loss: 0.3327, Type Classification Loss: 1.1856, Train_Acc: 0.9802,  Validate_Acc: 0.9688.\n",
      "Epoch [92/200],  Loss: 1.5196, Link Prediction Loss: 0.3334, Type Classification Loss: 1.1862, Train_Acc: 0.9792,  Validate_Acc: 0.9667.\n",
      "Epoch [93/200],  Loss: 1.5192, Link Prediction Loss: 0.3323, Type Classification Loss: 1.1869, Train_Acc: 0.9812,  Validate_Acc: 0.9604.\n",
      "Epoch [94/200],  Loss: 1.5178, Link Prediction Loss: 0.3327, Type Classification Loss: 1.1851, Train_Acc: 0.9807,  Validate_Acc: 0.9625.\n",
      "Epoch [95/200],  Loss: 1.5187, Link Prediction Loss: 0.3351, Type Classification Loss: 1.1836, Train_Acc: 0.9786,  Validate_Acc: 0.9625.\n",
      "Epoch [96/200],  Loss: 1.5131, Link Prediction Loss: 0.3329, Type Classification Loss: 1.1802, Train_Acc: 0.9807,  Validate_Acc: 0.9667.\n",
      "Epoch [97/200],  Loss: 1.5165, Link Prediction Loss: 0.3358, Type Classification Loss: 1.1807, Train_Acc: 0.9776,  Validate_Acc: 0.9625.\n",
      "Epoch [98/200],  Loss: 1.5072, Link Prediction Loss: 0.3341, Type Classification Loss: 1.1731, Train_Acc: 0.9792,  Validate_Acc: 0.9625.\n",
      "Epoch [99/200],  Loss: 1.5024, Link Prediction Loss: 0.3350, Type Classification Loss: 1.1674, Train_Acc: 0.9781,  Validate_Acc: 0.9604.\n",
      "Epoch [100/200],  Loss: 1.5209, Link Prediction Loss: 0.3353, Type Classification Loss: 1.1856, Train_Acc: 0.9771,  Validate_Acc: 0.9625.\n",
      "Epoch [101/200],  Loss: 1.5221, Link Prediction Loss: 0.3328, Type Classification Loss: 1.1893, Train_Acc: 0.9802,  Validate_Acc: 0.9604.\n",
      "Epoch [102/200],  Loss: 1.5223, Link Prediction Loss: 0.3341, Type Classification Loss: 1.1882, Train_Acc: 0.9786,  Validate_Acc: 0.9625.\n",
      "Epoch [103/200],  Loss: 1.5181, Link Prediction Loss: 0.3314, Type Classification Loss: 1.1867, Train_Acc: 0.9823,  Validate_Acc: 0.9667.\n",
      "Epoch [104/200],  Loss: 1.5201, Link Prediction Loss: 0.3319, Type Classification Loss: 1.1882, Train_Acc: 0.9812,  Validate_Acc: 0.9667.\n",
      "Epoch [105/200],  Loss: 1.5194, Link Prediction Loss: 0.3329, Type Classification Loss: 1.1866, Train_Acc: 0.9802,  Validate_Acc: 0.9646.\n",
      "Epoch [106/200],  Loss: 1.5168, Link Prediction Loss: 0.3318, Type Classification Loss: 1.1850, Train_Acc: 0.9818,  Validate_Acc: 0.9604.\n",
      "Epoch [107/200],  Loss: 1.5237, Link Prediction Loss: 0.3388, Type Classification Loss: 1.1849, Train_Acc: 0.9745,  Validate_Acc: 0.9604.\n",
      "Epoch [108/200],  Loss: 1.5203, Link Prediction Loss: 0.3307, Type Classification Loss: 1.1896, Train_Acc: 0.9828,  Validate_Acc: 0.9667.\n",
      "Epoch [109/200],  Loss: 1.5181, Link Prediction Loss: 0.3332, Type Classification Loss: 1.1849, Train_Acc: 0.9802,  Validate_Acc: 0.9646.\n",
      "Epoch [110/200],  Loss: 1.5199, Link Prediction Loss: 0.3334, Type Classification Loss: 1.1864, Train_Acc: 0.9797,  Validate_Acc: 0.9646.\n",
      "Epoch [111/200],  Loss: 1.5220, Link Prediction Loss: 0.3324, Type Classification Loss: 1.1896, Train_Acc: 0.9802,  Validate_Acc: 0.9646.\n",
      "Epoch [112/200],  Loss: 1.5185, Link Prediction Loss: 0.3320, Type Classification Loss: 1.1864, Train_Acc: 0.9812,  Validate_Acc: 0.9646.\n",
      "Epoch [113/200],  Loss: 1.5204, Link Prediction Loss: 0.3324, Type Classification Loss: 1.1880, Train_Acc: 0.9802,  Validate_Acc: 0.9646.\n",
      "Epoch [114/200],  Loss: 1.5177, Link Prediction Loss: 0.3313, Type Classification Loss: 1.1864, Train_Acc: 0.9823,  Validate_Acc: 0.9604.\n",
      "Epoch [115/200],  Loss: 1.5177, Link Prediction Loss: 0.3328, Type Classification Loss: 1.1848, Train_Acc: 0.9807,  Validate_Acc: 0.9646.\n",
      "Epoch [116/200],  Loss: 1.5190, Link Prediction Loss: 0.3326, Type Classification Loss: 1.1864, Train_Acc: 0.9807,  Validate_Acc: 0.9646.\n",
      "Epoch [117/200],  Loss: 1.5167, Link Prediction Loss: 0.3334, Type Classification Loss: 1.1833, Train_Acc: 0.9797,  Validate_Acc: 0.9646.\n",
      "Epoch [118/200],  Loss: 1.5209, Link Prediction Loss: 0.3313, Type Classification Loss: 1.1895, Train_Acc: 0.9818,  Validate_Acc: 0.9621.\n",
      "Epoch [119/200],  Loss: 1.5242, Link Prediction Loss: 0.3331, Type Classification Loss: 1.1911, Train_Acc: 0.9797,  Validate_Acc: 0.9667.\n",
      "Epoch [120/200],  Loss: 1.5157, Link Prediction Loss: 0.3308, Type Classification Loss: 1.1848, Train_Acc: 0.9823,  Validate_Acc: 0.9646.\n",
      "Epoch [121/200],  Loss: 1.5168, Link Prediction Loss: 0.3320, Type Classification Loss: 1.1848, Train_Acc: 0.9807,  Validate_Acc: 0.9646.\n",
      "Epoch [122/200],  Loss: 1.5164, Link Prediction Loss: 0.3315, Type Classification Loss: 1.1848, Train_Acc: 0.9812,  Validate_Acc: 0.9667.\n",
      "Epoch [123/200],  Loss: 1.5142, Link Prediction Loss: 0.3309, Type Classification Loss: 1.1833, Train_Acc: 0.9818,  Validate_Acc: 0.9646.\n",
      "Epoch [124/200],  Loss: 1.5174, Link Prediction Loss: 0.3310, Type Classification Loss: 1.1864, Train_Acc: 0.9818,  Validate_Acc: 0.9667.\n",
      "Epoch [125/200],  Loss: 1.5163, Link Prediction Loss: 0.3299, Type Classification Loss: 1.1864, Train_Acc: 0.9833,  Validate_Acc: 0.9646.\n",
      "Epoch [126/200],  Loss: 1.5196, Link Prediction Loss: 0.3317, Type Classification Loss: 1.1880, Train_Acc: 0.9818,  Validate_Acc: 0.9646.\n",
      "Epoch [127/200],  Loss: 1.5165, Link Prediction Loss: 0.3301, Type Classification Loss: 1.1864, Train_Acc: 0.9833,  Validate_Acc: 0.9646.\n",
      "Epoch [128/200],  Loss: 1.5168, Link Prediction Loss: 0.3320, Type Classification Loss: 1.1848, Train_Acc: 0.9812,  Validate_Acc: 0.9646.\n",
      "Epoch [129/200],  Loss: 1.5161, Link Prediction Loss: 0.3313, Type Classification Loss: 1.1848, Train_Acc: 0.9823,  Validate_Acc: 0.9667.\n",
      "Epoch [130/200],  Loss: 1.5188, Link Prediction Loss: 0.3308, Type Classification Loss: 1.1879, Train_Acc: 0.9823,  Validate_Acc: 0.9667.\n",
      "Epoch [131/200],  Loss: 1.5145, Link Prediction Loss: 0.3296, Type Classification Loss: 1.1848, Train_Acc: 0.9839,  Validate_Acc: 0.9646.\n",
      "Epoch [132/200],  Loss: 1.5195, Link Prediction Loss: 0.3316, Type Classification Loss: 1.1879, Train_Acc: 0.9818,  Validate_Acc: 0.9667.\n",
      "Epoch [133/200],  Loss: 1.5149, Link Prediction Loss: 0.3301, Type Classification Loss: 1.1848, Train_Acc: 0.9828,  Validate_Acc: 0.9688.\n",
      "Epoch [134/200],  Loss: 1.5203, Link Prediction Loss: 0.3307, Type Classification Loss: 1.1895, Train_Acc: 0.9823,  Validate_Acc: 0.9667.\n",
      "Epoch [135/200],  Loss: 1.5145, Link Prediction Loss: 0.3297, Type Classification Loss: 1.1848, Train_Acc: 0.9833,  Validate_Acc: 0.9646.\n",
      "Epoch [136/200],  Loss: 1.5159, Link Prediction Loss: 0.3296, Type Classification Loss: 1.1864, Train_Acc: 0.9839,  Validate_Acc: 0.9646.\n",
      "Epoch [137/200],  Loss: 1.5178, Link Prediction Loss: 0.3329, Type Classification Loss: 1.1848, Train_Acc: 0.9807,  Validate_Acc: 0.9646.\n",
      "Epoch [138/200],  Loss: 1.5191, Link Prediction Loss: 0.3312, Type Classification Loss: 1.1879, Train_Acc: 0.9818,  Validate_Acc: 0.9667.\n",
      "Epoch [139/200],  Loss: 1.5197, Link Prediction Loss: 0.3302, Type Classification Loss: 1.1895, Train_Acc: 0.9833,  Validate_Acc: 0.9667.\n",
      "Epoch [140/200],  Loss: 1.5187, Link Prediction Loss: 0.3308, Type Classification Loss: 1.1879, Train_Acc: 0.9823,  Validate_Acc: 0.9688.\n",
      "Epoch [141/200],  Loss: 1.5191, Link Prediction Loss: 0.3296, Type Classification Loss: 1.1895, Train_Acc: 0.9839,  Validate_Acc: 0.9646.\n",
      "Epoch [142/200],  Loss: 1.5175, Link Prediction Loss: 0.3296, Type Classification Loss: 1.1879, Train_Acc: 0.9833,  Validate_Acc: 0.9646.\n",
      "Epoch [143/200],  Loss: 1.5140, Link Prediction Loss: 0.3292, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [144/200],  Loss: 1.5171, Link Prediction Loss: 0.3291, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [145/200],  Loss: 1.5198, Link Prediction Loss: 0.3303, Type Classification Loss: 1.1895, Train_Acc: 0.9833,  Validate_Acc: 0.9667.\n",
      "Epoch [146/200],  Loss: 1.5157, Link Prediction Loss: 0.3293, Type Classification Loss: 1.1864, Train_Acc: 0.9839,  Validate_Acc: 0.9667.\n",
      "Epoch [147/200],  Loss: 1.5185, Link Prediction Loss: 0.3290, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [148/200],  Loss: 1.5200, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1911, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [149/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [150/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [151/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [152/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [153/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [154/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [155/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [156/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [157/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [158/200],  Loss: 1.5153, Link Prediction Loss: 0.3305, Type Classification Loss: 1.1848, Train_Acc: 0.9828,  Validate_Acc: 0.9667.\n",
      "Epoch [159/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [160/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [161/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [162/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [163/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [164/200],  Loss: 1.5184, Link Prediction Loss: 0.3305, Type Classification Loss: 1.1879, Train_Acc: 0.9828,  Validate_Acc: 0.9667.\n",
      "Epoch [165/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [166/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [167/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [168/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [169/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [170/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [171/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [172/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [173/200],  Loss: 1.5168, Link Prediction Loss: 0.3305, Type Classification Loss: 1.1864, Train_Acc: 0.9828,  Validate_Acc: 0.9667.\n",
      "Epoch [174/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [175/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [176/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [177/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [178/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [179/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [180/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [181/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [182/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [183/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [184/200],  Loss: 1.5200, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1911, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [185/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [186/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [187/200],  Loss: 1.5168, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1879, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [188/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [189/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [190/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [191/200],  Loss: 1.5106, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1817, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [192/200],  Loss: 1.5168, Link Prediction Loss: 0.3305, Type Classification Loss: 1.1864, Train_Acc: 0.9828,  Validate_Acc: 0.9667.\n",
      "Epoch [193/200],  Loss: 1.5153, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1864, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [194/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [195/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [196/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [197/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [198/200],  Loss: 1.5137, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1848, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [199/200],  Loss: 1.5184, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1895, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "Epoch [200/200],  Loss: 1.5200, Link Prediction Loss: 0.3289, Type Classification Loss: 1.1911, Train_Acc: 0.9844,  Validate_Acc: 0.9667.\n",
      "testing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NITDELHI LAB#04\\AppData\\Local\\Temp\\ipykernel_32620\\3900159450.py:289: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_validate_dir))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length 1896\n",
      "validate length 475\n",
      "test length 1561\n",
      "training model\n",
      "Epoch [1/200],  Loss: 2.0680, Link Prediction Loss: 0.6639, Type Classification Loss: 1.4041, Train_Acc: 0.6687,  Validate_Acc: 0.7299.\n",
      "Epoch [2/200],  Loss: 1.9329, Link Prediction Loss: 0.5841, Type Classification Loss: 1.3488, Train_Acc: 0.7260,  Validate_Acc: 0.7299.\n",
      "Epoch [3/200],  Loss: 1.8824, Link Prediction Loss: 0.5578, Type Classification Loss: 1.3245, Train_Acc: 0.7260,  Validate_Acc: 0.7299.\n",
      "Epoch [4/200],  Loss: 1.7490, Link Prediction Loss: 0.4828, Type Classification Loss: 1.2661, Train_Acc: 0.7740,  Validate_Acc: 0.9638.\n",
      "Epoch [5/200],  Loss: 1.6785, Link Prediction Loss: 0.4124, Type Classification Loss: 1.2660, Train_Acc: 0.9526,  Validate_Acc: 0.9680.\n",
      "Epoch [6/200],  Loss: 1.5874, Link Prediction Loss: 0.3658, Type Classification Loss: 1.2216, Train_Acc: 0.9609,  Validate_Acc: 0.9767.\n",
      "Epoch [7/200],  Loss: 1.5529, Link Prediction Loss: 0.3514, Type Classification Loss: 1.2015, Train_Acc: 0.9656,  Validate_Acc: 0.9767.\n",
      "Epoch [8/200],  Loss: 1.5507, Link Prediction Loss: 0.3501, Type Classification Loss: 1.2006, Train_Acc: 0.9641,  Validate_Acc: 0.9767.\n",
      "Epoch [9/200],  Loss: 1.5501, Link Prediction Loss: 0.3466, Type Classification Loss: 1.2035, Train_Acc: 0.9693,  Validate_Acc: 0.9746.\n",
      "Epoch [10/200],  Loss: 1.5428, Link Prediction Loss: 0.3448, Type Classification Loss: 1.1980, Train_Acc: 0.9703,  Validate_Acc: 0.9767.\n",
      "Epoch [11/200],  Loss: 1.5389, Link Prediction Loss: 0.3420, Type Classification Loss: 1.1968, Train_Acc: 0.9734,  Validate_Acc: 0.9767.\n",
      "Epoch [12/200],  Loss: 1.5353, Link Prediction Loss: 0.3408, Type Classification Loss: 1.1945, Train_Acc: 0.9724,  Validate_Acc: 0.9788.\n",
      "Epoch [13/200],  Loss: 1.5201, Link Prediction Loss: 0.3399, Type Classification Loss: 1.1802, Train_Acc: 0.9740,  Validate_Acc: 0.9746.\n",
      "Epoch [14/200],  Loss: 1.5276, Link Prediction Loss: 0.3401, Type Classification Loss: 1.1875, Train_Acc: 0.9740,  Validate_Acc: 0.9767.\n",
      "Epoch [15/200],  Loss: 1.5415, Link Prediction Loss: 0.3413, Type Classification Loss: 1.2003, Train_Acc: 0.9714,  Validate_Acc: 0.9767.\n",
      "Epoch [16/200],  Loss: 1.5278, Link Prediction Loss: 0.3397, Type Classification Loss: 1.1882, Train_Acc: 0.9740,  Validate_Acc: 0.9788.\n",
      "Epoch [17/200],  Loss: 1.5333, Link Prediction Loss: 0.3409, Type Classification Loss: 1.1925, Train_Acc: 0.9740,  Validate_Acc: 0.9809.\n",
      "Epoch [18/200],  Loss: 1.5293, Link Prediction Loss: 0.3393, Type Classification Loss: 1.1900, Train_Acc: 0.9745,  Validate_Acc: 0.9725.\n",
      "Epoch [19/200],  Loss: 1.5306, Link Prediction Loss: 0.3404, Type Classification Loss: 1.1902, Train_Acc: 0.9729,  Validate_Acc: 0.9809.\n",
      "Epoch [20/200],  Loss: 1.5332, Link Prediction Loss: 0.3397, Type Classification Loss: 1.1935, Train_Acc: 0.9729,  Validate_Acc: 0.9788.\n",
      "Epoch [21/200],  Loss: 1.5325, Link Prediction Loss: 0.3389, Type Classification Loss: 1.1936, Train_Acc: 0.9745,  Validate_Acc: 0.9788.\n",
      "Epoch [22/200],  Loss: 1.5285, Link Prediction Loss: 0.3380, Type Classification Loss: 1.1905, Train_Acc: 0.9755,  Validate_Acc: 0.9788.\n",
      "Epoch [23/200],  Loss: 1.5277, Link Prediction Loss: 0.3389, Type Classification Loss: 1.1888, Train_Acc: 0.9750,  Validate_Acc: 0.9788.\n",
      "Epoch [24/200],  Loss: 1.5257, Link Prediction Loss: 0.3375, Type Classification Loss: 1.1881, Train_Acc: 0.9750,  Validate_Acc: 0.9767.\n",
      "Epoch [25/200],  Loss: 1.5283, Link Prediction Loss: 0.3381, Type Classification Loss: 1.1902, Train_Acc: 0.9771,  Validate_Acc: 0.9788.\n",
      "Epoch [26/200],  Loss: 1.5263, Link Prediction Loss: 0.3376, Type Classification Loss: 1.1887, Train_Acc: 0.9755,  Validate_Acc: 0.9809.\n",
      "Epoch [27/200],  Loss: 1.5316, Link Prediction Loss: 0.3457, Type Classification Loss: 1.1859, Train_Acc: 0.9651,  Validate_Acc: 0.9788.\n",
      "Epoch [28/200],  Loss: 1.5385, Link Prediction Loss: 0.3431, Type Classification Loss: 1.1954, Train_Acc: 0.9698,  Validate_Acc: 0.9725.\n",
      "Epoch [29/200],  Loss: 1.5300, Link Prediction Loss: 0.3387, Type Classification Loss: 1.1914, Train_Acc: 0.9750,  Validate_Acc: 0.9767.\n",
      "Epoch [30/200],  Loss: 1.5284, Link Prediction Loss: 0.3395, Type Classification Loss: 1.1889, Train_Acc: 0.9740,  Validate_Acc: 0.9809.\n",
      "Epoch [31/200],  Loss: 1.5267, Link Prediction Loss: 0.3377, Type Classification Loss: 1.1890, Train_Acc: 0.9755,  Validate_Acc: 0.9767.\n",
      "Epoch [32/200],  Loss: 1.5218, Link Prediction Loss: 0.3378, Type Classification Loss: 1.1839, Train_Acc: 0.9750,  Validate_Acc: 0.9725.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import time, os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import time, os\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 16\n",
    "\n",
    "class edgeFeatures(object):\n",
    "    def __init__(self, label=None, type = None, embeddings = None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        return\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ', skiprows=0)\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ', skiprows=0)\n",
    "\n",
    "    train_Real_Graph = nx.Graph()\n",
    "    train_Fake_Graph = nx.Graph()\n",
    "    test_Real_Graph = nx.Graph()\n",
    "    test_Fake_Graph = nx.Graph()\n",
    "\n",
    "    real_edge_Attritube = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edge_Attritube = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    lenReal = len(real_edge_Attritube)\n",
    "    lenFake = len(fake_edge_Attritube)\n",
    "\n",
    "    # print(real_edge_Attritube)\n",
    "    #new type id according to dataset\n",
    "    if dataset.lower() == 'facebook':\n",
    "        dataNewType = [9, 8, 7, 6, 5, 4]\n",
    "    else:\n",
    "        dataNewType = [2]\n",
    "\n",
    "    for i in range(lenReal):\n",
    "        relation = real_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Real_Graph.add_edge(real_edge_Attritube[i][0], real_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "\n",
    "    for i in range(lenFake):\n",
    "        relation = fake_edge_Attritube[i][2]\n",
    "        if relation in dataNewType:\n",
    "            test_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "        else:\n",
    "            train_Fake_Graph.add_edge(fake_edge_Attritube[i][0], fake_edge_Attritube[i][1], relationship=relation)\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    realFileName = 'Datasets/' + dataset + '/realData.csv'\n",
    "    fakeFileName = 'Datasets/' + dataset + '/fakeData.csv'\n",
    "    train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph = structuralGraph(realFileName, fakeFileName, dataset)\n",
    "    node2vecReFile = \"Datasets/node2vecFeature/\" + dataset + \"Feature.txt\"\n",
    "    data = pd.read_csv(node2vecReFile, sep=' ', skiprows=1, header=None)\n",
    "    edges = np.array(data.iloc[:, 0:1]) + np.array(data.iloc[:, 1:2])\n",
    "    embeddings = np.array(data.iloc[:, 2:66])\n",
    "    nodeL = np.array(data.iloc[:, 0:1])\n",
    "    nodeR = np.array(data.iloc[:, 1:2])\n",
    "    train_data = []\n",
    "    test = []\n",
    "    for i in range(len(edges)):\n",
    "        edgeFeature = edgeFeatures(\" \")\n",
    "        nodel = int(re.sub(\"\\D\", \"\", nodeL[i][0]))\n",
    "        noder = int(re.sub(\"\\D\", \"\", nodeR[i][0]))\n",
    "        if train_Real_Graph.has_edge(nodel, noder) or train_Fake_Graph.has_edge(nodel, noder): # train set\n",
    "            if train_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = train_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = train_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_Real_Graph.has_edge(nodel, noder) or test_Fake_Graph.has_edge(nodel, noder):  # test set\n",
    "            if test_Real_Graph.has_edge(nodel, noder):\n",
    "                label = 1\n",
    "                type = test_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            else:\n",
    "                label = 0\n",
    "                type = test_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            edgeFeature.type = type\n",
    "            test.append(edgeFeature)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)  # train_test_split返回切分的数据集train/validate\n",
    "    train_dataset = []\n",
    "    validate_dataset = []\n",
    "    test_dataset = []\n",
    "    for index, element in enumerate(train):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        train_dataset.append(m)\n",
    "    for index, element in enumerate(validate):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        validate_dataset.append(m)\n",
    "    for index, element in enumerate(test):\n",
    "        vectors = torch.tensor(element.embeddings, dtype=torch.float32)\n",
    "        label = torch.tensor(element.label, dtype=torch.float32)\n",
    "        type = torch.tensor(element.type, dtype=torch.float32)\n",
    "        m = [vectors, label, type]\n",
    "        test_dataset.append(m)\n",
    "    print('train length', len(train_dataset))\n",
    "    print('validate length', len(validate_dataset))\n",
    "    print('test length', len(test_dataset))\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(dataset=validate_dataset, batch_size=batch_size,  shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validate_loader, test_loader\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "class re_shape(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x.reshape(len(x),len(x[0][0])))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output =  grad_output.reshape(len(grad_output),1,len(grad_output[0]))\n",
    "        return output,None\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @ staticmethod\n",
    "    def forward(ctx, x, lambd, **kwargs: None):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_output):\n",
    "        return grad_output[0] * -ctx.lambd, None\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "class adversarial_neural_networks(nn.Module):\n",
    "    def __init__(self, predicted_Type):\n",
    "        super(adversarial_neural_networks, self).__init__()\n",
    "        self.predicted_Type = predicted_Type\n",
    "\n",
    "        ##The generative predictor\n",
    "        self.predictor = nn.Sequential()\n",
    "        self.predictor.add_module('exta_Conv1',nn.Conv1d(in_channels=1, out_channels=1, kernel_size=10, stride=1, padding=0))\n",
    "        self.predictor.add_module('fully_connected_layer1', nn.Linear(55, 32))\n",
    "\n",
    "        self.predictor_classifier = nn.Sequential()\n",
    "        self.predictor_classifier.add_module('c_fc1', nn.Linear(32,24))\n",
    "        self.predictor_classifier.add_module('c_fc1_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc2', nn.Linear(24, 16))\n",
    "        self.predictor_classifier.add_module('c_fc2_relu', nn.ReLU())\n",
    "        self.predictor_classifier.add_module('c_fc3', nn.Linear(16, 2))\n",
    "        self.predictor_classifier.add_module('c_softmax', nn.Softmax(dim=1))  # 对每一行进行softmax\n",
    "\n",
    "        #discriminative classifier learn shared feature\n",
    "        self.discriminative_classifier = nn.Sequential()\n",
    "        self.discriminative_classifier.add_module('d_fc1', nn.Linear(32, 16))\n",
    "        self.discriminative_classifier.add_module('relu_f1', nn.ReLU())\n",
    "        self.discriminative_classifier.add_module('d_fc2', nn.Linear(16, self.predicted_Type))\n",
    "        self.discriminative_classifier.add_module('d_softmax',nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.predictor(embeddings)\n",
    "        shared_embeddings = re_shape.apply(embeddings)\n",
    "        link_output = self.predictor_classifier(shared_embeddings)\n",
    "        reverse_embeddings = GradReverse.apply(shared_embeddings, 1.0)\n",
    "        type_output = self.discriminative_classifier(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def train_adversarial_neural_networks(train_loader, validate_loader, model, output_file):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=learning_rate)\n",
    "    best_validate_acc = 0.000\n",
    "    best_validate_dir = ''\n",
    "\n",
    "    print('training model')\n",
    "    # Train of Model\n",
    "    for epoch in range(num_epochs):  # num_epochs is 50\n",
    "        p = float(epoch) / 100\n",
    "        lr = learning_rate / (1. + 10 * p) ** 0.75\n",
    "        optimizer.lr = lr\n",
    "        cost_vector = []\n",
    "        prediction_cost_vector = []\n",
    "        classification_cost_vector = []\n",
    "        acc_vector = []\n",
    "        valid_acc_vector = []\n",
    "        vali_cost_vector = []\n",
    "        train_score = []\n",
    "        train_label = []\n",
    "        for i, (train_data, train_labels, type_labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_data = to_var(train_data)\n",
    "            train_labels = to_var(train_labels),\n",
    "            type_labels = to_var(type_labels)\n",
    "            link_outputs, type_outputs = model(train_data.unsqueeze(1))\n",
    "            train_score += list(link_outputs[:, 1].cpu().detach().numpy())\n",
    "            train_label += list(train_labels[0].numpy())\n",
    "            train_labels = train_labels[0]\n",
    "            train_labels = train_labels.long()\n",
    "            type_labels = type_labels.long()\n",
    "            prediction_loss = criterion(link_outputs, train_labels)\n",
    "            classification_loss = criterion(type_outputs, type_labels)\n",
    "            loss = prediction_loss + classification_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(link_outputs, 1)\n",
    "            accuracy = (train_labels == argmax.squeeze()).float().mean()\n",
    "            prediction_cost_vector.append(prediction_loss.item())\n",
    "            classification_cost_vector.append(classification_loss.item())\n",
    "            cost_vector.append(loss.item())\n",
    "            acc_vector.append(accuracy.item())\n",
    "\n",
    "        # validate process\n",
    "        model.eval()\n",
    "        validate_acc_vector_temp = []\n",
    "        for i, (validate_data, validate_labels, type_labels) in enumerate(validate_loader):\n",
    "            validate_data = to_var(validate_data)\n",
    "            validate_labels = to_var(validate_labels)\n",
    "            type_labels = to_var(type_labels)\n",
    "            validate_outputs, type_outputs = model(validate_data.unsqueeze(1))\n",
    "            _, validate_argmax = torch.max(validate_outputs, 1)\n",
    "            validate_labels = validate_labels.long()\n",
    "            vali_loss = criterion(validate_outputs, validate_labels)\n",
    "            validate_accuracy = (validate_labels == validate_argmax.squeeze()).float().mean()\n",
    "            vali_cost_vector.append(vali_loss.item())\n",
    "            validate_acc_vector_temp.append(validate_accuracy.item())\n",
    "        validate_acc = np.mean(validate_acc_vector_temp)\n",
    "        valid_acc_vector.append(validate_acc)\n",
    "        model.train()\n",
    "        print('Epoch [%d/%d],  Loss: %.4f, Link Prediction Loss: %.4f, Type Classification Loss: %.4f, Train_Acc: %.4f,  Validate_Acc: %.4f.'\n",
    "              % (epoch + 1, num_epochs, np.mean(cost_vector), np.mean(prediction_cost_vector), np.mean(classification_cost_vector),\n",
    "                 np.mean(acc_vector), validate_acc))\n",
    "\n",
    "        if validate_acc > best_validate_acc:\n",
    "            best_validate_acc = validate_acc\n",
    "            if not os.path.exists(output_file):\n",
    "                os.mkdir(output_file)\n",
    "            best_validate_dir = output_file + str(epoch + 1) + '.pkl'\n",
    "            torch.save(model.state_dict(), best_validate_dir)\n",
    "    return best_validate_dir\n",
    "\n",
    "def test_adversarial_neural_networks(best_validate_dir, test_loader, model):\n",
    "    # Test the Model\n",
    "    print('testing model')\n",
    "    model.load_state_dict(torch.load(best_validate_dir))\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    test_score = []\n",
    "    test_pred = []\n",
    "    test_true = []\n",
    "    tes_score = []\n",
    "    tes_label = []\n",
    "\n",
    "    for i, (test_data, test_labels, type_labels) in enumerate(test_loader):\n",
    "        test_data = to_var(test_data)\n",
    "        test_labels = to_var(test_labels)\n",
    "        # type_labels = to_var(type_labels)\n",
    "        test_outputs, type_outputs = model(test_data.unsqueeze(1))\n",
    "        tes_score += list(test_outputs[:, 1].cpu().detach().numpy())\n",
    "        tes_label += list(test_labels.numpy())\n",
    "        _, test_argmax = torch.max(test_outputs, 1)\n",
    "        if i == 0:\n",
    "            test_score = to_np(test_outputs)\n",
    "            test_pred = to_np(test_argmax)\n",
    "            test_true = to_np(test_labels)\n",
    "        else:\n",
    "            test_score = np.concatenate((test_score, to_np(test_outputs)), axis=0)\n",
    "            test_pred = np.concatenate((test_pred, to_np(test_argmax)), axis=0)\n",
    "            test_true = np.concatenate((test_true, to_np(test_labels)), axis=0)\n",
    "\n",
    "    test_accuracy = metrics.accuracy_score(test_true, test_pred)\n",
    "    test_precision = metrics.precision_score(test_true, test_pred, average='macro')\n",
    "    test_aucroc = metrics.roc_auc_score(tes_label, tes_score, average='macro')\n",
    "\n",
    "    return test_aucroc, test_precision, test_accuracy\n",
    "\n",
    "\n",
    "class TabuSearchOptimizer:\n",
    "    def __init__(self, max_iters, tabu_tenure, neighborhood_size):\n",
    "        self.max_iters = max_iters\n",
    "        self.tabu_list = []\n",
    "        self.tabu_tenure = tabu_tenure\n",
    "        self.neighborhood_size = neighborhood_size\n",
    "\n",
    "    def optimize(self, objective_func, initial_solution):\n",
    "        current_solution = initial_solution\n",
    "        best_solution = initial_solution\n",
    "        best_score = objective_func(current_solution)\n",
    "        for _ in range(self.max_iters):\n",
    "            neighborhood = self.generate_neighborhood(current_solution)\n",
    "            neighborhood = [sol for sol in neighborhood if sol not in self.tabu_list]\n",
    "            if not neighborhood:\n",
    "                continue\n",
    "            scores = [(sol, objective_func(sol)) for sol in neighborhood]\n",
    "            best_candidate, best_candidate_score = min(scores, key=lambda x: x[1])\n",
    "\n",
    "            if best_candidate_score < best_score:\n",
    "                best_solution = best_candidate\n",
    "                best_score = best_candidate_score\n",
    "            self.update_tabu_list(best_candidate)\n",
    "            current_solution = best_candidate\n",
    "        return best_solution\n",
    "\n",
    "    def generate_neighborhood(self, solution):\n",
    "        neighbors = []\n",
    "        for _ in range(self.neighborhood_size):\n",
    "            new_solution = solution.copy()\n",
    "            new_solution['num_epochs'] = random.randint(50, 300)\n",
    "            new_solution['batch_size'] = random.choice([16, 32, 64])\n",
    "            new_solution['learning_rate'] = round(random.uniform(0.0001, 0.01), 5)\n",
    "            new_solution['hidden_dim'] = random.choice([8, 16, 32, 64])\n",
    "            neighbors.append(new_solution)\n",
    "        return neighbors\n",
    "\n",
    "    def update_tabu_list(self, solution):\n",
    "        self.tabu_list.append(solution)\n",
    "        if len(self.tabu_list) > self.tabu_tenure:\n",
    "            self.tabu_list.pop(0)\n",
    "\n",
    "# Your existing classes and functions remain unchanged...\n",
    "\n",
    "def evaluate_hyperparameters(params):\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    hidden_dim = params['hidden_dim']\n",
    "    \n",
    "    train_loader, validate_loader, test_loader = get_train_validate_test('Facebook')\n",
    "    model = adversarial_neural_networks(4)\n",
    "    best_validate_dir = train_adversarial_neural_networks(train_loader, validate_loader, model, 'trainOutput/facebook/output.txt')\n",
    "    auc, precision, accuracy = test_adversarial_neural_networks(best_validate_dir, test_loader, model)\n",
    "    \n",
    "    return 1 - accuracy  # Objective is to minimize error (maximize accuracy)\n",
    "\n",
    "# def main(predicted_Type, dataset, output_file):\n",
    "#     train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "#     model = adversarial_neural_networks(predicted_Type)\n",
    "#     best_validate_dir = train_adversarial_neural_networks(train_loader, validate_loader, model, output_file)\n",
    "#     auc, precision, accuracy = test_adversarial_neural_networks(best_validate_dir, test_loader, model)\n",
    "#     print(\"Final reault: AUC -- %.4f  \" % (auc), \"Precision -- %.4f  \" % (precision), 'Accuracy -- %.4f  ' % (accuracy))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     datasets = ['Facebook', 'IMDB', 'YELP', 'DBLP']\n",
    "#     dataset = datasets[0]\n",
    "#     print('Input dataset is:', dataset)\n",
    "#     predicted_Type_datasets = {'Facebook': 4, 'IMDB': 2, 'YELP': 2, 'DBLP': 2}\n",
    "#     predicted_Type = predicted_Type_datasets[dataset]\n",
    "#     output_file = 'trainOutput/' + dataset.lower() + '/output.txt'\n",
    "#     main(predicted_Type, dataset, output_file)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    initial_solution = {\n",
    "        'num_epochs': 200,\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.01,\n",
    "        'hidden_dim': 16\n",
    "    }\n",
    "    \n",
    "    tabu_search = TabuSearchOptimizer(max_iters=30, tabu_tenure=5, neighborhood_size=5)\n",
    "    optimal_solution = tabu_search.optimize(evaluate_hyperparameters, initial_solution)\n",
    "    print(\"Optimal Hyperparameters Found:\", optimal_solution)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7134399-408d-44ea-b4a4-d3b4f5baa050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
