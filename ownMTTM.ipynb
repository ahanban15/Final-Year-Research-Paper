{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9e0f8b6-3196-464f-aef9-0d17ae205ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Facebook\n",
      "leftNode: 2 rightNode: 126\n",
      "leftNode: 3 rightNode: 293\n",
      "leftNode: 4 rightNode: 2004\n",
      "leftNode: 4 rightNode: 1182\n",
      "leftNode: 4 rightNode: 1443\n",
      "leftNode: 4 rightNode: 187\n",
      "leftNode: 4 rightNode: 1489\n",
      "leftNode: 4 rightNode: 1882\n",
      "leftNode: 4 rightNode: 2282\n",
      "leftNode: 4 rightNode: 1879\n",
      "leftNode: 4 rightNode: 1377\n",
      "leftNode: 4 rightNode: 1997\n",
      "leftNode: 5 rightNode: 1840\n",
      "leftNode: 5 rightNode: 945\n",
      "leftNode: 6 rightNode: 1193\n",
      "leftNode: 6 rightNode: 290\n",
      "leftNode: 10 rightNode: 14\n",
      "leftNode: 13 rightNode: 1744\n",
      "leftNode: 13 rightNode: 395\n",
      "leftNode: 13 rightNode: 2280\n",
      "leftNode: 13 rightNode: 213\n",
      "leftNode: 13 rightNode: 501\n",
      "leftNode: 13 rightNode: 1494\n",
      "leftNode: 14 rightNode: 1041\n",
      "leftNode: 14 rightNode: 1187\n",
      "leftNode: 14 rightNode: 2075\n",
      "leftNode: 14 rightNode: 1943\n",
      "leftNode: 14 rightNode: 2327\n",
      "leftNode: 15 rightNode: 353\n",
      "leftNode: 17 rightNode: 1626\n",
      "leftNode: 18 rightNode: 2224\n",
      "leftNode: 18 rightNode: 961\n",
      "leftNode: 18 rightNode: 1209\n",
      "leftNode: 19 rightNode: 330\n",
      "leftNode: 19 rightNode: 1773\n",
      "leftNode: 19 rightNode: 701\n",
      "leftNode: 20 rightNode: 364\n",
      "leftNode: 22 rightNode: 897\n",
      "leftNode: 22 rightNode: 481\n",
      "leftNode: 22 rightNode: 1189\n",
      "leftNode: 31 rightNode: 1722\n",
      "leftNode: 32 rightNode: 1703\n",
      "leftNode: 32 rightNode: 1783\n",
      "leftNode: 32 rightNode: 120\n",
      "leftNode: 32 rightNode: 2157\n",
      "leftNode: 32 rightNode: 1980\n",
      "leftNode: 32 rightNode: 1024\n",
      "leftNode: 36 rightNode: 1293\n",
      "leftNode: 38 rightNode: 2124\n",
      "leftNode: 38 rightNode: 1381\n",
      "leftNode: 38 rightNode: 783\n",
      "leftNode: 39 rightNode: 1176\n",
      "leftNode: 41 rightNode: 1200\n",
      "leftNode: 41 rightNode: 922\n",
      "leftNode: 41 rightNode: 45\n",
      "leftNode: 42 rightNode: 909\n",
      "leftNode: 43 rightNode: 1273\n",
      "leftNode: 43 rightNode: 1750\n",
      "leftNode: 43 rightNode: 188\n",
      "leftNode: 44 rightNode: 1126\n",
      "leftNode: 44 rightNode: 568\n",
      "leftNode: 44 rightNode: 839\n",
      "leftNode: 44 rightNode: 1618\n",
      "leftNode: 44 rightNode: 2160\n",
      "leftNode: 44 rightNode: 1517\n",
      "leftNode: 45 rightNode: 1713\n",
      "leftNode: 46 rightNode: 2252\n",
      "leftNode: 46 rightNode: 365\n",
      "leftNode: 47 rightNode: 1125\n",
      "leftNode: 47 rightNode: 630\n",
      "leftNode: 47 rightNode: 1503\n",
      "leftNode: 49 rightNode: 655\n",
      "leftNode: 49 rightNode: 1274\n",
      "leftNode: 49 rightNode: 280\n",
      "leftNode: 49 rightNode: 701\n",
      "leftNode: 50 rightNode: 197\n",
      "leftNode: 55 rightNode: 2316\n",
      "leftNode: 55 rightNode: 2348\n",
      "leftNode: 56 rightNode: 496\n",
      "leftNode: 56 rightNode: 1339\n",
      "leftNode: 56 rightNode: 1635\n",
      "leftNode: 56 rightNode: 723\n",
      "leftNode: 57 rightNode: 2117\n",
      "leftNode: 60 rightNode: 1193\n",
      "leftNode: 60 rightNode: 290\n",
      "leftNode: 61 rightNode: 1922\n",
      "leftNode: 61 rightNode: 1769\n",
      "leftNode: 61 rightNode: 1521\n",
      "leftNode: 61 rightNode: 1544\n",
      "leftNode: 61 rightNode: 348\n",
      "leftNode: 61 rightNode: 1728\n",
      "leftNode: 61 rightNode: 785\n",
      "leftNode: 61 rightNode: 2252\n",
      "leftNode: 65 rightNode: 872\n",
      "leftNode: 65 rightNode: 789\n",
      "leftNode: 65 rightNode: 701\n",
      "leftNode: 66 rightNode: 1679\n",
      "leftNode: 66 rightNode: 2216\n",
      "leftNode: 67 rightNode: 791\n",
      "leftNode: 68 rightNode: 366\n",
      "leftNode: 69 rightNode: 526\n",
      "leftNode: 69 rightNode: 959\n",
      "leftNode: 70 rightNode: 924\n",
      "leftNode: 72 rightNode: 1901\n",
      "leftNode: 72 rightNode: 1381\n",
      "leftNode: 72 rightNode: 783\n",
      "leftNode: 73 rightNode: 239\n",
      "leftNode: 73 rightNode: 580\n",
      "leftNode: 73 rightNode: 1621\n",
      "leftNode: 74 rightNode: 2359\n",
      "leftNode: 74 rightNode: 1327\n",
      "leftNode: 77 rightNode: 1597\n",
      "leftNode: 77 rightNode: 2270\n",
      "leftNode: 79 rightNode: 410\n",
      "leftNode: 81 rightNode: 486\n",
      "leftNode: 81 rightNode: 2232\n",
      "leftNode: 81 rightNode: 626\n",
      "leftNode: 82 rightNode: 1142\n",
      "leftNode: 85 rightNode: 1586\n",
      "leftNode: 86 rightNode: 1129\n",
      "leftNode: 88 rightNode: 840\n",
      "leftNode: 88 rightNode: 1047\n",
      "leftNode: 88 rightNode: 679\n",
      "leftNode: 88 rightNode: 1271\n",
      "leftNode: 89 rightNode: 543\n",
      "leftNode: 90 rightNode: 1123\n",
      "leftNode: 91 rightNode: 1660\n",
      "leftNode: 92 rightNode: 1225\n",
      "leftNode: 94 rightNode: 767\n",
      "leftNode: 94 rightNode: 1901\n",
      "leftNode: 94 rightNode: 1372\n",
      "leftNode: 94 rightNode: 1047\n",
      "leftNode: 95 rightNode: 1357\n",
      "leftNode: 95 rightNode: 1181\n",
      "leftNode: 95 rightNode: 1401\n",
      "leftNode: 95 rightNode: 1689\n",
      "leftNode: 98 rightNode: 1282\n",
      "leftNode: 98 rightNode: 1283\n",
      "leftNode: 98 rightNode: 729\n",
      "leftNode: 99 rightNode: 2051\n",
      "leftNode: 99 rightNode: 1789\n",
      "leftNode: 99 rightNode: 216\n",
      "leftNode: 100 rightNode: 1503\n",
      "leftNode: 101 rightNode: 787\n",
      "leftNode: 101 rightNode: 25\n",
      "leftNode: 101 rightNode: 410\n",
      "leftNode: 101 rightNode: 1618\n",
      "leftNode: 101 rightNode: 1517\n",
      "leftNode: 101 rightNode: 1558\n",
      "leftNode: 102 rightNode: 1766\n",
      "leftNode: 102 rightNode: 2153\n",
      "leftNode: 103 rightNode: 1136\n",
      "leftNode: 103 rightNode: 827\n",
      "leftNode: 104 rightNode: 589\n",
      "leftNode: 104 rightNode: 1722\n",
      "leftNode: 105 rightNode: 1640\n",
      "leftNode: 106 rightNode: 1602\n",
      "leftNode: 106 rightNode: 187\n",
      "leftNode: 106 rightNode: 2208\n",
      "leftNode: 106 rightNode: 2282\n",
      "leftNode: 106 rightNode: 1377\n",
      "leftNode: 107 rightNode: 467\n",
      "leftNode: 108 rightNode: 469\n",
      "leftNode: 109 rightNode: 511\n",
      "leftNode: 110 rightNode: 701\n",
      "leftNode: 111 rightNode: 1302\n",
      "leftNode: 114 rightNode: 1705\n",
      "leftNode: 114 rightNode: 364\n",
      "leftNode: 120 rightNode: 1783\n",
      "leftNode: 123 rightNode: 375\n",
      "leftNode: 123 rightNode: 130\n",
      "leftNode: 123 rightNode: 343\n",
      "leftNode: 126 rightNode: 1533\n",
      "leftNode: 127 rightNode: 917\n",
      "leftNode: 127 rightNode: 356\n",
      "leftNode: 127 rightNode: 405\n",
      "leftNode: 127 rightNode: 2311\n",
      "leftNode: 127 rightNode: 770\n",
      "leftNode: 127 rightNode: 1639\n",
      "leftNode: 127 rightNode: 679\n",
      "leftNode: 127 rightNode: 1123\n",
      "leftNode: 128 rightNode: 682\n",
      "leftNode: 128 rightNode: 2119\n",
      "leftNode: 128 rightNode: 2080\n",
      "leftNode: 128 rightNode: 1156\n",
      "leftNode: 128 rightNode: 641\n",
      "leftNode: 128 rightNode: 672\n",
      "leftNode: 128 rightNode: 142\n",
      "leftNode: 129 rightNode: 1035\n",
      "leftNode: 129 rightNode: 1339\n",
      "leftNode: 129 rightNode: 1860\n",
      "leftNode: 130 rightNode: 375\n",
      "leftNode: 130 rightNode: 1617\n",
      "leftNode: 131 rightNode: 306\n",
      "leftNode: 132 rightNode: 264\n",
      "leftNode: 133 rightNode: 1901\n",
      "leftNode: 133 rightNode: 715\n",
      "leftNode: 133 rightNode: 1087\n",
      "leftNode: 133 rightNode: 1979\n",
      "leftNode: 133 rightNode: 1053\n",
      "leftNode: 134 rightNode: 555\n",
      "leftNode: 134 rightNode: 2144\n",
      "leftNode: 136 rightNode: 2170\n",
      "leftNode: 136 rightNode: 1964\n",
      "leftNode: 138 rightNode: 1293\n",
      "leftNode: 138 rightNode: 1962\n",
      "leftNode: 138 rightNode: 51\n",
      "leftNode: 138 rightNode: 554\n",
      "leftNode: 138 rightNode: 968\n",
      "leftNode: 139 rightNode: 2053\n",
      "leftNode: 139 rightNode: 602\n",
      "leftNode: 140 rightNode: 1011\n",
      "leftNode: 141 rightNode: 1211\n",
      "leftNode: 141 rightNode: 233\n",
      "leftNode: 142 rightNode: 682\n",
      "leftNode: 142 rightNode: 2119\n",
      "leftNode: 142 rightNode: 2080\n",
      "leftNode: 142 rightNode: 1156\n",
      "leftNode: 142 rightNode: 672\n",
      "leftNode: 147 rightNode: 1658\n",
      "leftNode: 149 rightNode: 248\n",
      "leftNode: 149 rightNode: 481\n",
      "leftNode: 151 rightNode: 526\n",
      "leftNode: 151 rightNode: 959\n",
      "leftNode: 154 rightNode: 729\n",
      "leftNode: 154 rightNode: 1054\n",
      "leftNode: 154 rightNode: 701\n",
      "leftNode: 157 rightNode: 1827\n",
      "leftNode: 157 rightNode: 1701\n",
      "leftNode: 159 rightNode: 961\n",
      "leftNode: 159 rightNode: 983\n",
      "leftNode: 159 rightNode: 159\n",
      "leftNode: 160 rightNode: 170\n",
      "leftNode: 160 rightNode: 386\n",
      "leftNode: 162 rightNode: 325\n",
      "leftNode: 163 rightNode: 822\n",
      "leftNode: 164 rightNode: 421\n",
      "leftNode: 164 rightNode: 971\n",
      "leftNode: 164 rightNode: 349\n",
      "leftNode: 164 rightNode: 1551\n",
      "leftNode: 165 rightNode: 2271\n",
      "leftNode: 173 rightNode: 1169\n",
      "leftNode: 173 rightNode: 710\n",
      "leftNode: 173 rightNode: 1817\n",
      "leftNode: 173 rightNode: 1094\n",
      "leftNode: 173 rightNode: 1078\n",
      "leftNode: 173 rightNode: 2242\n",
      "leftNode: 173 rightNode: 522\n",
      "leftNode: 176 rightNode: 1978\n",
      "leftNode: 176 rightNode: 1627\n",
      "leftNode: 176 rightNode: 1101\n",
      "leftNode: 176 rightNode: 894\n",
      "leftNode: 176 rightNode: 1985\n",
      "leftNode: 182 rightNode: 1912\n",
      "leftNode: 183 rightNode: 300\n",
      "leftNode: 184 rightNode: 481\n",
      "leftNode: 185 rightNode: 1886\n",
      "leftNode: 185 rightNode: 1148\n",
      "leftNode: 186 rightNode: 1168\n",
      "leftNode: 186 rightNode: 827\n",
      "leftNode: 187 rightNode: 2004\n",
      "leftNode: 187 rightNode: 1691\n",
      "leftNode: 187 rightNode: 811\n",
      "leftNode: 187 rightNode: 2208\n",
      "leftNode: 187 rightNode: 2282\n",
      "leftNode: 187 rightNode: 2378\n",
      "leftNode: 187 rightNode: 1531\n",
      "leftNode: 187 rightNode: 1443\n",
      "leftNode: 187 rightNode: 2058\n",
      "leftNode: 187 rightNode: 1602\n",
      "leftNode: 187 rightNode: 1882\n",
      "leftNode: 187 rightNode: 1377\n",
      "leftNode: 187 rightNode: 1997\n",
      "leftNode: 188 rightNode: 1750\n",
      "leftNode: 188 rightNode: 1273\n",
      "leftNode: 191 rightNode: 1483\n",
      "leftNode: 191 rightNode: 1834\n",
      "leftNode: 191 rightNode: 302\n",
      "leftNode: 191 rightNode: 2089\n",
      "leftNode: 193 rightNode: 1370\n",
      "leftNode: 196 rightNode: 1960\n",
      "leftNode: 196 rightNode: 659\n",
      "leftNode: 196 rightNode: 1503\n",
      "leftNode: 196 rightNode: 478\n",
      "leftNode: 197 rightNode: 1025\n",
      "leftNode: 197 rightNode: 614\n",
      "leftNode: 197 rightNode: 2186\n",
      "leftNode: 197 rightNode: 1914\n",
      "leftNode: 197 rightNode: 1683\n",
      "leftNode: 197 rightNode: 2078\n",
      "leftNode: 197 rightNode: 701\n",
      "leftNode: 197 rightNode: 1068\n",
      "leftNode: 197 rightNode: 1330\n",
      "leftNode: 197 rightNode: 496\n",
      "leftNode: 197 rightNode: 927\n",
      "leftNode: 197 rightNode: 338\n",
      "leftNode: 197 rightNode: 863\n",
      "leftNode: 198 rightNode: 759\n",
      "leftNode: 198 rightNode: 2365\n",
      "leftNode: 199 rightNode: 2169\n",
      "leftNode: 199 rightNode: 680\n",
      "leftNode: 201 rightNode: 1863\n",
      "leftNode: 203 rightNode: 212\n",
      "leftNode: 203 rightNode: 1582\n",
      "leftNode: 204 rightNode: 2070\n",
      "leftNode: 205 rightNode: 789\n",
      "leftNode: 205 rightNode: 227\n",
      "leftNode: 205 rightNode: 970\n",
      "leftNode: 205 rightNode: 649\n",
      "leftNode: 206 rightNode: 1783\n",
      "leftNode: 210 rightNode: 1193\n",
      "leftNode: 210 rightNode: 290\n",
      "leftNode: 211 rightNode: 511\n",
      "leftNode: 212 rightNode: 2181\n",
      "leftNode: 212 rightNode: 1967\n",
      "leftNode: 212 rightNode: 2344\n",
      "leftNode: 212 rightNode: 1582\n",
      "leftNode: 212 rightNode: 1236\n",
      "leftNode: 212 rightNode: 2286\n",
      "leftNode: 212 rightNode: 1422\n",
      "leftNode: 212 rightNode: 647\n",
      "leftNode: 212 rightNode: 837\n",
      "leftNode: 212 rightNode: 365\n",
      "leftNode: 212 rightNode: 1270\n",
      "leftNode: 212 rightNode: 1124\n",
      "leftNode: 213 rightNode: 1744\n",
      "leftNode: 213 rightNode: 2280\n",
      "leftNode: 213 rightNode: 501\n",
      "leftNode: 213 rightNode: 1494\n",
      "leftNode: 214 rightNode: 1244\n",
      "leftNode: 216 rightNode: 369\n",
      "leftNode: 216 rightNode: 362\n",
      "leftNode: 218 rightNode: 1636\n",
      "leftNode: 220 rightNode: 217\n",
      "leftNode: 220 rightNode: 1326\n",
      "leftNode: 221 rightNode: 503\n",
      "leftNode: 221 rightNode: 701\n",
      "leftNode: 221 rightNode: 2047\n",
      "leftNode: 222 rightNode: 1912\n",
      "leftNode: 222 rightNode: 406\n",
      "leftNode: 224 rightNode: 641\n",
      "leftNode: 226 rightNode: 2056\n",
      "leftNode: 226 rightNode: 1786\n",
      "leftNode: 226 rightNode: 1929\n",
      "leftNode: 227 rightNode: 493\n",
      "leftNode: 227 rightNode: 2325\n",
      "leftNode: 228 rightNode: 1654\n",
      "leftNode: 229 rightNode: 1499\n",
      "leftNode: 230 rightNode: 2278\n",
      "leftNode: 230 rightNode: 1953\n",
      "leftNode: 231 rightNode: 920\n",
      "leftNode: 233 rightNode: 1211\n",
      "leftNode: 235 rightNode: 1355\n",
      "leftNode: 238 rightNode: 1078\n",
      "leftNode: 238 rightNode: 710\n",
      "leftNode: 238 rightNode: 2366\n",
      "leftNode: 238 rightNode: 2242\n",
      "leftNode: 238 rightNode: 522\n",
      "leftNode: 239 rightNode: 1621\n",
      "leftNode: 1439 rightNode: 1032\n",
      "leftNode: 242 rightNode: 2174\n",
      "leftNode: 246 rightNode: 1872\n",
      "leftNode: 246 rightNode: 1599\n",
      "leftNode: 247 rightNode: 1878\n",
      "leftNode: 248 rightNode: 1052\n",
      "leftNode: 248 rightNode: 481\n",
      "leftNode: 250 rightNode: 519\n",
      "leftNode: 250 rightNode: 1175\n",
      "leftNode: 250 rightNode: 250\n",
      "leftNode: 251 rightNode: 603\n",
      "leftNode: 253 rightNode: 378\n",
      "leftNode: 254 rightNode: 1745\n",
      "leftNode: 254 rightNode: 1921\n",
      "leftNode: 254 rightNode: 1123\n",
      "leftNode: 254 rightNode: 977\n",
      "leftNode: 255 rightNode: 1519\n",
      "leftNode: 256 rightNode: 1303\n",
      "leftNode: 257 rightNode: 956\n",
      "leftNode: 257 rightNode: 1459\n",
      "leftNode: 261 rightNode: 963\n",
      "leftNode: 262 rightNode: 262\n",
      "leftNode: 262 rightNode: 1989\n",
      "leftNode: 267 rightNode: 267\n",
      "leftNode: 268 rightNode: 1337\n",
      "leftNode: 268 rightNode: 2232\n",
      "leftNode: 270 rightNode: 1745\n",
      "leftNode: 270 rightNode: 2311\n",
      "leftNode: 275 rightNode: 2390\n",
      "leftNode: 276 rightNode: 1694\n",
      "leftNode: 277 rightNode: 935\n",
      "leftNode: 278 rightNode: 1095\n",
      "leftNode: 278 rightNode: 909\n",
      "leftNode: 278 rightNode: 2112\n",
      "leftNode: 280 rightNode: 1274\n",
      "leftNode: 281 rightNode: 1642\n",
      "leftNode: 283 rightNode: 2134\n",
      "leftNode: 283 rightNode: 1008\n",
      "leftNode: 283 rightNode: 1850\n",
      "leftNode: 283 rightNode: 1318\n",
      "leftNode: 283 rightNode: 502\n",
      "leftNode: 283 rightNode: 2019\n",
      "leftNode: 283 rightNode: 2131\n",
      "leftNode: 285 rightNode: 1370\n",
      "leftNode: 285 rightNode: 1244\n",
      "leftNode: 287 rightNode: 516\n",
      "leftNode: 287 rightNode: 654\n",
      "leftNode: 287 rightNode: 871\n",
      "leftNode: 288 rightNode: 1281\n",
      "leftNode: 289 rightNode: 2220\n",
      "leftNode: 289 rightNode: 799\n",
      "leftNode: 289 rightNode: 1553\n",
      "leftNode: 289 rightNode: 687\n",
      "leftNode: 290 rightNode: 2104\n",
      "leftNode: 290 rightNode: 722\n",
      "leftNode: 290 rightNode: 1937\n",
      "leftNode: 290 rightNode: 729\n",
      "leftNode: 290 rightNode: 562\n",
      "leftNode: 290 rightNode: 1077\n",
      "leftNode: 290 rightNode: 1089\n",
      "leftNode: 290 rightNode: 1282\n",
      "leftNode: 290 rightNode: 2023\n",
      "leftNode: 290 rightNode: 2374\n",
      "leftNode: 290 rightNode: 1692\n",
      "leftNode: 290 rightNode: 1193\n",
      "leftNode: 290 rightNode: 685\n",
      "leftNode: 290 rightNode: 2238\n",
      "leftNode: 290 rightNode: 2025\n",
      "leftNode: 290 rightNode: 702\n",
      "leftNode: 296 rightNode: 1998\n",
      "leftNode: 296 rightNode: 1827\n",
      "leftNode: 296 rightNode: 1959\n",
      "leftNode: 296 rightNode: 560\n",
      "leftNode: 297 rightNode: 1402\n",
      "leftNode: 299 rightNode: 430\n",
      "leftNode: 302 rightNode: 1234\n",
      "leftNode: 302 rightNode: 1483\n",
      "leftNode: 304 rightNode: 1480\n",
      "leftNode: 304 rightNode: 1078\n",
      "leftNode: 304 rightNode: 1866\n",
      "leftNode: 304 rightNode: 2058\n",
      "leftNode: 304 rightNode: 1936\n",
      "leftNode: 304 rightNode: 2040\n",
      "leftNode: 304 rightNode: 1595\n",
      "leftNode: 305 rightNode: 1780\n",
      "leftNode: 305 rightNode: 1969\n",
      "leftNode: 305 rightNode: 2280\n",
      "leftNode: 305 rightNode: 660\n",
      "leftNode: 305 rightNode: 1531\n",
      "leftNode: 305 rightNode: 505\n",
      "leftNode: 305 rightNode: 1494\n",
      "leftNode: 306 rightNode: 463\n",
      "leftNode: 306 rightNode: 2057\n",
      "leftNode: 306 rightNode: 2391\n",
      "leftNode: 306 rightNode: 2071\n",
      "leftNode: 306 rightNode: 351\n",
      "leftNode: 307 rightNode: 1740\n",
      "leftNode: 307 rightNode: 1312\n",
      "leftNode: 307 rightNode: 2331\n",
      "leftNode: 307 rightNode: 1282\n",
      "leftNode: 307 rightNode: 904\n",
      "leftNode: 307 rightNode: 1205\n",
      "leftNode: 307 rightNode: 1387\n",
      "leftNode: 309 rightNode: 2018\n",
      "leftNode: 309 rightNode: 1466\n",
      "leftNode: 317 rightNode: 572\n",
      "leftNode: 317 rightNode: 601\n",
      "leftNode: 318 rightNode: 914\n",
      "leftNode: 321 rightNode: 2043\n",
      "leftNode: 321 rightNode: 1275\n",
      "leftNode: 321 rightNode: 1037\n",
      "leftNode: 321 rightNode: 1796\n",
      "leftNode: 321 rightNode: 1668\n",
      "leftNode: 321 rightNode: 1763\n",
      "leftNode: 323 rightNode: 2272\n",
      "leftNode: 323 rightNode: 1068\n",
      "leftNode: 325 rightNode: 999\n",
      "leftNode: 325 rightNode: 544\n",
      "leftNode: 325 rightNode: 2362\n",
      "leftNode: 325 rightNode: 2323\n",
      "leftNode: 325 rightNode: 358\n",
      "leftNode: 325 rightNode: 2213\n",
      "leftNode: 325 rightNode: 701\n",
      "leftNode: 326 rightNode: 1690\n",
      "leftNode: 326 rightNode: 1351\n",
      "leftNode: 326 rightNode: 463\n",
      "leftNode: 326 rightNode: 904\n",
      "leftNode: 329 rightNode: 2142\n",
      "leftNode: 329 rightNode: 339\n",
      "leftNode: 330 rightNode: 1230\n",
      "leftNode: 336 rightNode: 881\n",
      "leftNode: 337 rightNode: 1330\n",
      "leftNode: 337 rightNode: 2162\n",
      "leftNode: 337 rightNode: 496\n",
      "leftNode: 338 rightNode: 1914\n",
      "leftNode: 338 rightNode: 2186\n",
      "leftNode: 339 rightNode: 524\n",
      "leftNode: 339 rightNode: 1387\n",
      "leftNode: 339 rightNode: 405\n",
      "leftNode: 339 rightNode: 1639\n",
      "leftNode: 339 rightNode: 2085\n",
      "leftNode: 339 rightNode: 2142\n",
      "leftNode: 340 rightNode: 563\n",
      "leftNode: 341 rightNode: 1960\n",
      "leftNode: 341 rightNode: 478\n",
      "leftNode: 341 rightNode: 992\n",
      "leftNode: 342 rightNode: 2134\n",
      "leftNode: 342 rightNode: 738\n",
      "leftNode: 342 rightNode: 417\n",
      "leftNode: 343 rightNode: 375\n",
      "leftNode: 343 rightNode: 1617\n",
      "leftNode: 344 rightNode: 25\n",
      "leftNode: 345 rightNode: 2059\n",
      "leftNode: 346 rightNode: 859\n",
      "leftNode: 346 rightNode: 829\n",
      "leftNode: 346 rightNode: 1437\n",
      "leftNode: 347 rightNode: 916\n",
      "leftNode: 347 rightNode: 590\n",
      "leftNode: 347 rightNode: 1063\n",
      "leftNode: 347 rightNode: 1710\n",
      "leftNode: 349 rightNode: 2094\n",
      "leftNode: 349 rightNode: 1059\n",
      "leftNode: 350 rightNode: 1316\n",
      "leftNode: 350 rightNode: 1827\n",
      "leftNode: 350 rightNode: 58\n",
      "leftNode: 352 rightNode: 2306\n",
      "leftNode: 352 rightNode: 2400\n",
      "leftNode: 356 rightNode: 1360\n",
      "leftNode: 356 rightNode: 591\n",
      "leftNode: 356 rightNode: 2085\n",
      "leftNode: 359 rightNode: 1300\n",
      "leftNode: 360 rightNode: 1939\n",
      "leftNode: 361 rightNode: 1111\n",
      "leftNode: 364 rightNode: 1293\n",
      "leftNode: 364 rightNode: 448\n",
      "leftNode: 364 rightNode: 709\n",
      "leftNode: 364 rightNode: 2109\n",
      "leftNode: 364 rightNode: 1490\n",
      "leftNode: 364 rightNode: 992\n",
      "leftNode: 364 rightNode: 1328\n",
      "leftNode: 364 rightNode: 2055\n",
      "leftNode: 364 rightNode: 760\n",
      "leftNode: 364 rightNode: 1121\n",
      "leftNode: 364 rightNode: 1454\n",
      "leftNode: 364 rightNode: 1705\n",
      "leftNode: 364 rightNode: 1559\n",
      "leftNode: 365 rightNode: 2335\n",
      "leftNode: 365 rightNode: 1541\n",
      "leftNode: 365 rightNode: 572\n",
      "leftNode: 365 rightNode: 1270\n",
      "leftNode: 365 rightNode: 443\n",
      "leftNode: 365 rightNode: 2042\n",
      "leftNode: 365 rightNode: 2252\n",
      "leftNode: 367 rightNode: 2337\n",
      "leftNode: 367 rightNode: 1803\n",
      "leftNode: 368 rightNode: 1387\n",
      "leftNode: 368 rightNode: 1025\n",
      "leftNode: 368 rightNode: 614\n",
      "leftNode: 368 rightNode: 2272\n",
      "leftNode: 368 rightNode: 1959\n",
      "leftNode: 368 rightNode: 2297\n",
      "leftNode: 369 rightNode: 2278\n",
      "leftNode: 369 rightNode: 2159\n",
      "leftNode: 370 rightNode: 458\n",
      "leftNode: 371 rightNode: 1535\n",
      "leftNode: 375 rightNode: 2294\n",
      "leftNode: 375 rightNode: 1390\n",
      "leftNode: 375 rightNode: 1617\n",
      "leftNode: 375 rightNode: 1946\n",
      "leftNode: 1042 rightNode: 1469\n",
      "leftNode: 377 rightNode: 983\n",
      "leftNode: 377 rightNode: 754\n",
      "leftNode: 377 rightNode: 2287\n",
      "leftNode: 377 rightNode: 2245\n",
      "leftNode: 377 rightNode: 1123\n",
      "leftNode: 378 rightNode: 783\n",
      "leftNode: 380 rightNode: 1842\n",
      "leftNode: 381 rightNode: 1686\n",
      "leftNode: 382 rightNode: 904\n",
      "leftNode: 383 rightNode: 1501\n",
      "leftNode: 383 rightNode: 496\n",
      "leftNode: 384 rightNode: 1551\n",
      "leftNode: 387 rightNode: 1424\n",
      "leftNode: 388 rightNode: 1063\n",
      "leftNode: 389 rightNode: 1923\n",
      "leftNode: 389 rightNode: 398\n",
      "leftNode: 391 rightNode: 1431\n",
      "leftNode: 391 rightNode: 1063\n",
      "leftNode: 392 rightNode: 2280\n",
      "leftNode: 392 rightNode: 406\n",
      "leftNode: 392 rightNode: 866\n",
      "leftNode: 392 rightNode: 395\n",
      "leftNode: 393 rightNode: 1457\n",
      "leftNode: 393 rightNode: 2007\n",
      "leftNode: 394 rightNode: 2001\n",
      "leftNode: 395 rightNode: 1840\n",
      "leftNode: 395 rightNode: 2280\n",
      "leftNode: 395 rightNode: 1387\n",
      "leftNode: 395 rightNode: 1182\n",
      "leftNode: 395 rightNode: 406\n",
      "leftNode: 398 rightNode: 731\n",
      "leftNode: 400 rightNode: 1901\n",
      "leftNode: 400 rightNode: 703\n",
      "leftNode: 402 rightNode: 1041\n",
      "leftNode: 403 rightNode: 1874\n",
      "leftNode: 403 rightNode: 1111\n",
      "leftNode: 405 rightNode: 524\n",
      "leftNode: 405 rightNode: 715\n",
      "leftNode: 405 rightNode: 1387\n",
      "leftNode: 405 rightNode: 2245\n",
      "leftNode: 405 rightNode: 2142\n",
      "leftNode: 405 rightNode: 1761\n",
      "leftNode: 405 rightNode: 1654\n",
      "leftNode: 406 rightNode: 1650\n",
      "leftNode: 406 rightNode: 1744\n",
      "leftNode: 406 rightNode: 1912\n",
      "leftNode: 406 rightNode: 1494\n",
      "leftNode: 406 rightNode: 2280\n",
      "leftNode: 406 rightNode: 2202\n",
      "leftNode: 406 rightNode: 1785\n",
      "leftNode: 406 rightNode: 495\n",
      "leftNode: 406 rightNode: 701\n",
      "leftNode: 406 rightNode: 866\n",
      "leftNode: 406 rightNode: 1279\n",
      "leftNode: 407 rightNode: 1600\n",
      "leftNode: 407 rightNode: 1601\n",
      "leftNode: 407 rightNode: 613\n",
      "leftNode: 407 rightNode: 2081\n",
      "leftNode: 407 rightNode: 1594\n",
      "leftNode: 407 rightNode: 1814\n",
      "leftNode: 410 rightNode: 1618\n",
      "leftNode: 410 rightNode: 1517\n",
      "leftNode: 412 rightNode: 2315\n",
      "leftNode: 413 rightNode: 994\n",
      "leftNode: 413 rightNode: 2385\n",
      "leftNode: 413 rightNode: 272\n",
      "leftNode: 413 rightNode: 1540\n",
      "leftNode: 417 rightNode: 738\n",
      "leftNode: 417 rightNode: 873\n",
      "leftNode: 420 rightNode: 1990\n",
      "leftNode: 420 rightNode: 1712\n",
      "leftNode: 420 rightNode: 2021\n",
      "leftNode: 420 rightNode: 1462\n",
      "leftNode: 421 rightNode: 918\n",
      "leftNode: 425 rightNode: 1737\n",
      "leftNode: 426 rightNode: 961\n",
      "leftNode: 426 rightNode: 2096\n",
      "leftNode: 426 rightNode: 1917\n",
      "leftNode: 426 rightNode: 2085\n",
      "leftNode: 428 rightNode: 452\n",
      "leftNode: 430 rightNode: 2167\n",
      "leftNode: 430 rightNode: 2323\n",
      "leftNode: 433 rightNode: 1453\n",
      "leftNode: 436 rightNode: 1330\n",
      "leftNode: 436 rightNode: 496\n",
      "leftNode: 436 rightNode: 904\n",
      "leftNode: 436 rightNode: 1476\n",
      "leftNode: 436 rightNode: 1508\n",
      "leftNode: 437 rightNode: 1926\n",
      "leftNode: 437 rightNode: 1374\n",
      "leftNode: 437 rightNode: 1264\n",
      "leftNode: 439 rightNode: 1036\n",
      "leftNode: 441 rightNode: 798\n",
      "leftNode: 445 rightNode: 1467\n",
      "leftNode: 447 rightNode: 1468\n",
      "leftNode: 447 rightNode: 1391\n",
      "leftNode: 450 rightNode: 1491\n",
      "leftNode: 451 rightNode: 1429\n",
      "leftNode: 452 rightNode: 1429\n",
      "leftNode: 452 rightNode: 1139\n",
      "leftNode: 457 rightNode: 1035\n",
      "leftNode: 459 rightNode: 1061\n",
      "leftNode: 459 rightNode: 1587\n",
      "leftNode: 464 rightNode: 657\n",
      "leftNode: 464 rightNode: 1666\n",
      "leftNode: 464 rightNode: 1230\n",
      "leftNode: 466 rightNode: 1873\n",
      "leftNode: 469 rightNode: 1228\n",
      "leftNode: 478 rightNode: 1960\n",
      "leftNode: 478 rightNode: 2103\n",
      "leftNode: 478 rightNode: 64\n",
      "leftNode: 478 rightNode: 1849\n",
      "leftNode: 478 rightNode: 1503\n",
      "leftNode: 478 rightNode: 2328\n",
      "leftNode: 478 rightNode: 992\n",
      "leftNode: 481 rightNode: 252\n",
      "leftNode: 481 rightNode: 1052\n",
      "leftNode: 481 rightNode: 1773\n",
      "leftNode: 481 rightNode: 509\n",
      "leftNode: 481 rightNode: 1856\n",
      "leftNode: 481 rightNode: 2237\n",
      "leftNode: 481 rightNode: 688\n",
      "leftNode: 483 rightNode: 2229\n",
      "leftNode: 483 rightNode: 1441\n",
      "leftNode: 484 rightNode: 909\n",
      "leftNode: 485 rightNode: 1146\n",
      "leftNode: 486 rightNode: 1337\n",
      "leftNode: 486 rightNode: 559\n",
      "leftNode: 486 rightNode: 2312\n",
      "leftNode: 487 rightNode: 1865\n",
      "leftNode: 487 rightNode: 2306\n",
      "leftNode: 490 rightNode: 2233\n",
      "leftNode: 494 rightNode: 1224\n",
      "leftNode: 495 rightNode: 1001\n",
      "leftNode: 495 rightNode: 1465\n",
      "leftNode: 495 rightNode: 1699\n",
      "leftNode: 495 rightNode: 701\n",
      "leftNode: 496 rightNode: 1387\n",
      "leftNode: 496 rightNode: 1041\n",
      "leftNode: 496 rightNode: 723\n",
      "leftNode: 496 rightNode: 1588\n",
      "leftNode: 496 rightNode: 2284\n",
      "leftNode: 496 rightNode: 1068\n",
      "leftNode: 496 rightNode: 642\n",
      "leftNode: 496 rightNode: 754\n",
      "leftNode: 496 rightNode: 1114\n",
      "leftNode: 496 rightNode: 961\n",
      "leftNode: 496 rightNode: 1701\n",
      "leftNode: 496 rightNode: 863\n",
      "leftNode: 496 rightNode: 1017\n",
      "leftNode: 496 rightNode: 614\n",
      "leftNode: 497 rightNode: 2190\n",
      "leftNode: 497 rightNode: 1184\n",
      "leftNode: 497 rightNode: 1213\n",
      "leftNode: 497 rightNode: 701\n",
      "leftNode: 497 rightNode: 1301\n",
      "leftNode: 499 rightNode: 1139\n",
      "leftNode: 501 rightNode: 2280\n",
      "leftNode: 501 rightNode: 1494\n",
      "leftNode: 502 rightNode: 1850\n",
      "leftNode: 502 rightNode: 2134\n",
      "leftNode: 502 rightNode: 2019\n",
      "leftNode: 502 rightNode: 1318\n",
      "leftNode: 504 rightNode: 1064\n",
      "leftNode: 504 rightNode: 548\n",
      "leftNode: 504 rightNode: 504\n",
      "leftNode: 505 rightNode: 2280\n",
      "leftNode: 505 rightNode: 1694\n",
      "leftNode: 507 rightNode: 781\n",
      "leftNode: 507 rightNode: 1603\n",
      "leftNode: 507 rightNode: 844\n",
      "leftNode: 507 rightNode: 796\n",
      "leftNode: 507 rightNode: 2077\n",
      "leftNode: 512 rightNode: 1433\n",
      "leftNode: 513 rightNode: 2112\n",
      "leftNode: 514 rightNode: 1827\n",
      "leftNode: 514 rightNode: 560\n",
      "leftNode: 521 rightNode: 750\n",
      "leftNode: 522 rightNode: 710\n",
      "leftNode: 522 rightNode: 1573\n",
      "leftNode: 522 rightNode: 1169\n",
      "leftNode: 522 rightNode: 1078\n",
      "leftNode: 522 rightNode: 741\n",
      "leftNode: 522 rightNode: 1458\n",
      "leftNode: 522 rightNode: 2242\n",
      "leftNode: 526 rightNode: 1576\n",
      "leftNode: 528 rightNode: 1267\n",
      "leftNode: 530 rightNode: 1654\n",
      "leftNode: 531 rightNode: 701\n",
      "leftNode: 533 rightNode: 750\n",
      "leftNode: 536 rightNode: 904\n",
      "leftNode: 538 rightNode: 1360\n",
      "leftNode: 541 rightNode: 1626\n",
      "leftNode: 542 rightNode: 1344\n",
      "leftNode: 542 rightNode: 696\n",
      "leftNode: 542 rightNode: 879\n",
      "leftNode: 546 rightNode: 1912\n",
      "leftNode: 547 rightNode: 1747\n",
      "leftNode: 548 rightNode: 1988\n",
      "leftNode: 550 rightNode: 575\n",
      "leftNode: 550 rightNode: 1128\n",
      "leftNode: 550 rightNode: 2319\n",
      "leftNode: 550 rightNode: 535\n",
      "leftNode: 550 rightNode: 1945\n",
      "leftNode: 551 rightNode: 2383\n",
      "leftNode: 552 rightNode: 1531\n",
      "leftNode: 552 rightNode: 2058\n",
      "leftNode: 552 rightNode: 1377\n",
      "leftNode: 552 rightNode: 2208\n",
      "leftNode: 553 rightNode: 2051\n",
      "leftNode: 554 rightNode: 835\n",
      "leftNode: 554 rightNode: 1228\n",
      "leftNode: 555 rightNode: 1352\n",
      "leftNode: 557 rightNode: 557\n",
      "leftNode: 559 rightNode: 2232\n",
      "leftNode: 559 rightNode: 2312\n",
      "leftNode: 560 rightNode: 1998\n",
      "leftNode: 560 rightNode: 1959\n",
      "leftNode: 560 rightNode: 1372\n",
      "leftNode: 560 rightNode: 1827\n",
      "leftNode: 561 rightNode: 599\n",
      "leftNode: 562 rightNode: 1193\n",
      "leftNode: 562 rightNode: 2374\n",
      "leftNode: 563 rightNode: 1756\n",
      "leftNode: 565 rightNode: 1269\n",
      "leftNode: 567 rightNode: 1921\n",
      "leftNode: 567 rightNode: 1271\n",
      "leftNode: 567 rightNode: 2284\n",
      "leftNode: 567 rightNode: 2085\n",
      "leftNode: 567 rightNode: 1071\n",
      "leftNode: 568 rightNode: 1126\n",
      "leftNode: 568 rightNode: 839\n",
      "leftNode: 568 rightNode: 1618\n",
      "leftNode: 568 rightNode: 2160\n",
      "leftNode: 568 rightNode: 1517\n",
      "leftNode: 570 rightNode: 2018\n",
      "leftNode: 578 rightNode: 578\n",
      "leftNode: 580 rightNode: 1902\n",
      "leftNode: 582 rightNode: 1119\n",
      "leftNode: 583 rightNode: 2278\n",
      "leftNode: 583 rightNode: 2329\n",
      "leftNode: 585 rightNode: 904\n",
      "leftNode: 586 rightNode: 293\n",
      "leftNode: 589 rightNode: 1408\n",
      "leftNode: 591 rightNode: 1100\n",
      "leftNode: 592 rightNode: 2078\n",
      "leftNode: 592 rightNode: 1289\n",
      "leftNode: 593 rightNode: 1401\n",
      "leftNode: 593 rightNode: 1689\n",
      "leftNode: 593 rightNode: 1965\n",
      "leftNode: 593 rightNode: 1181\n",
      "leftNode: 594 rightNode: 155\n",
      "leftNode: 594 rightNode: 1779\n",
      "leftNode: 595 rightNode: 1466\n",
      "leftNode: 595 rightNode: 1949\n",
      "leftNode: 595 rightNode: 2018\n",
      "leftNode: 597 rightNode: 746\n",
      "leftNode: 598 rightNode: 1403\n",
      "leftNode: 600 rightNode: 2315\n",
      "leftNode: 604 rightNode: 904\n",
      "leftNode: 606 rightNode: 2389\n",
      "leftNode: 610 rightNode: 1674\n",
      "leftNode: 610 rightNode: 2091\n",
      "leftNode: 613 rightNode: 2179\n",
      "leftNode: 613 rightNode: 2081\n",
      "leftNode: 613 rightNode: 1594\n",
      "leftNode: 614 rightNode: 2297\n",
      "leftNode: 614 rightNode: 1387\n",
      "leftNode: 614 rightNode: 1665\n",
      "leftNode: 614 rightNode: 614\n",
      "leftNode: 617 rightNode: 1509\n",
      "leftNode: 619 rightNode: 1945\n",
      "leftNode: 620 rightNode: 1944\n",
      "leftNode: 620 rightNode: 2233\n",
      "leftNode: 621 rightNode: 1226\n",
      "leftNode: 622 rightNode: 1161\n",
      "leftNode: 622 rightNode: 933\n",
      "leftNode: 622 rightNode: 1107\n",
      "leftNode: 622 rightNode: 1895\n",
      "leftNode: 625 rightNode: 759\n",
      "leftNode: 625 rightNode: 2365\n",
      "leftNode: 627 rightNode: 1313\n",
      "leftNode: 627 rightNode: 715\n",
      "leftNode: 631 rightNode: 945\n",
      "leftNode: 631 rightNode: 721\n",
      "leftNode: 632 rightNode: 2278\n",
      "leftNode: 634 rightNode: 701\n",
      "leftNode: 636 rightNode: 2229\n",
      "leftNode: 638 rightNode: 1312\n",
      "leftNode: 638 rightNode: 1366\n",
      "leftNode: 642 rightNode: 2377\n",
      "leftNode: 642 rightNode: 1330\n",
      "leftNode: 643 rightNode: 656\n",
      "leftNode: 643 rightNode: 1413\n",
      "leftNode: 645 rightNode: 1871\n",
      "leftNode: 645 rightNode: 1543\n",
      "leftNode: 647 rightNode: 1779\n",
      "leftNode: 648 rightNode: 2112\n",
      "leftNode: 654 rightNode: 1876\n",
      "leftNode: 655 rightNode: 1274\n",
      "leftNode: 657 rightNode: 2156\n",
      "leftNode: 657 rightNode: 1031\n",
      "leftNode: 659 rightNode: 1503\n",
      "leftNode: 659 rightNode: 1960\n",
      "leftNode: 659 rightNode: 64\n",
      "leftNode: 660 rightNode: 1780\n",
      "leftNode: 660 rightNode: 1694\n",
      "leftNode: 660 rightNode: 891\n",
      "leftNode: 660 rightNode: 1494\n",
      "leftNode: 660 rightNode: 2280\n",
      "leftNode: 660 rightNode: 1969\n",
      "leftNode: 662 rightNode: 1185\n",
      "leftNode: 664 rightNode: 666\n",
      "leftNode: 664 rightNode: 2110\n",
      "leftNode: 664 rightNode: 937\n",
      "leftNode: 664 rightNode: 701\n",
      "leftNode: 664 rightNode: 1512\n",
      "leftNode: 664 rightNode: 784\n",
      "leftNode: 666 rightNode: 2110\n",
      "leftNode: 666 rightNode: 937\n",
      "leftNode: 668 rightNode: 1268\n",
      "leftNode: 669 rightNode: 1931\n",
      "leftNode: 669 rightNode: 2248\n",
      "leftNode: 669 rightNode: 2163\n",
      "leftNode: 672 rightNode: 682\n",
      "leftNode: 672 rightNode: 2119\n",
      "leftNode: 672 rightNode: 2080\n",
      "leftNode: 672 rightNode: 1156\n",
      "leftNode: 674 rightNode: 2216\n",
      "leftNode: 675 rightNode: 1535\n",
      "leftNode: 679 rightNode: 917\n",
      "leftNode: 679 rightNode: 961\n",
      "leftNode: 679 rightNode: 1387\n",
      "leftNode: 679 rightNode: 2311\n",
      "leftNode: 679 rightNode: 770\n",
      "leftNode: 679 rightNode: 732\n",
      "leftNode: 679 rightNode: 1123\n",
      "leftNode: 682 rightNode: 2119\n",
      "leftNode: 682 rightNode: 789\n",
      "leftNode: 682 rightNode: 2080\n",
      "leftNode: 682 rightNode: 1156\n",
      "leftNode: 682 rightNode: 2230\n",
      "leftNode: 682 rightNode: 1184\n",
      "leftNode: 685 rightNode: 1283\n",
      "leftNode: 685 rightNode: 901\n",
      "leftNode: 686 rightNode: 701\n",
      "leftNode: 686 rightNode: 1241\n",
      "leftNode: 688 rightNode: 1033\n",
      "leftNode: 688 rightNode: 2237\n",
      "leftNode: 688 rightNode: 701\n",
      "leftNode: 688 rightNode: 736\n",
      "leftNode: 688 rightNode: 868\n",
      "leftNode: 689 rightNode: 2081\n",
      "leftNode: 689 rightNode: 1618\n",
      "leftNode: 690 rightNode: 1801\n",
      "leftNode: 690 rightNode: 217\n",
      "leftNode: 693 rightNode: 1225\n",
      "leftNode: 694 rightNode: 2018\n",
      "leftNode: 694 rightNode: 1466\n",
      "leftNode: 698 rightNode: 1124\n",
      "leftNode: 701 rightNode: 2110\n",
      "leftNode: 701 rightNode: 1050\n",
      "leftNode: 701 rightNode: 1271\n",
      "leftNode: 701 rightNode: 1442\n",
      "leftNode: 701 rightNode: 363\n",
      "leftNode: 701 rightNode: 867\n",
      "leftNode: 701 rightNode: 1449\n",
      "leftNode: 701 rightNode: 2242\n",
      "leftNode: 701 rightNode: 1494\n",
      "leftNode: 701 rightNode: 829\n",
      "leftNode: 701 rightNode: 1288\n",
      "leftNode: 701 rightNode: 861\n",
      "leftNode: 701 rightNode: 2126\n",
      "leftNode: 701 rightNode: 2280\n",
      "leftNode: 701 rightNode: 1235\n",
      "leftNode: 701 rightNode: 1244\n",
      "leftNode: 701 rightNode: 902\n",
      "leftNode: 701 rightNode: 1592\n",
      "leftNode: 701 rightNode: 1654\n",
      "leftNode: 701 rightNode: 920\n",
      "leftNode: 701 rightNode: 1964\n",
      "leftNode: 701 rightNode: 1969\n",
      "leftNode: 701 rightNode: 1620\n",
      "leftNode: 701 rightNode: 937\n",
      "leftNode: 701 rightNode: 1988\n",
      "leftNode: 701 rightNode: 2011\n",
      "leftNode: 701 rightNode: 1858\n",
      "leftNode: 701 rightNode: 2047\n",
      "leftNode: 702 rightNode: 1193\n",
      "leftNode: 702 rightNode: 2025\n",
      "leftNode: 705 rightNode: 863\n",
      "leftNode: 705 rightNode: 1339\n",
      "leftNode: 708 rightNode: 782\n",
      "leftNode: 709 rightNode: 1169\n",
      "leftNode: 709 rightNode: 2259\n",
      "leftNode: 709 rightNode: 2059\n",
      "leftNode: 709 rightNode: 1454\n",
      "leftNode: 709 rightNode: 1896\n",
      "leftNode: 709 rightNode: 1328\n",
      "leftNode: 710 rightNode: 1169\n",
      "leftNode: 710 rightNode: 1573\n",
      "leftNode: 710 rightNode: 741\n",
      "leftNode: 710 rightNode: 1078\n",
      "leftNode: 710 rightNode: 1458\n",
      "leftNode: 710 rightNode: 2242\n",
      "leftNode: 712 rightNode: 1299\n",
      "leftNode: 712 rightNode: 1252\n",
      "leftNode: 713 rightNode: 293\n",
      "leftNode: 714 rightNode: 2389\n",
      "leftNode: 715 rightNode: 961\n",
      "leftNode: 715 rightNode: 1387\n",
      "leftNode: 715 rightNode: 983\n",
      "leftNode: 715 rightNode: 1654\n",
      "leftNode: 715 rightNode: 1271\n",
      "leftNode: 715 rightNode: 2078\n",
      "leftNode: 715 rightNode: 1979\n",
      "leftNode: 715 rightNode: 2085\n",
      "leftNode: 715 rightNode: 1123\n",
      "leftNode: 718 rightNode: 2112\n",
      "leftNode: 720 rightNode: 925\n",
      "leftNode: 721 rightNode: 1741\n",
      "leftNode: 721 rightNode: 2040\n",
      "leftNode: 722 rightNode: 2238\n",
      "leftNode: 722 rightNode: 1193\n",
      "leftNode: 725 rightNode: 1050\n",
      "leftNode: 729 rightNode: 1796\n",
      "leftNode: 729 rightNode: 2024\n",
      "leftNode: 729 rightNode: 1466\n",
      "leftNode: 729 rightNode: 1282\n",
      "leftNode: 729 rightNode: 1149\n",
      "leftNode: 729 rightNode: 2170\n",
      "leftNode: 729 rightNode: 1368\n",
      "leftNode: 730 rightNode: 1691\n",
      "leftNode: 730 rightNode: 1694\n",
      "leftNode: 730 rightNode: 1443\n",
      "leftNode: 730 rightNode: 1969\n",
      "leftNode: 730 rightNode: 2208\n",
      "leftNode: 738 rightNode: 873\n",
      "leftNode: 738 rightNode: 2134\n",
      "leftNode: 741 rightNode: 1078\n",
      "leftNode: 742 rightNode: 1886\n",
      "leftNode: 742 rightNode: 1585\n",
      "leftNode: 743 rightNode: 1359\n",
      "leftNode: 744 rightNode: 1235\n",
      "leftNode: 746 rightNode: 2115\n",
      "leftNode: 749 rightNode: 2163\n",
      "leftNode: 752 rightNode: 1571\n",
      "leftNode: 752 rightNode: 2029\n",
      "leftNode: 754 rightNode: 961\n",
      "leftNode: 754 rightNode: 1387\n",
      "leftNode: 754 rightNode: 1389\n",
      "leftNode: 754 rightNode: 1209\n",
      "leftNode: 754 rightNode: 2284\n",
      "leftNode: 754 rightNode: 983\n",
      "leftNode: 754 rightNode: 1271\n",
      "leftNode: 754 rightNode: 1460\n",
      "leftNode: 754 rightNode: 2245\n",
      "leftNode: 754 rightNode: 1007\n",
      "leftNode: 754 rightNode: 1084\n",
      "leftNode: 754 rightNode: 767\n",
      "leftNode: 754 rightNode: 1278\n",
      "leftNode: 754 rightNode: 1103\n",
      "leftNode: 754 rightNode: 1745\n",
      "leftNode: 754 rightNode: 1654\n",
      "leftNode: 754 rightNode: 2031\n",
      "leftNode: 755 rightNode: 2114\n",
      "leftNode: 755 rightNode: 850\n",
      "leftNode: 755 rightNode: 1468\n",
      "leftNode: 755 rightNode: 2180\n",
      "leftNode: 757 rightNode: 1387\n",
      "leftNode: 757 rightNode: 899\n",
      "leftNode: 758 rightNode: 1446\n",
      "leftNode: 758 rightNode: 980\n",
      "leftNode: 758 rightNode: 2380\n",
      "leftNode: 759 rightNode: 2365\n",
      "leftNode: 761 rightNode: 1924\n",
      "leftNode: 761 rightNode: 2348\n",
      "leftNode: 761 rightNode: 1354\n",
      "leftNode: 762 rightNode: 2335\n",
      "leftNode: 763 rightNode: 1195\n",
      "leftNode: 763 rightNode: 1025\n",
      "leftNode: 764 rightNode: 1248\n",
      "leftNode: 764 rightNode: 1093\n",
      "leftNode: 767 rightNode: 899\n",
      "leftNode: 767 rightNode: 1271\n",
      "leftNode: 767 rightNode: 977\n",
      "leftNode: 767 rightNode: 2218\n",
      "leftNode: 768 rightNode: 1199\n",
      "leftNode: 768 rightNode: 1136\n",
      "leftNode: 768 rightNode: 1020\n",
      "leftNode: 768 rightNode: 827\n",
      "leftNode: 770 rightNode: 1387\n",
      "leftNode: 770 rightNode: 1463\n",
      "leftNode: 770 rightNode: 1750\n",
      "leftNode: 770 rightNode: 732\n",
      "leftNode: 770 rightNode: 917\n",
      "leftNode: 770 rightNode: 1271\n",
      "leftNode: 770 rightNode: 2311\n",
      "leftNode: 770 rightNode: 1654\n",
      "leftNode: 770 rightNode: 1123\n",
      "leftNode: 774 rightNode: 1502\n",
      "leftNode: 774 rightNode: 1085\n",
      "leftNode: 774 rightNode: 2219\n",
      "leftNode: 781 rightNode: 839\n",
      "leftNode: 781 rightNode: 2114\n",
      "leftNode: 782 rightNode: 51\n",
      "leftNode: 783 rightNode: 1381\n",
      "leftNode: 790 rightNode: 891\n",
      "leftNode: 790 rightNode: 1182\n",
      "leftNode: 794 rightNode: 2217\n",
      "leftNode: 795 rightNode: 1722\n",
      "leftNode: 797 rightNode: 2293\n",
      "leftNode: 797 rightNode: 2201\n",
      "leftNode: 797 rightNode: 854\n",
      "leftNode: 799 rightNode: 2220\n",
      "leftNode: 799 rightNode: 1158\n",
      "leftNode: 799 rightNode: 1553\n",
      "leftNode: 802 rightNode: 1142\n",
      "leftNode: 802 rightNode: 945\n",
      "leftNode: 804 rightNode: 1718\n",
      "leftNode: 808 rightNode: 1123\n",
      "leftNode: 809 rightNode: 1722\n",
      "leftNode: 811 rightNode: 1602\n",
      "leftNode: 811 rightNode: 2282\n",
      "leftNode: 811 rightNode: 2058\n",
      "leftNode: 814 rightNode: 2184\n",
      "leftNode: 814 rightNode: 1152\n",
      "leftNode: 822 rightNode: 1970\n",
      "leftNode: 827 rightNode: 1168\n",
      "leftNode: 827 rightNode: 1199\n",
      "leftNode: 827 rightNode: 1136\n",
      "leftNode: 833 rightNode: 1455\n",
      "leftNode: 833 rightNode: 1057\n",
      "leftNode: 839 rightNode: 1126\n",
      "leftNode: 839 rightNode: 1618\n",
      "leftNode: 839 rightNode: 2160\n",
      "leftNode: 839 rightNode: 1517\n",
      "leftNode: 840 rightNode: 1387\n",
      "leftNode: 840 rightNode: 1041\n",
      "leftNode: 840 rightNode: 1617\n",
      "leftNode: 840 rightNode: 1047\n",
      "leftNode: 845 rightNode: 955\n",
      "leftNode: 845 rightNode: 1727\n",
      "leftNode: 846 rightNode: 1742\n",
      "leftNode: 846 rightNode: 1779\n",
      "leftNode: 848 rightNode: 2135\n",
      "leftNode: 848 rightNode: 2326\n",
      "leftNode: 848 rightNode: 1879\n",
      "leftNode: 852 rightNode: 1106\n",
      "leftNode: 863 rightNode: 1718\n",
      "leftNode: 863 rightNode: 1001\n",
      "leftNode: 863 rightNode: 1011\n",
      "leftNode: 863 rightNode: 2377\n",
      "leftNode: 866 rightNode: 909\n",
      "leftNode: 869 rightNode: 1359\n",
      "leftNode: 871 rightNode: 1485\n",
      "leftNode: 871 rightNode: 909\n",
      "leftNode: 876 rightNode: 1269\n",
      "leftNode: 878 rightNode: 1225\n",
      "leftNode: 879 rightNode: 1266\n",
      "leftNode: 1785 rightNode: 2280\n",
      "leftNode: 1785 rightNode: 1105\n",
      "leftNode: 887 rightNode: 1762\n",
      "leftNode: 888 rightNode: 1132\n",
      "leftNode: 888 rightNode: 1612\n",
      "leftNode: 890 rightNode: 1790\n",
      "leftNode: 891 rightNode: 2093\n",
      "leftNode: 891 rightNode: 1340\n",
      "leftNode: 891 rightNode: 1504\n",
      "leftNode: 891 rightNode: 1182\n",
      "leftNode: 897 rightNode: 1600\n",
      "leftNode: 897 rightNode: 252\n",
      "leftNode: 897 rightNode: 1619\n",
      "leftNode: 897 rightNode: 2236\n",
      "leftNode: 897 rightNode: 2018\n",
      "leftNode: 897 rightNode: 1189\n",
      "leftNode: 897 rightNode: 1773\n",
      "leftNode: 899 rightNode: 1387\n",
      "leftNode: 901 rightNode: 1283\n",
      "leftNode: 901 rightNode: 1149\n",
      "leftNode: 901 rightNode: 1325\n",
      "leftNode: 902 rightNode: 942\n",
      "leftNode: 903 rightNode: 2252\n",
      "leftNode: 904 rightNode: 1096\n",
      "leftNode: 904 rightNode: 2151\n",
      "leftNode: 904 rightNode: 2323\n",
      "leftNode: 904 rightNode: 1847\n",
      "leftNode: 904 rightNode: 1690\n",
      "leftNode: 907 rightNode: 1234\n",
      "leftNode: 907 rightNode: 1598\n",
      "leftNode: 908 rightNode: 2051\n",
      "leftNode: 909 rightNode: 2100\n",
      "leftNode: 909 rightNode: 2105\n",
      "leftNode: 909 rightNode: 2187\n",
      "leftNode: 909 rightNode: 1660\n",
      "leftNode: 909 rightNode: 2268\n",
      "leftNode: 909 rightNode: 2192\n",
      "leftNode: 909 rightNode: 1578\n",
      "leftNode: 909 rightNode: 1220\n",
      "leftNode: 909 rightNode: 2117\n",
      "leftNode: 909 rightNode: 1272\n",
      "leftNode: 909 rightNode: 1485\n",
      "leftNode: 909 rightNode: 1639\n",
      "leftNode: 909 rightNode: 2245\n",
      "leftNode: 915 rightNode: 2323\n",
      "leftNode: 917 rightNode: 1654\n",
      "leftNode: 917 rightNode: 2311\n",
      "leftNode: 917 rightNode: 1123\n",
      "leftNode: 917 rightNode: 732\n",
      "leftNode: 919 rightNode: 1209\n",
      "leftNode: 919 rightNode: 961\n",
      "leftNode: 920 rightNode: 967\n",
      "leftNode: 920 rightNode: 952\n",
      "leftNode: 922 rightNode: 2138\n",
      "leftNode: 926 rightNode: 1429\n",
      "leftNode: 927 rightNode: 1387\n",
      "leftNode: 927 rightNode: 2078\n",
      "leftNode: 930 rightNode: 2043\n",
      "leftNode: 930 rightNode: 1275\n",
      "leftNode: 930 rightNode: 2113\n",
      "leftNode: 930 rightNode: 1037\n",
      "leftNode: 930 rightNode: 1668\n",
      "leftNode: 931 rightNode: 1987\n",
      "leftNode: 933 rightNode: 1895\n",
      "leftNode: 935 rightNode: 1827\n",
      "leftNode: 936 rightNode: 1907\n",
      "leftNode: 937 rightNode: 1512\n",
      "leftNode: 937 rightNode: 2110\n",
      "leftNode: 937 rightNode: 784\n",
      "leftNode: 942 rightNode: 1934\n",
      "leftNode: 945 rightNode: 1840\n",
      "leftNode: 945 rightNode: 1319\n",
      "leftNode: 945 rightNode: 2149\n",
      "leftNode: 945 rightNode: 2214\n",
      "leftNode: 948 rightNode: 2373\n",
      "leftNode: 952 rightNode: 967\n",
      "leftNode: 956 rightNode: 1950\n",
      "leftNode: 957 rightNode: 2301\n",
      "leftNode: 957 rightNode: 995\n",
      "leftNode: 960 rightNode: 1181\n",
      "leftNode: 960 rightNode: 1689\n",
      "leftNode: 961 rightNode: 2085\n",
      "leftNode: 961 rightNode: 1391\n",
      "leftNode: 961 rightNode: 1586\n",
      "leftNode: 961 rightNode: 1209\n",
      "leftNode: 961 rightNode: 2284\n",
      "leftNode: 961 rightNode: 1245\n",
      "leftNode: 961 rightNode: 1387\n",
      "leftNode: 961 rightNode: 1084\n",
      "leftNode: 961 rightNode: 1141\n",
      "leftNode: 961 rightNode: 1271\n",
      "leftNode: 961 rightNode: 1772\n",
      "leftNode: 961 rightNode: 2311\n",
      "leftNode: 961 rightNode: 1112\n",
      "leftNode: 961 rightNode: 1123\n",
      "leftNode: 961 rightNode: 1654\n",
      "leftNode: 961 rightNode: 2345\n",
      "leftNode: 961 rightNode: 977\n",
      "leftNode: 961 rightNode: 983\n",
      "leftNode: 961 rightNode: 2209\n",
      "leftNode: 961 rightNode: 2224\n",
      "leftNode: 961 rightNode: 1007\n",
      "leftNode: 961 rightNode: 1053\n",
      "leftNode: 961 rightNode: 2245\n",
      "leftNode: 961 rightNode: 1205\n",
      "leftNode: 967 rightNode: 1179\n",
      "leftNode: 968 rightNode: 2247\n",
      "leftNode: 969 rightNode: 2165\n",
      "leftNode: 977 rightNode: 1387\n",
      "leftNode: 977 rightNode: 1745\n",
      "leftNode: 977 rightNode: 1713\n",
      "leftNode: 977 rightNode: 2078\n",
      "leftNode: 977 rightNode: 1271\n",
      "leftNode: 977 rightNode: 1123\n",
      "leftNode: 983 rightNode: 1215\n",
      "leftNode: 983 rightNode: 1387\n",
      "leftNode: 983 rightNode: 2031\n",
      "leftNode: 983 rightNode: 1209\n",
      "leftNode: 983 rightNode: 1654\n",
      "leftNode: 983 rightNode: 864\n",
      "leftNode: 983 rightNode: 1084\n",
      "leftNode: 983 rightNode: 1278\n",
      "leftNode: 983 rightNode: 1103\n",
      "leftNode: 983 rightNode: 1007\n",
      "leftNode: 983 rightNode: 1745\n",
      "leftNode: 992 rightNode: 1503\n",
      "leftNode: 992 rightNode: 2103\n",
      "leftNode: 994 rightNode: 272\n",
      "leftNode: 994 rightNode: 2385\n",
      "leftNode: 994 rightNode: 1540\n",
      "leftNode: 997 rightNode: 1330\n",
      "leftNode: 997 rightNode: 1683\n",
      "leftNode: 1000 rightNode: 1375\n",
      "leftNode: 1000 rightNode: 1799\n",
      "leftNode: 1001 rightNode: 1768\n",
      "leftNode: 1001 rightNode: 1655\n",
      "leftNode: 1004 rightNode: 1969\n",
      "leftNode: 1004 rightNode: 1531\n",
      "leftNode: 1004 rightNode: 1049\n",
      "leftNode: 1007 rightNode: 1387\n",
      "leftNode: 1007 rightNode: 2345\n",
      "leftNode: 1007 rightNode: 2245\n",
      "leftNode: 1007 rightNode: 1278\n",
      "leftNode: 1007 rightNode: 1654\n",
      "leftNode: 1008 rightNode: 1850\n",
      "leftNode: 1008 rightNode: 2134\n",
      "leftNode: 1009 rightNode: 1177\n",
      "leftNode: 1009 rightNode: 1812\n",
      "leftNode: 1012 rightNode: 2277\n",
      "leftNode: 1012 rightNode: 2354\n",
      "leftNode: 1017 rightNode: 1827\n",
      "leftNode: 1017 rightNode: 1372\n",
      "leftNode: 1017 rightNode: 2174\n",
      "leftNode: 1018 rightNode: 1159\n",
      "leftNode: 1020 rightNode: 1231\n",
      "leftNode: 1021 rightNode: 988\n",
      "leftNode: 1024 rightNode: 1783\n",
      "leftNode: 1025 rightNode: 1330\n",
      "leftNode: 1025 rightNode: 2334\n",
      "leftNode: 1025 rightNode: 1387\n",
      "leftNode: 1025 rightNode: 2341\n",
      "leftNode: 1025 rightNode: 2272\n",
      "leftNode: 1025 rightNode: 1195\n",
      "leftNode: 1025 rightNode: 2162\n",
      "leftNode: 1025 rightNode: 1678\n",
      "leftNode: 1025 rightNode: 1683\n",
      "leftNode: 1025 rightNode: 1068\n",
      "leftNode: 1034 rightNode: 2138\n",
      "leftNode: 1036 rightNode: 1779\n",
      "leftNode: 1037 rightNode: 2043\n",
      "leftNode: 1037 rightNode: 1796\n",
      "leftNode: 1037 rightNode: 1493\n",
      "leftNode: 1037 rightNode: 1668\n",
      "leftNode: 1037 rightNode: 1752\n",
      "leftNode: 1037 rightNode: 1940\n",
      "leftNode: 1037 rightNode: 1149\n",
      "leftNode: 1037 rightNode: 1759\n",
      "leftNode: 1037 rightNode: 1275\n",
      "leftNode: 1037 rightNode: 2246\n",
      "leftNode: 1041 rightNode: 1901\n",
      "leftNode: 1041 rightNode: 1733\n",
      "leftNode: 1041 rightNode: 1387\n",
      "leftNode: 1041 rightNode: 1390\n",
      "leftNode: 1041 rightNode: 1047\n",
      "leftNode: 1044 rightNode: 2114\n",
      "leftNode: 1047 rightNode: 1901\n",
      "leftNode: 1047 rightNode: 2019\n",
      "leftNode: 1047 rightNode: 2275\n",
      "leftNode: 1047 rightNode: 1372\n",
      "leftNode: 1047 rightNode: 2131\n",
      "leftNode: 1047 rightNode: 1351\n",
      "leftNode: 1049 rightNode: 1094\n",
      "leftNode: 1049 rightNode: 2239\n",
      "leftNode: 1050 rightNode: 1366\n",
      "leftNode: 1061 rightNode: 1266\n",
      "leftNode: 1063 rightNode: 1330\n",
      "leftNode: 1064 rightNode: 1269\n",
      "leftNode: 1068 rightNode: 1683\n",
      "leftNode: 1070 rightNode: 2087\n",
      "leftNode: 1071 rightNode: 1921\n",
      "leftNode: 1071 rightNode: 1271\n",
      "leftNode: 1072 rightNode: 1996\n",
      "leftNode: 1076 rightNode: 1319\n",
      "leftNode: 1078 rightNode: 1573\n",
      "leftNode: 1078 rightNode: 1622\n",
      "leftNode: 1078 rightNode: 1595\n",
      "leftNode: 1078 rightNode: 1169\n",
      "leftNode: 1078 rightNode: 1458\n",
      "leftNode: 1078 rightNode: 2242\n",
      "leftNode: 1078 rightNode: 1814\n",
      "leftNode: 1079 rightNode: 293\n",
      "leftNode: 1080 rightNode: 2384\n",
      "leftNode: 1084 rightNode: 1215\n",
      "leftNode: 1084 rightNode: 1586\n",
      "leftNode: 1084 rightNode: 1278\n",
      "leftNode: 1084 rightNode: 1205\n",
      "leftNode: 1087 rightNode: 1901\n",
      "leftNode: 1089 rightNode: 2166\n",
      "leftNode: 1089 rightNode: 1193\n",
      "leftNode: 1093 rightNode: 1248\n",
      "leftNode: 1093 rightNode: 1700\n",
      "leftNode: 1094 rightNode: 1694\n",
      "leftNode: 1094 rightNode: 1529\n",
      "leftNode: 1094 rightNode: 1969\n",
      "leftNode: 1094 rightNode: 1531\n",
      "leftNode: 1094 rightNode: 2239\n",
      "leftNode: 1097 rightNode: 1586\n",
      "leftNode: 1097 rightNode: 1921\n",
      "leftNode: 1658 rightNode: 1428\n",
      "leftNode: 1658 rightNode: 1349\n",
      "leftNode: 1103 rightNode: 1654\n",
      "leftNode: 1103 rightNode: 1946\n",
      "leftNode: 1103 rightNode: 1123\n",
      "leftNode: 1106 rightNode: 1808\n",
      "leftNode: 1106 rightNode: 1938\n",
      "leftNode: 1107 rightNode: 1123\n",
      "leftNode: 1109 rightNode: 2195\n",
      "leftNode: 1113 rightNode: 1934\n",
      "leftNode: 1113 rightNode: 2105\n",
      "leftNode: 1113 rightNode: 1529\n",
      "leftNode: 1115 rightNode: 1177\n",
      "leftNode: 1118 rightNode: 2117\n",
      "leftNode: 1119 rightNode: 1901\n",
      "leftNode: 1119 rightNode: 1226\n",
      "leftNode: 1120 rightNode: 2352\n",
      "leftNode: 1122 rightNode: 535\n",
      "leftNode: 1123 rightNode: 1215\n",
      "leftNode: 1123 rightNode: 1387\n",
      "leftNode: 1123 rightNode: 2209\n",
      "leftNode: 1123 rightNode: 2368\n",
      "leftNode: 1123 rightNode: 1761\n",
      "leftNode: 1123 rightNode: 2284\n",
      "leftNode: 1123 rightNode: 2311\n",
      "leftNode: 1123 rightNode: 240\n",
      "leftNode: 1123 rightNode: 1278\n",
      "leftNode: 1123 rightNode: 1654\n",
      "leftNode: 1123 rightNode: 732\n",
      "leftNode: 1123 rightNode: 2245\n",
      "leftNode: 1123 rightNode: 1711\n",
      "leftNode: 1123 rightNode: 864\n",
      "leftNode: 1124 rightNode: 1201\n",
      "leftNode: 1124 rightNode: 1221\n",
      "leftNode: 1124 rightNode: 2252\n",
      "leftNode: 1125 rightNode: 1137\n",
      "leftNode: 1126 rightNode: 2088\n",
      "leftNode: 1126 rightNode: 1618\n",
      "leftNode: 1126 rightNode: 2160\n",
      "leftNode: 1126 rightNode: 1517\n",
      "leftNode: 1127 rightNode: 2278\n",
      "leftNode: 1127 rightNode: 2159\n",
      "leftNode: 1128 rightNode: 1663\n",
      "leftNode: 1135 rightNode: 1689\n",
      "leftNode: 1136 rightNode: 1168\n",
      "leftNode: 1141 rightNode: 1639\n",
      "leftNode: 1141 rightNode: 1387\n",
      "leftNode: 1145 rightNode: 1626\n",
      "leftNode: 1145 rightNode: 1438\n",
      "leftNode: 1146 rightNode: 1246\n",
      "leftNode: 1146 rightNode: 2195\n",
      "leftNode: 1148 rightNode: 1886\n",
      "leftNode: 1148 rightNode: 1585\n",
      "leftNode: 1151 rightNode: 1409\n",
      "leftNode: 1152 rightNode: 2184\n",
      "leftNode: 1156 rightNode: 2119\n",
      "leftNode: 1156 rightNode: 2080\n",
      "leftNode: 1158 rightNode: 1553\n",
      "leftNode: 1168 rightNode: 1199\n",
      "leftNode: 1175 rightNode: 1175\n",
      "leftNode: 1180 rightNode: 2372\n",
      "leftNode: 1181 rightNode: 1357\n",
      "leftNode: 1181 rightNode: 1401\n",
      "leftNode: 1181 rightNode: 1689\n",
      "leftNode: 1182 rightNode: 2208\n",
      "leftNode: 1187 rightNode: 2075\n",
      "leftNode: 1193 rightNode: 2104\n",
      "leftNode: 1193 rightNode: 2023\n",
      "leftNode: 1193 rightNode: 2166\n",
      "leftNode: 1193 rightNode: 2374\n",
      "leftNode: 1193 rightNode: 1692\n",
      "leftNode: 1193 rightNode: 2238\n",
      "leftNode: 1193 rightNode: 2025\n",
      "leftNode: 1195 rightNode: 1330\n",
      "leftNode: 1195 rightNode: 2162\n",
      "leftNode: 1200 rightNode: 1746\n",
      "leftNode: 1202 rightNode: 1951\n",
      "leftNode: 1205 rightNode: 1387\n",
      "leftNode: 1205 rightNode: 1391\n",
      "leftNode: 1205 rightNode: 1745\n",
      "leftNode: 1205 rightNode: 2245\n",
      "leftNode: 1205 rightNode: 1271\n",
      "leftNode: 1205 rightNode: 1278\n",
      "leftNode: 1205 rightNode: 1654\n",
      "leftNode: 1206 rightNode: 1235\n",
      "leftNode: 1209 rightNode: 1387\n",
      "leftNode: 1209 rightNode: 1271\n",
      "leftNode: 1209 rightNode: 2284\n",
      "leftNode: 1213 rightNode: 2268\n",
      "leftNode: 1217 rightNode: 2212\n",
      "leftNode: 1221 rightNode: 2252\n",
      "leftNode: 1226 rightNode: 1339\n",
      "leftNode: 1226 rightNode: 2283\n",
      "leftNode: 1227 rightNode: 1783\n",
      "leftNode: 1230 rightNode: 1904\n",
      "leftNode: 1230 rightNode: 2077\n",
      "leftNode: 1233 rightNode: 2059\n",
      "leftNode: 1234 rightNode: 1483\n",
      "leftNode: 1234 rightNode: 2089\n",
      "leftNode: 1237 rightNode: 2306\n",
      "leftNode: 1241 rightNode: 1704\n",
      "leftNode: 1241 rightNode: 1407\n",
      "leftNode: 1243 rightNode: 1303\n",
      "leftNode: 1243 rightNode: 1367\n",
      "leftNode: 1243 rightNode: 1814\n",
      "leftNode: 1244 rightNode: 1934\n",
      "leftNode: 1244 rightNode: 1225\n",
      "leftNode: 1244 rightNode: 252\n",
      "leftNode: 1246 rightNode: 2195\n",
      "leftNode: 1247 rightNode: 2284\n",
      "leftNode: 1247 rightNode: 1745\n",
      "leftNode: 1248 rightNode: 1700\n",
      "leftNode: 1248 rightNode: 962\n",
      "leftNode: 1253 rightNode: 2252\n",
      "leftNode: 1255 rightNode: 1677\n",
      "leftNode: 1256 rightNode: 2031\n",
      "leftNode: 1259 rightNode: 1919\n",
      "leftNode: 1259 rightNode: 2020\n",
      "leftNode: 1264 rightNode: 1374\n",
      "leftNode: 1264 rightNode: 1926\n",
      "leftNode: 1266 rightNode: 1575\n",
      "leftNode: 1267 rightNode: 1903\n",
      "leftNode: 1267 rightNode: 2159\n",
      "leftNode: 1269 rightNode: 1432\n",
      "leftNode: 1269 rightNode: 2324\n",
      "leftNode: 1269 rightNode: 2305\n",
      "leftNode: 1269 rightNode: 415\n",
      "leftNode: 1269 rightNode: 2175\n",
      "leftNode: 1270 rightNode: 2335\n",
      "leftNode: 1271 rightNode: 1654\n",
      "leftNode: 1271 rightNode: 2085\n",
      "leftNode: 1271 rightNode: 1921\n",
      "leftNode: 1271 rightNode: 1761\n",
      "leftNode: 1271 rightNode: 2284\n",
      "leftNode: 1271 rightNode: 1767\n",
      "leftNode: 1271 rightNode: 2245\n",
      "leftNode: 1275 rightNode: 1149\n",
      "leftNode: 1275 rightNode: 1668\n",
      "leftNode: 1276 rightNode: 1225\n",
      "leftNode: 1278 rightNode: 1391\n",
      "leftNode: 1278 rightNode: 2224\n",
      "leftNode: 1278 rightNode: 1586\n",
      "leftNode: 1279 rightNode: 2202\n",
      "leftNode: 1279 rightNode: 1912\n",
      "leftNode: 1281 rightNode: 2094\n",
      "leftNode: 1282 rightNode: 2024\n",
      "leftNode: 1282 rightNode: 1668\n",
      "leftNode: 1282 rightNode: 1387\n",
      "leftNode: 1283 rightNode: 1964\n",
      "leftNode: 1286 rightNode: 293\n",
      "leftNode: 1286 rightNode: 1799\n",
      "leftNode: 1290 rightNode: 1964\n",
      "leftNode: 1292 rightNode: 1985\n",
      "leftNode: 1292 rightNode: 1978\n",
      "leftNode: 1292 rightNode: 1627\n",
      "leftNode: 1299 rightNode: 2208\n",
      "leftNode: 1300 rightNode: 1629\n",
      "leftNode: 1300 rightNode: 1718\n",
      "leftNode: 1300 rightNode: 2034\n",
      "leftNode: 1300 rightNode: 2101\n",
      "leftNode: 1303 rightNode: 1705\n",
      "leftNode: 1303 rightNode: 1951\n",
      "leftNode: 1303 rightNode: 1594\n",
      "leftNode: 1311 rightNode: 630\n",
      "leftNode: 1311 rightNode: 1539\n",
      "leftNode: 1312 rightNode: 217\n",
      "leftNode: 1312 rightNode: 1366\n",
      "leftNode: 1313 rightNode: 1639\n",
      "leftNode: 1316 rightNode: 2258\n",
      "leftNode: 1316 rightNode: 1372\n",
      "leftNode: 1316 rightNode: 1701\n",
      "leftNode: 1318 rightNode: 2134\n",
      "leftNode: 1318 rightNode: 2275\n",
      "leftNode: 1318 rightNode: 1342\n",
      "leftNode: 1319 rightNode: 2004\n",
      "leftNode: 1319 rightNode: 1461\n",
      "leftNode: 1321 rightNode: 1589\n",
      "leftNode: 1323 rightNode: 1547\n",
      "leftNode: 1325 rightNode: 1149\n",
      "leftNode: 1327 rightNode: 2359\n",
      "leftNode: 1328 rightNode: 2059\n",
      "leftNode: 1329 rightNode: 1750\n",
      "leftNode: 1329 rightNode: 2233\n",
      "leftNode: 1330 rightNode: 2334\n",
      "leftNode: 1330 rightNode: 1683\n",
      "leftNode: 1330 rightNode: 1372\n",
      "leftNode: 1331 rightNode: 1367\n",
      "leftNode: 1331 rightNode: 2323\n",
      "leftNode: 1331 rightNode: 1740\n",
      "leftNode: 1331 rightNode: 2167\n",
      "leftNode: 1332 rightNode: 1748\n",
      "leftNode: 1332 rightNode: 2229\n",
      "leftNode: 1332 rightNode: 2033\n",
      "leftNode: 1339 rightNode: 2148\n",
      "leftNode: 1339 rightNode: 1913\n",
      "leftNode: 1339 rightNode: 1749\n",
      "leftNode: 1339 rightNode: 1635\n",
      "leftNode: 1342 rightNode: 1363\n",
      "leftNode: 1342 rightNode: 1695\n",
      "leftNode: 1342 rightNode: 1723\n",
      "leftNode: 1344 rightNode: 2317\n",
      "leftNode: 1349 rightNode: 2229\n",
      "leftNode: 1354 rightNode: 2316\n",
      "leftNode: 1354 rightNode: 2348\n",
      "leftNode: 1354 rightNode: 1924\n",
      "leftNode: 1357 rightNode: 1401\n",
      "leftNode: 1357 rightNode: 1689\n",
      "leftNode: 1364 rightNode: 1761\n",
      "leftNode: 1366 rightNode: 1740\n",
      "leftNode: 1366 rightNode: 1539\n",
      "leftNode: 1367 rightNode: 2179\n",
      "leftNode: 1367 rightNode: 1964\n",
      "leftNode: 1367 rightNode: 1740\n",
      "leftNode: 1367 rightNode: 1957\n",
      "leftNode: 1369 rightNode: 1911\n",
      "leftNode: 1372 rightNode: 1500\n",
      "leftNode: 1372 rightNode: 1501\n",
      "leftNode: 1372 rightNode: 2096\n",
      "leftNode: 1377 rightNode: 1691\n",
      "leftNode: 1377 rightNode: 2208\n",
      "leftNode: 1377 rightNode: 2282\n",
      "leftNode: 1377 rightNode: 2378\n",
      "leftNode: 1377 rightNode: 1602\n",
      "leftNode: 1377 rightNode: 1443\n",
      "leftNode: 1377 rightNode: 2058\n",
      "leftNode: 1378 rightNode: 1620\n",
      "leftNode: 1385 rightNode: 293\n",
      "leftNode: 1387 rightNode: 1389\n",
      "leftNode: 1387 rightNode: 1390\n",
      "leftNode: 1387 rightNode: 1391\n",
      "leftNode: 1387 rightNode: 1745\n",
      "leftNode: 1387 rightNode: 1946\n",
      "leftNode: 1387 rightNode: 732\n",
      "leftNode: 1387 rightNode: 2218\n",
      "leftNode: 1387 rightNode: 1665\n",
      "leftNode: 1387 rightNode: 2244\n",
      "leftNode: 1387 rightNode: 1891\n",
      "leftNode: 1387 rightNode: 1586\n",
      "leftNode: 1387 rightNode: 1933\n",
      "leftNode: 1387 rightNode: 1512\n",
      "leftNode: 1387 rightNode: 1638\n",
      "leftNode: 1387 rightNode: 1639\n",
      "leftNode: 1387 rightNode: 1395\n",
      "leftNode: 1387 rightNode: 1654\n",
      "leftNode: 1387 rightNode: 2297\n",
      "leftNode: 1391 rightNode: 2284\n",
      "leftNode: 1391 rightNode: 1654\n",
      "leftNode: 1391 rightNode: 2245\n",
      "leftNode: 1391 rightNode: 2311\n",
      "leftNode: 1391 rightNode: 864\n",
      "leftNode: 1393 rightNode: 2114\n",
      "leftNode: 1401 rightNode: 1689\n",
      "leftNode: 1402 rightNode: 1954\n",
      "leftNode: 1408 rightNode: 1627\n",
      "leftNode: 1408 rightNode: 1985\n",
      "leftNode: 1408 rightNode: 1978\n",
      "leftNode: 1409 rightNode: 2164\n",
      "leftNode: 1412 rightNode: 1666\n",
      "leftNode: 1415 rightNode: 1827\n",
      "leftNode: 1416 rightNode: 1948\n",
      "leftNode: 1417 rightNode: 2360\n",
      "leftNode: 1419 rightNode: 2388\n",
      "leftNode: 1421 rightNode: 1980\n",
      "leftNode: 1421 rightNode: 1783\n",
      "leftNode: 1426 rightNode: 1702\n",
      "leftNode: 1433 rightNode: 2067\n",
      "leftNode: 1434 rightNode: 2390\n",
      "leftNode: 240 rightNode: 1761\n",
      "leftNode: 240 rightNode: 1654\n",
      "leftNode: 1442 rightNode: 2036\n",
      "leftNode: 1443 rightNode: 1882\n",
      "leftNode: 1443 rightNode: 1997\n",
      "leftNode: 1446 rightNode: 2376\n",
      "leftNode: 1447 rightNode: 2354\n",
      "leftNode: 1454 rightNode: 1960\n",
      "leftNode: 1454 rightNode: 2059\n",
      "leftNode: 1458 rightNode: 1573\n",
      "leftNode: 1465 rightNode: 1699\n",
      "leftNode: 1466 rightNode: 252\n",
      "leftNode: 1466 rightNode: 1835\n",
      "leftNode: 1466 rightNode: 1957\n",
      "leftNode: 1466 rightNode: 2236\n",
      "leftNode: 1466 rightNode: 2018\n",
      "leftNode: 1466 rightNode: 1724\n",
      "leftNode: 1469 rightNode: 1585\n",
      "leftNode: 1472 rightNode: 1472\n",
      "leftNode: 1475 rightNode: 2198\n",
      "leftNode: 1479 rightNode: 2114\n",
      "leftNode: 1480 rightNode: 1936\n",
      "leftNode: 1480 rightNode: 2386\n",
      "leftNode: 1480 rightNode: 1866\n",
      "leftNode: 1483 rightNode: 1834\n",
      "leftNode: 1483 rightNode: 2089\n",
      "leftNode: 1484 rightNode: 2067\n",
      "leftNode: 1486 rightNode: 1845\n",
      "leftNode: 1488 rightNode: 2379\n",
      "leftNode: 1494 rightNode: 2105\n",
      "leftNode: 1494 rightNode: 2280\n",
      "leftNode: 1494 rightNode: 1780\n",
      "leftNode: 1494 rightNode: 2135\n",
      "leftNode: 1494 rightNode: 1694\n",
      "leftNode: 1494 rightNode: 2058\n",
      "leftNode: 1494 rightNode: 2149\n",
      "leftNode: 1494 rightNode: 1969\n",
      "leftNode: 1501 rightNode: 1827\n",
      "leftNode: 1502 rightNode: 2219\n",
      "leftNode: 1503 rightNode: 1960\n",
      "leftNode: 1503 rightNode: 2103\n",
      "leftNode: 1512 rightNode: 2110\n",
      "leftNode: 1514 rightNode: 1987\n",
      "leftNode: 1515 rightNode: 1749\n",
      "leftNode: 1517 rightNode: 1618\n",
      "leftNode: 1517 rightNode: 2160\n",
      "leftNode: 1518 rightNode: 1740\n",
      "leftNode: 1520 rightNode: 1827\n",
      "leftNode: 1526 rightNode: 973\n",
      "leftNode: 1528 rightNode: 1927\n",
      "leftNode: 1531 rightNode: 1694\n",
      "leftNode: 1531 rightNode: 1969\n",
      "leftNode: 1531 rightNode: 2239\n",
      "leftNode: 1536 rightNode: 2015\n",
      "leftNode: 1540 rightNode: 2385\n",
      "leftNode: 1542 rightNode: 2159\n",
      "leftNode: 1544 rightNode: 1437\n",
      "leftNode: 1545 rightNode: 2340\n",
      "leftNode: 1552 rightNode: 2302\n",
      "leftNode: 1553 rightNode: 2220\n",
      "leftNode: 1554 rightNode: 2082\n",
      "leftNode: 1559 rightNode: 1672\n",
      "leftNode: 1566 rightNode: 2216\n",
      "leftNode: 1568 rightNode: 1722\n",
      "leftNode: 1573 rightNode: 1814\n",
      "leftNode: 1577 rightNode: 1718\n",
      "leftNode: 1579 rightNode: 2296\n",
      "leftNode: 1581 rightNode: 1783\n",
      "leftNode: 1582 rightNode: 2181\n",
      "leftNode: 1582 rightNode: 2344\n",
      "leftNode: 1582 rightNode: 1779\n",
      "leftNode: 1583 rightNode: 1654\n",
      "leftNode: 1585 rightNode: 1886\n",
      "leftNode: 1587 rightNode: 2317\n",
      "leftNode: 1589 rightNode: 2368\n",
      "leftNode: 1591 rightNode: 1751\n",
      "leftNode: 1594 rightNode: 2081\n",
      "leftNode: 1597 rightNode: 2270\n",
      "leftNode: 1598 rightNode: 2113\n",
      "leftNode: 1598 rightNode: 2089\n",
      "leftNode: 1600 rightNode: 2242\n",
      "leftNode: 1602 rightNode: 2058\n",
      "leftNode: 1602 rightNode: 2208\n",
      "leftNode: 1602 rightNode: 2378\n",
      "leftNode: 1606 rightNode: 1627\n",
      "leftNode: 1611 rightNode: 1973\n",
      "leftNode: 1611 rightNode: 1934\n",
      "leftNode: 1613 rightNode: 2316\n",
      "leftNode: 1613 rightNode: 2348\n",
      "leftNode: 1615 rightNode: 1617\n",
      "leftNode: 1619 rightNode: 1516\n",
      "leftNode: 1620 rightNode: 2105\n",
      "leftNode: 1620 rightNode: 2268\n",
      "leftNode: 1620 rightNode: 2280\n",
      "leftNode: 1624 rightNode: 2146\n",
      "leftNode: 1627 rightNode: 1985\n",
      "leftNode: 1627 rightNode: 1978\n",
      "leftNode: 1628 rightNode: 2138\n",
      "leftNode: 1630 rightNode: 1915\n",
      "leftNode: 1637 rightNode: 2182\n",
      "leftNode: 1638 rightNode: 1053\n",
      "leftNode: 1639 rightNode: 1745\n",
      "leftNode: 1639 rightNode: 1654\n",
      "leftNode: 1647 rightNode: 1780\n",
      "leftNode: 1648 rightNode: 2187\n",
      "leftNode: 1650 rightNode: 1840\n",
      "leftNode: 1650 rightNode: 1912\n",
      "leftNode: 1653 rightNode: 1907\n",
      "leftNode: 1653 rightNode: 2051\n",
      "leftNode: 1654 rightNode: 1745\n",
      "leftNode: 1654 rightNode: 1761\n",
      "leftNode: 1654 rightNode: 2311\n",
      "leftNode: 1654 rightNode: 2313\n",
      "leftNode: 1654 rightNode: 1395\n",
      "leftNode: 1654 rightNode: 2345\n",
      "leftNode: 1654 rightNode: 1665\n",
      "leftNode: 1654 rightNode: 2355\n",
      "leftNode: 1654 rightNode: 1891\n",
      "leftNode: 1654 rightNode: 2224\n",
      "leftNode: 1654 rightNode: 301\n",
      "leftNode: 1654 rightNode: 2245\n",
      "leftNode: 1657 rightNode: 2306\n",
      "leftNode: 1661 rightNode: 2399\n",
      "leftNode: 1663 rightNode: 1945\n",
      "leftNode: 1665 rightNode: 2297\n",
      "leftNode: 1665 rightNode: 1827\n",
      "leftNode: 1668 rightNode: 2043\n",
      "leftNode: 1668 rightNode: 1796\n",
      "leftNode: 1668 rightNode: 1752\n",
      "leftNode: 1668 rightNode: 1759\n",
      "leftNode: 1668 rightNode: 1940\n",
      "leftNode: 1668 rightNode: 1149\n",
      "leftNode: 1668 rightNode: 2246\n",
      "leftNode: 1683 rightNode: 2341\n",
      "leftNode: 1690 rightNode: 1847\n",
      "leftNode: 1691 rightNode: 2282\n",
      "leftNode: 1694 rightNode: 1969\n",
      "leftNode: 1701 rightNode: 1431\n",
      "leftNode: 1702 rightNode: 1702\n",
      "leftNode: 1703 rightNode: 1781\n",
      "leftNode: 1703 rightNode: 1783\n",
      "leftNode: 1705 rightNode: 1945\n",
      "leftNode: 1711 rightNode: 1718\n",
      "leftNode: 1712 rightNode: 2021\n",
      "leftNode: 1712 rightNode: 1462\n",
      "leftNode: 1717 rightNode: 2360\n",
      "leftNode: 1720 rightNode: 1505\n",
      "leftNode: 1722 rightNode: 2007\n",
      "leftNode: 1722 rightNode: 2294\n",
      "leftNode: 1722 rightNode: 1760\n",
      "leftNode: 1724 rightNode: 2018\n",
      "leftNode: 1733 rightNode: 1823\n",
      "leftNode: 1738 rightNode: 2368\n",
      "leftNode: 1741 rightNode: 2040\n",
      "leftNode: 1744 rightNode: 2280\n",
      "leftNode: 1745 rightNode: 1921\n",
      "leftNode: 1745 rightNode: 2085\n",
      "leftNode: 1748 rightNode: 2229\n",
      "leftNode: 1748 rightNode: 2033\n",
      "leftNode: 1750 rightNode: 1273\n",
      "leftNode: 1750 rightNode: 1944\n",
      "leftNode: 1750 rightNode: 1959\n",
      "leftNode: 1750 rightNode: 2233\n",
      "leftNode: 1751 rightNode: 2174\n",
      "leftNode: 1752 rightNode: 2246\n",
      "leftNode: 1756 rightNode: 2211\n",
      "leftNode: 1759 rightNode: 2043\n",
      "leftNode: 1764 rightNode: 1764\n",
      "leftNode: 1765 rightNode: 1999\n",
      "leftNode: 1765 rightNode: 2335\n",
      "leftNode: 1766 rightNode: 2347\n",
      "leftNode: 1766 rightNode: 2153\n",
      "leftNode: 1779 rightNode: 962\n",
      "leftNode: 1779 rightNode: 2344\n",
      "leftNode: 1779 rightNode: 1831\n",
      "leftNode: 1779 rightNode: 2286\n",
      "leftNode: 1780 rightNode: 2280\n",
      "leftNode: 1782 rightNode: 1986\n",
      "leftNode: 1783 rightNode: 2318\n",
      "leftNode: 1786 rightNode: 2056\n",
      "leftNode: 1786 rightNode: 1929\n",
      "leftNode: 1789 rightNode: 2316\n",
      "leftNode: 1789 rightNode: 2051\n",
      "leftNode: 1791 rightNode: 2252\n",
      "leftNode: 1795 rightNode: 2130\n",
      "leftNode: 1798 rightNode: 2058\n",
      "leftNode: 1803 rightNode: 2337\n",
      "leftNode: 1807 rightNode: 1777\n",
      "leftNode: 1808 rightNode: 1808\n",
      "leftNode: 1808 rightNode: 1938\n",
      "leftNode: 1814 rightNode: 1622\n",
      "leftNode: 1827 rightNode: 1431\n",
      "leftNode: 1827 rightNode: 1959\n",
      "leftNode: 1831 rightNode: 2372\n",
      "leftNode: 1834 rightNode: 2238\n",
      "leftNode: 1835 rightNode: 1957\n",
      "leftNode: 1835 rightNode: 2018\n",
      "leftNode: 1840 rightNode: 2149\n",
      "leftNode: 1845 rightNode: 2012\n",
      "leftNode: 2282 rightNode: 1882\n",
      "leftNode: 1850 rightNode: 2134\n",
      "leftNode: 1850 rightNode: 2019\n",
      "leftNode: 1850 rightNode: 2131\n",
      "leftNode: 1851 rightNode: 2124\n",
      "leftNode: 1853 rightNode: 1104\n",
      "leftNode: 1857 rightNode: 2314\n",
      "leftNode: 1859 rightNode: 2004\n",
      "leftNode: 1862 rightNode: 1505\n",
      "leftNode: 1865 rightNode: 2306\n",
      "leftNode: 1866 rightNode: 1936\n",
      "leftNode: 1868 rightNode: 2284\n",
      "leftNode: 1875 rightNode: 2078\n",
      "leftNode: 1883 rightNode: 2268\n",
      "leftNode: 1887 rightNode: 988\n",
      "leftNode: 1891 rightNode: 301\n",
      "leftNode: 1896 rightNode: 2059\n",
      "leftNode: 1901 rightNode: 2377\n",
      "leftNode: 1901 rightNode: 1979\n",
      "leftNode: 1903 rightNode: 2278\n",
      "leftNode: 1903 rightNode: 2159\n",
      "leftNode: 1907 rightNode: 2051\n",
      "leftNode: 1909 rightNode: 2040\n",
      "leftNode: 1909 rightNode: 2149\n",
      "leftNode: 1912 rightNode: 2202\n",
      "leftNode: 1915 rightNode: 2303\n",
      "leftNode: 1918 rightNode: 1162\n",
      "leftNode: 1921 rightNode: 2078\n",
      "leftNode: 1924 rightNode: 2348\n",
      "leftNode: 1926 rightNode: 1374\n",
      "leftNode: 1929 rightNode: 2056\n",
      "leftNode: 1934 rightNode: 1225\n",
      "leftNode: 1936 rightNode: 2386\n",
      "leftNode: 1940 rightNode: 1964\n",
      "leftNode: 1940 rightNode: 1149\n",
      "leftNode: 1940 rightNode: 2246\n",
      "leftNode: 1943 rightNode: 2075\n",
      "leftNode: 1948 rightNode: 217\n",
      "leftNode: 1949 rightNode: 2018\n",
      "leftNode: 1957 rightNode: 2018\n",
      "leftNode: 1964 rightNode: 2024\n",
      "leftNode: 1964 rightNode: 2170\n",
      "leftNode: 1969 rightNode: 2239\n",
      "leftNode: 1973 rightNode: 2049\n",
      "leftNode: 1978 rightNode: 1985\n",
      "leftNode: 1988 rightNode: 1988\n",
      "leftNode: 1990 rightNode: 2021\n",
      "leftNode: 1990 rightNode: 1462\n",
      "leftNode: 2003 rightNode: 1315\n",
      "leftNode: 2009 rightNode: 2062\n",
      "leftNode: 2011 rightNode: 1813\n",
      "leftNode: 2011 rightNode: 2306\n",
      "leftNode: 2018 rightNode: 2107\n",
      "leftNode: 2018 rightNode: 2027\n",
      "leftNode: 2018 rightNode: 2236\n",
      "leftNode: 2019 rightNode: 2134\n",
      "leftNode: 2021 rightNode: 1462\n",
      "leftNode: 2023 rightNode: 2104\n",
      "leftNode: 2033 rightNode: 2229\n",
      "leftNode: 2037 rightNode: 2284\n",
      "leftNode: 2040 rightNode: 2214\n",
      "leftNode: 2053 rightNode: 2001\n",
      "leftNode: 2058 rightNode: 2208\n",
      "leftNode: 2058 rightNode: 2378\n",
      "leftNode: 2059 rightNode: 1960\n",
      "leftNode: 2068 rightNode: 2377\n",
      "leftNode: 2072 rightNode: 2323\n",
      "leftNode: 2075 rightNode: 2085\n",
      "leftNode: 2075 rightNode: 2327\n",
      "leftNode: 2077 rightNode: 2230\n",
      "leftNode: 2077 rightNode: 1618\n",
      "leftNode: 2078 rightNode: 2085\n",
      "leftNode: 2078 rightNode: 2284\n",
      "leftNode: 2080 rightNode: 2119\n",
      "leftNode: 2080 rightNode: 2112\n",
      "leftNode: 2081 rightNode: 2179\n",
      "leftNode: 2084 rightNode: 2049\n",
      "leftNode: 2091 rightNode: 1674\n",
      "leftNode: 2094 rightNode: 2137\n",
      "leftNode: 2100 rightNode: 2284\n",
      "leftNode: 2110 rightNode: 784\n",
      "leftNode: 2114 rightNode: 2190\n",
      "leftNode: 2114 rightNode: 2126\n",
      "leftNode: 2114 rightNode: 1812\n",
      "leftNode: 2117 rightNode: 2265\n",
      "leftNode: 2123 rightNode: 1370\n",
      "leftNode: 2123 rightNode: 1368\n",
      "leftNode: 2124 rightNode: 1375\n",
      "leftNode: 2131 rightNode: 2134\n",
      "leftNode: 2133 rightNode: 1375\n",
      "leftNode: 2135 rightNode: 2149\n",
      "leftNode: 2153 rightNode: 2347\n",
      "leftNode: 2159 rightNode: 2278\n",
      "leftNode: 2159 rightNode: 2315\n",
      "leftNode: 2160 rightNode: 1618\n",
      "leftNode: 2162 rightNode: 2341\n",
      "leftNode: 2167 rightNode: 2323\n",
      "leftNode: 2167 rightNode: 999\n",
      "leftNode: 2167 rightNode: 2394\n",
      "leftNode: 2174 rightNode: 2377\n",
      "leftNode: 2177 rightNode: 1370\n",
      "leftNode: 2183 rightNode: 2228\n",
      "leftNode: 2189 rightNode: 1429\n",
      "leftNode: 2192 rightNode: 2228\n",
      "leftNode: 2213 rightNode: 2323\n",
      "leftNode: 2216 rightNode: 2228\n",
      "leftNode: 2216 rightNode: 2268\n",
      "leftNode: 2228 rightNode: 2265\n",
      "leftNode: 2235 rightNode: 1296\n",
      "leftNode: 2235 rightNode: 1719\n",
      "leftNode: 2245 rightNode: 2284\n",
      "leftNode: 2249 rightNode: 293\n",
      "leftNode: 2252 rightNode: 2335\n",
      "leftNode: 2252 rightNode: 964\n",
      "leftNode: 2260 rightNode: 2156\n",
      "leftNode: 2275 rightNode: 2134\n",
      "leftNode: 2285 rightNode: 2333\n",
      "leftNode: 2301 rightNode: 818\n",
      "leftNode: 2304 rightNode: 2304\n",
      "leftNode: 2306 rightNode: 363\n",
      "leftNode: 2306 rightNode: 2400\n",
      "leftNode: 2311 rightNode: 732\n",
      "leftNode: 2311 rightNode: 2031\n",
      "leftNode: 2316 rightNode: 2348\n",
      "leftNode: 2323 rightNode: 2353\n",
      "leftNode: 2323 rightNode: 2394\n",
      "leftNode: 2323 rightNode: 999\n",
      "leftNode: 2323 rightNode: 2239\n",
      "leftNode: 2335 rightNode: 2335\n",
      "leftNode: 2335 rightNode: 2399\n",
      "leftNode: 2346 rightNode: 2367\n",
      "leftNode: 2349 rightNode: 537\n",
      "leftNode: 2355 rightNode: 2355\n",
      "leftNode: 2379 rightNode: 1716\n",
      "leftNode: 2380 rightNode: 2157\n",
      "leftNode: 1799 rightNode: 293\n",
      "leftNode: 1985 rightNode: 894\n",
      "leftNode: 949 rightNode: 1370\n",
      "leftNode: 1149 rightNode: 1796\n",
      "leftNode: 1431 rightNode: 1959\n",
      "leftNode: 331 rightNode: 1043\n",
      "leftNode: 1324 rightNode: 1324\n",
      "leftNode: 1719 rightNode: 1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|████████████████████████████████████████████████████████████████████████████| 1396/1396 [00:00<00:00, 8044.14it/s]\n",
      "Generating walks (CPU: 1): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 27.76it/s]\n",
      "Generating edge features: 100%|██████████████████████████████████████████████████████████████████████████████| 975106/975106.0 [00:06<00:00, 148817.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 2s 19ms/step - loss: 0.2139 - accuracy: 0.8975 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 7.3727e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.9789e-04 - accuracy: 1.0000 - val_loss: 1.0344e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 5.7884e-05 - accuracy: 1.0000 - val_loss: 3.6177e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 2.5127e-05 - accuracy: 1.0000 - val_loss: 1.9478e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 1.4703e-05 - accuracy: 1.0000 - val_loss: 1.2487e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 9.8240e-06 - accuracy: 1.0000 - val_loss: 8.7133e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 6.9973e-06 - accuracy: 1.0000 - val_loss: 6.3761e-06 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.1286e-06 - accuracy: 1.0000 - val_loss: 4.7454e-06 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.9053e-06 - accuracy: 1.0000 - val_loss: 3.6807e-06 - val_accuracy: 1.0000\n",
      "19/19 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacebook\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mpro_data_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 102\u001b[0m, in \u001b[0;36mpro_data_main\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     99\u001b[0m gen_history \u001b[38;5;241m=\u001b[39m train_model(gen_model, X_train, y_train, X_test, y_test)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Evaluate and plot metrics for generative model\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m plot_metrics(gen_history)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Build and train discriminative model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 72\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_test, y_test)\u001b[0m\n\u001b[0;32m     70\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[0;32m     71\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, y_pred)\n\u001b[1;32m---> 72\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, precision, auc\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:640\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[0;32m    639\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    649\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[0;32m    650\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    654\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_base.py:76\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     79\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load Graph Data and Create Graph\n",
    "def get_graph(filepath):\n",
    "    dataGraph = pd.read_csv(filepath, sep=' ', skiprows=0)\n",
    "    graph = nx.Graph()\n",
    "    edges = np.array(dataGraph)\n",
    "    for edge in edges:\n",
    "        print('leftNode:', edge[0], 'rightNode:', edge[1])\n",
    "        graph.add_edge(edge[0], edge[1])\n",
    "    return graph\n",
    "\n",
    "# Step 2: Generate Node2Vec Edge Embeddings\n",
    "def get_edge_embeddings(graph, savepath):\n",
    "    node2vec = Node2Vec(graph, dimensions=64, walk_length=5, num_walks=10, workers=1)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=1)\n",
    "    edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "    edges_kv = edges_embs.as_keyed_vectors()\n",
    "    edges_kv.save_word2vec_format(savepath)\n",
    "    return edges_kv\n",
    "\n",
    "# Step 3: Prepare Data for Generative and Discriminative Models\n",
    "def prepare_data(graph, edges_kv):\n",
    "    X, y = [], []\n",
    "    for edge in graph.edges():\n",
    "        u, v = edge\n",
    "        if f\"{u}_{v}\" in edges_kv:\n",
    "            X.append(edges_kv[f\"{u}_{v}\"])\n",
    "            y.append(1)  # Positive sample\n",
    "        else:\n",
    "            X.append(np.random.random(64))  # Random vector for negative sample\n",
    "            y.append(0)  # Negative sample\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Build Generative Model for Link Prediction\n",
    "def build_generative_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 5: Build Discriminative Classifier for Link Types\n",
    "def build_discriminative_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))  # Assuming 4 link types\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 6: Train the Models\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=10):\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_test, y_test))\n",
    "    return history\n",
    "\n",
    "# Step 7: Evaluate Model Performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, AUC: {auc}\")\n",
    "    return accuracy, precision, auc\n",
    "\n",
    "# Step 8: Plot Training Metrics\n",
    "def plot_metrics(history):\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Step 9: Main Function to Run the Process\n",
    "def pro_data_main(dataset):\n",
    "    print(\"Processing dataset:\", dataset)\n",
    "    pathReal = f'Datasets/{dataset}/realData.csv'\n",
    "    graph = get_graph(pathReal)  # Create graph from real data\n",
    "    savepath = f'Datasets/node2vecFeature/{dataset.lower()}Feature.txt'\n",
    "    edges_kv = get_edge_embeddings(graph, savepath)  # Generate edge embeddings\n",
    "\n",
    "    # Prepare data for training\n",
    "    X, y = prepare_data(graph, edges_kv)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Build and train generative model\n",
    "    gen_model = build_generative_model(X_train.shape[1])\n",
    "    gen_history = train_model(gen_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Evaluate and plot metrics for generative model\n",
    "    evaluate_model(gen_model, X_test, y_test)\n",
    "    plot_metrics(gen_history)\n",
    "\n",
    "    # Build and train discriminative model\n",
    "    dis_model = build_discriminative_model(X_train.shape[1])\n",
    "    dis_history = train_model(dis_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Evaluate and plot metrics for discriminative model\n",
    "    evaluate_model(dis_model, X_test, y_test)\n",
    "    plot_metrics(dis_history)\n",
    "\n",
    "# Entry Point\n",
    "if __name__ == '__main__':\n",
    "    datasets = ['Facebook']\n",
    "    for dataset in datasets:\n",
    "        pro_data_main(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de02483a-c03a-4813-a633-c5a8ea9610aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 198\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 198\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 184\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    183\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacebook\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 184\u001b[0m     train_loader, validate_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_train_validate_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m genetic_algorithm(train_loader, validate_loader)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 66\u001b[0m, in \u001b[0;36mget_train_validate_test\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     64\u001b[0m edgeFeature \u001b[38;5;241m=\u001b[39m edgeFeatures()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# In the loop over the edges\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m nodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnodeL\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Clean and strip unwanted characters\u001b[39;00m\n\u001b[0;32m     67\u001b[0m noder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(nodeR[i][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()))  \u001b[38;5;66;03m# Handle parentheses or stray symbols\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_real\u001b[38;5;241m.\u001b[39mhas_edge(nodel, noder) \u001b[38;5;129;01mor\u001b[39;00m train_fake\u001b[38;5;241m.\u001b[39mhas_edge(nodel, noder):\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import re, networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, os, random\n",
    "\n",
    "# ===================\n",
    "# GA CONFIGURATION\n",
    "# ===================\n",
    "NUM_EPOCHS = 50\n",
    "POPULATION_SIZE = 10\n",
    "NUM_GENERATIONS = 5\n",
    "MUTATION_RATE = 0.1\n",
    "\n",
    "class edgeFeatures:\n",
    "    def __init__(self, label=None, type=None, embeddings=None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ')\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ')\n",
    "\n",
    "    train_Real_Graph = nx.Graph()\n",
    "    train_Fake_Graph = nx.Graph()\n",
    "    test_Real_Graph = nx.Graph()\n",
    "    test_Fake_Graph = nx.Graph()\n",
    "\n",
    "    real_edges = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edges = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    dataNewType = [9, 8, 7, 6, 5, 4] if dataset.lower() == 'facebook' else [2]\n",
    "\n",
    "    for edge in real_edges:\n",
    "        graph = test_Real_Graph if edge[2] in dataNewType else train_Real_Graph\n",
    "        graph.add_edge(edge[0], edge[1], relationship=edge[2])\n",
    "\n",
    "    for edge in fake_edges:\n",
    "        graph = test_Fake_Graph if edge[2] in dataNewType else train_Fake_Graph\n",
    "        graph.add_edge(edge[0], edge[1], relationship=edge[2])\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    realFile = f'Datasets/{dataset}/realData.csv'\n",
    "    fakeFile = f'Datasets/{dataset}/fakeData.csv'\n",
    "    train_real, train_fake, test_real, test_fake = structuralGraph(realFile, fakeFile, dataset)\n",
    "\n",
    "    node2vecFile = f'Datasets/node2vecFeature/{dataset}Feature.txt'\n",
    "    data = pd.read_csv(node2vecFile, sep=' ', skiprows=1, header=None)\n",
    "\n",
    "    edges = np.array(data.iloc[:, 0:2])\n",
    "    embeddings = np.array(data.iloc[:, 2:])\n",
    "    \n",
    "    train_data, test_data = [], []\n",
    "\n",
    "    for i, (nodeL, nodeR) in enumerate(edges):\n",
    "        edgeFeature = edgeFeatures()\n",
    "        # In the loop over the edges\n",
    "        nodel = int(re.sub(r\"\\D\", \"\", str(nodeL[i][0]).strip()))  # Clean and strip unwanted characters\n",
    "        noder = int(re.sub(r\"\\D\", \"\", str(nodeR[i][0]).strip()))  # Handle parentheses or stray symbols\n",
    "\n",
    "        \n",
    "        if train_real.has_edge(nodel, noder) or train_fake.has_edge(nodel, noder):\n",
    "            label = 1 if train_real.has_edge(nodel, noder) else 0\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_real.has_edge(nodel, noder) or test_fake.has_edge(nodel, noder):\n",
    "            label = 1 if test_real.has_edge(nodel, noder) else 0\n",
    "            edgeFeature.embeddings = embeddings[i]\n",
    "            edgeFeature.label = label\n",
    "            test_data.append(edgeFeature)\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)\n",
    "    return create_dataloader(train), create_dataloader(validate), create_dataloader(test_data)\n",
    "\n",
    "def create_dataloader(data):\n",
    "    dataset = [[torch.tensor(e.embeddings, dtype=torch.float32),\n",
    "                torch.tensor(e.label, dtype=torch.float32)] for e in data]\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambd):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ANN, self).__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(64, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = Variable(data), Variable(labels.long())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    return metrics.accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# ============================\n",
    "# GA OPTIMIZATION FUNCTIONS\n",
    "# ============================\n",
    "def initialize_population():\n",
    "    return [{'lr': random.uniform(0.0001, 0.01),\n",
    "             'hidden_dim': random.choice([8, 16, 32, 64]),\n",
    "             'batch_size': random.choice([16, 32, 64])} for _ in range(POPULATION_SIZE)]\n",
    "\n",
    "def mutate(params):\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        params['lr'] = random.uniform(0.0001, 0.01)\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        params['hidden_dim'] = random.choice([8, 16, 32, 64])\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        params['batch_size'] = random.choice([16, 32, 64])\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    for key in parent1:\n",
    "        child[key] = parent1[key] if random.random() > 0.5 else parent2[key]\n",
    "    return child\n",
    "\n",
    "def genetic_algorithm(train_loader, validate_loader):\n",
    "    population = initialize_population()\n",
    "    best_params, best_acc = None, 0\n",
    "\n",
    "    for generation in range(NUM_GENERATIONS):\n",
    "        scores = []\n",
    "        for params in population:\n",
    "            model = ANN(params['hidden_dim'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "            train(model, train_loader, criterion, optimizer)\n",
    "            acc = evaluate(model, validate_loader)\n",
    "            scores.append((acc, params))\n",
    "\n",
    "        scores.sort(reverse=True, key=lambda x: x[0])\n",
    "        best_acc, best_params = scores[0]\n",
    "\n",
    "        next_gen = [scores[0][1], scores[1][1]]  # Keep top 2\n",
    "        for _ in range(POPULATION_SIZE - 2):\n",
    "            parent1, parent2 = random.sample(scores[:5], 2)\n",
    "            child = crossover(parent1[1], parent2[1])\n",
    "            mutate(child)\n",
    "            next_gen.append(child)\n",
    "\n",
    "        population = next_gen\n",
    "        print(f'Generation {generation}, Best Accuracy: {best_acc}')\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def main():\n",
    "    dataset = 'Facebook'\n",
    "    train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "\n",
    "    best_params = genetic_algorithm(train_loader, validate_loader)\n",
    "    print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "    model = ANN(best_params['hidden_dim'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "    train(model, train_loader, criterion, optimizer)\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d928dbeb-5f69-471f-8376-cda1384bda53",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([real_data, fake_data])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Encode link types\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m link_types \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     17\u001b[0m type_to_idx \u001b[38;5;241m=\u001b[39m {link_type: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, link_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(link_types)}\n\u001b[0;32m     18\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(type_to_idx)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load datasets\n",
    "real_data = pd.read_csv('realData.csv')\n",
    "fake_data = pd.read_csv('fakeData.csv')\n",
    "\n",
    "# Combine datasets\n",
    "data = pd.concat([real_data, fake_data])\n",
    "\n",
    "# Encode link types\n",
    "link_types = data['type'].unique()\n",
    "type_to_idx = {link_type: idx for idx, link_type in enumerate(link_types)}\n",
    "data['type'] = data['type'].map(type_to_idx)\n",
    "\n",
    "# Define dataset class\n",
    "class LinkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return {\n",
    "            'id_left': torch.tensor(row['id_left'], dtype=torch.long),\n",
    "            'id_right': torch.tensor(row['id_right'], dtype=torch.long),\n",
    "            'type': torch.tensor(row['type'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define the generative predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the discriminative classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_types):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_types)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models, loss functions, and optimizers\n",
    "input_dim = 2  # id_left and id_right\n",
    "hidden_dim = 128\n",
    "num_types = len(link_types)\n",
    "\n",
    "gen_predictor = GenerativePredictor(input_dim, hidden_dim)\n",
    "disc_classifier = DiscriminativeClassifier(input_dim, hidden_dim, num_types)\n",
    "\n",
    "criterion_gen = nn.BCELoss()\n",
    "criterion_disc = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_gen = optim.Adam(gen_predictor.parameters(), lr=0.001)\n",
    "optimizer_disc = optim.Adam(disc_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare data loader\n",
    "dataset = LinkDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        id_left = batch['id_left'].float()\n",
    "        id_right = batch['id_right'].float()\n",
    "        link_type = batch['type']\n",
    "\n",
    "        # Generative predictor forward pass\n",
    "        gen_input = torch.stack((id_left, id_right), dim=1)\n",
    "        gen_output = gen_predictor(gen_input)\n",
    "        gen_loss = criterion_gen(gen_output, link_type.float())\n",
    "\n",
    "        # Discriminative classifier forward pass\n",
    "        disc_output = disc_classifier(gen_input)\n",
    "        disc_loss = criterion_disc(disc_output, link_type)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer_gen.zero_grad()\n",
    "        optimizer_disc.zero_grad()\n",
    "\n",
    "        gen_loss.backward(retain_graph=True)\n",
    "        disc_loss.backward()\n",
    "\n",
    "        optimizer_gen.step()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Gen Loss: {gen_loss.item():.4f}, Disc Loss: {disc_loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d69ba50-c61a-4848-9675-495c129359b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([real_data, fake_data])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Encode link types\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m link_types \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     20\u001b[0m type_to_idx \u001b[38;5;241m=\u001b[39m {link_type: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, link_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(link_types)}\n\u001b[0;32m     21\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(type_to_idx)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load datasets\n",
    "real_data = pd.read_csv('realData.csv')\n",
    "fake_data = pd.read_csv('fakeData.csv')\n",
    "\n",
    "# Combine datasets\n",
    "data = pd.concat([real_data, fake_data])\n",
    "\n",
    "# Encode link types\n",
    "link_types = data['type'].unique()\n",
    "type_to_idx = {link_type: idx for idx, link_type in enumerate(link_types)}\n",
    "data['type'] = data['type'].map(type_to_idx)\n",
    "\n",
    "# Define dataset class\n",
    "class LinkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return {\n",
    "            'id_left': torch.tensor(row['id_left'], dtype=torch.long),\n",
    "            'id_right': torch.tensor(row['id_right'], dtype=torch.long),\n",
    "            'type': torch.tensor(row['type'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define the generative predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the discriminative classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_types):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_types)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models, loss functions, and optimizers\n",
    "input_dim = 2  # id_left and id_right\n",
    "hidden_dim = 128\n",
    "num_types = len(link_types)\n",
    "\n",
    "gen_predictor = GenerativePredictor(input_dim, hidden_dim)\n",
    "disc_classifier = DiscriminativeClassifier(input_dim, hidden_dim, num_types)\n",
    "\n",
    "criterion_gen = nn.BCELoss()\n",
    "criterion_disc = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_gen = optim.Adam(gen_predictor.parameters(), lr=0.001)\n",
    "optimizer_disc = optim.Adam(disc_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare data loader\n",
    "dataset = LinkDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        id_left = batch['id_left'].float()\n",
    "        id_right = batch['id_right'].float()\n",
    "        link_type = batch['type']\n",
    "\n",
    "        # Generative predictor forward pass\n",
    "        gen_input = torch.stack((id_left, id_right), dim=1)\n",
    "        gen_output = gen_predictor(gen_input)\n",
    "        gen_loss = criterion_gen(gen_output, link_type.float())\n",
    "\n",
    "        # Discriminative classifier forward pass\n",
    "        disc_output = disc_classifier(gen_input)\n",
    "        disc_loss = criterion_disc(disc_output, link_type)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer_gen.zero_grad()\n",
    "        optimizer_disc.zero_grad()\n",
    "\n",
    "        gen_loss.backward(retain_graph=True)\n",
    "        disc_loss.backward()\n",
    "\n",
    "        optimizer_gen.step()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Gen Loss: {gen_loss.item():.4f}, Disc Loss: {disc_loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Additional functions for graph processing and edge embeddings\n",
    "def get_graph(filepath):\n",
    "    dataGraph = pd.read_csv(filepath, sep=' ', skiprows=0)\n",
    "    graph = nx.Graph()\n",
    "    edges = np.array(dataGraph)\n",
    "    for edge in edges:\n",
    "        print('leftNode:', edge[0], 'rightNode:', edge[1])\n",
    "        graph.add_edge(edge[0], edge[1])\n",
    "    return graph\n",
    "\n",
    "def get_edge_embeddings(graph, savepath):  # node2vec生成边向量表征(64维)\n",
    "    node2vec = Node2Vec(graph, dimensions=64, walk_length=5, num_walks=10, workers=1)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=1)\n",
    "    edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "    edges_kv = edges_embs.as_keyed_vectors()\n",
    "    edges_kv.save_word2vec_format(savepath)\n",
    "\n",
    "def pro_data_main(dataset):\n",
    "    print(\"Input dataset:\", dataset)\n",
    "    pathReal = './' + dataset + '/realData.csv'\n",
    "    #only real graph to be embedded\n",
    "    # pathFake = 'Data/' + dataset + '/fakeData.csv'\n",
    "    graph = get_graph(pathReal) #real graph\n",
    "    savepath = './node2vecFeature/' + dataset.lower()+ 'Feature.txt'  # 最终数据\n",
    "    get_edge_embeddings(graph, savepath)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = ['Facebook']\n",
    "    for dataset in datasets:\n",
    "        pro_data_main(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c27d6747-e1f1-49d0-8e1f-33b34d3f2246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m gen_predictor\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     75\u001b[0m disc_classifier\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 76\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Train the generative predictor\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:221\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the dataset class\n",
    "class LinkDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define the generative predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the discriminative classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Load the dataset\n",
    "url = \"realData.csv\"  # Replace with the actual dataset URL\n",
    "data = pd.read_csv(url)\n",
    "data = data.values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "np.random.shuffle(data)\n",
    "train_data = data[:int(0.8 * len(data))]\n",
    "test_data = data[int(0.8 * len(data)):]\n",
    "\n",
    "train_dataset = LinkDataset(train_data)\n",
    "test_dataset = LinkDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the models\n",
    "input_dim = train_data.shape[1] - 1\n",
    "hidden_dim = 64\n",
    "gen_predictor = GenerativePredictor(input_dim, hidden_dim)\n",
    "disc_classifier = DiscriminativeClassifier(input_dim, hidden_dim)\n",
    "\n",
    "# Define the loss functions and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(gen_predictor.parameters(), lr=0.001)\n",
    "disc_optimizer = optim.Adam(disc_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    gen_predictor.train()\n",
    "    disc_classifier.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch[:, :-1].float(), batch[:, -1].float()\n",
    "\n",
    "        # Train the generative predictor\n",
    "        gen_optimizer.zero_grad()\n",
    "        gen_outputs = gen_predictor(inputs)\n",
    "        gen_loss = criterion(gen_outputs, labels)\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Train the discriminative classifier\n",
    "        disc_optimizer.zero_grad()\n",
    "        disc_outputs = disc_classifier(inputs)\n",
    "        disc_loss = criterion(disc_outputs, labels)\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Gen Loss: {gen_loss.item()}, Disc Loss: {disc_loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "gen_predictor.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch[:, :-1].float(), batch[:, -1].float()\n",
    "        outputs = gen_predictor(inputs)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48ef2198-3a69-4142-8e33-f98cb62af92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss Gen: 0.6960, Loss Disc: 0.6903\n",
      "Epoch [20/100], Loss Gen: 0.6808, Loss Disc: 0.6944\n",
      "Epoch [30/100], Loss Gen: 0.7042, Loss Disc: 0.6856\n",
      "Epoch [40/100], Loss Gen: 0.7242, Loss Disc: 0.7080\n",
      "Epoch [50/100], Loss Gen: 0.6730, Loss Disc: 0.6917\n",
      "Epoch [60/100], Loss Gen: 0.7116, Loss Disc: 0.6883\n",
      "Epoch [70/100], Loss Gen: 0.7131, Loss Disc: 0.6828\n",
      "Epoch [80/100], Loss Gen: 0.7174, Loss Disc: 0.6905\n",
      "Epoch [90/100], Loss Gen: 0.6927, Loss Disc: 0.7129\n",
      "Epoch [100/100], Loss Gen: 0.6712, Loss Disc: 0.6950\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Generative Predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the Discriminative Classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Initialize models\n",
    "input_dim = 128  # Example input dimension\n",
    "hidden_dim = 64  # Example hidden dimension\n",
    "output_dim = 1   # Output dimension for predictor (binary classification)\n",
    "\n",
    "gen_predictor = GenerativePredictor(input_dim, hidden_dim, output_dim)\n",
    "disc_classifier = DiscriminativeClassifier(input_dim, hidden_dim, 2)  # Assuming 2 link types\n",
    "\n",
    "# Loss functions and optimizers\n",
    "criterion_gen = nn.BCELoss()\n",
    "criterion_disc = nn.CrossEntropyLoss()\n",
    "optimizer_gen = optim.Adam(gen_predictor.parameters(), lr=0.001)\n",
    "optimizer_disc = optim.Adam(disc_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Sample data (replace with actual data loading)\n",
    "    data = torch.randn(32, input_dim)  # Example batch of data\n",
    "    labels = torch.randint(0, 2, (32,))  # Example labels for discriminator\n",
    "    missing_links = torch.randint(0, 2, (32, 1)).float()  # Example missing link labels\n",
    "\n",
    "    # Train Generative Predictor\n",
    "    optimizer_gen.zero_grad()\n",
    "    gen_output = gen_predictor(data)\n",
    "    loss_gen = criterion_gen(gen_output, missing_links)\n",
    "    loss_gen.backward()\n",
    "    optimizer_gen.step()\n",
    "\n",
    "    # Train Discriminative Classifier\n",
    "    optimizer_disc.zero_grad()\n",
    "    disc_output = disc_classifier(data)\n",
    "    loss_disc = criterion_disc(disc_output, labels)\n",
    "    loss_disc.backward()\n",
    "    optimizer_disc.step()\n",
    "\n",
    "    # Print losses\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss Gen: {loss_gen.item():.4f}, Loss Disc: {loss_disc.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e02405a8-d447-4441-a47b-8e4c511c9ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Gen Loss: 1.0909, Disc Loss: 1.0909\n",
      "Epoch [2/50], Gen Loss: 1.1011, Disc Loss: 1.1031\n",
      "Epoch [3/50], Gen Loss: 1.0916, Disc Loss: 1.0921\n",
      "Epoch [4/50], Gen Loss: 1.0914, Disc Loss: 1.0921\n",
      "Epoch [5/50], Gen Loss: 1.0857, Disc Loss: 1.0863\n",
      "Epoch [6/50], Gen Loss: 1.0844, Disc Loss: 1.0861\n",
      "Epoch [7/50], Gen Loss: 1.0767, Disc Loss: 1.0871\n",
      "Epoch [8/50], Gen Loss: 1.0548, Disc Loss: 1.0547\n",
      "Epoch [9/50], Gen Loss: 1.0800, Disc Loss: 1.0818\n",
      "Epoch [10/50], Gen Loss: 1.0694, Disc Loss: 1.0707\n",
      "Epoch [11/50], Gen Loss: 1.0719, Disc Loss: 1.0753\n",
      "Epoch [12/50], Gen Loss: 1.0959, Disc Loss: 1.0973\n",
      "Epoch [13/50], Gen Loss: 1.0563, Disc Loss: 1.0577\n",
      "Epoch [14/50], Gen Loss: 1.0434, Disc Loss: 1.0409\n",
      "Epoch [15/50], Gen Loss: 1.0537, Disc Loss: 1.0545\n",
      "Epoch [16/50], Gen Loss: 1.0064, Disc Loss: 1.0192\n",
      "Epoch [17/50], Gen Loss: 1.0751, Disc Loss: 1.0781\n",
      "Epoch [18/50], Gen Loss: 0.9992, Disc Loss: 1.0045\n",
      "Epoch [19/50], Gen Loss: 0.9965, Disc Loss: 0.9992\n",
      "Epoch [20/50], Gen Loss: 1.0278, Disc Loss: 1.0310\n",
      "Epoch [21/50], Gen Loss: 0.9834, Disc Loss: 0.9887\n",
      "Epoch [22/50], Gen Loss: 1.0131, Disc Loss: 1.0157\n",
      "Epoch [23/50], Gen Loss: 0.9871, Disc Loss: 1.0078\n",
      "Epoch [24/50], Gen Loss: 1.0133, Disc Loss: 1.0129\n",
      "Epoch [25/50], Gen Loss: 0.9807, Disc Loss: 0.9867\n",
      "Epoch [26/50], Gen Loss: 0.9813, Disc Loss: 0.9938\n",
      "Epoch [27/50], Gen Loss: 1.0023, Disc Loss: 1.0118\n",
      "Epoch [28/50], Gen Loss: 0.9865, Disc Loss: 0.9933\n",
      "Epoch [29/50], Gen Loss: 0.9964, Disc Loss: 1.0044\n",
      "Epoch [30/50], Gen Loss: 0.9809, Disc Loss: 0.9858\n",
      "Epoch [31/50], Gen Loss: 1.0484, Disc Loss: 1.0546\n",
      "Epoch [32/50], Gen Loss: 0.9563, Disc Loss: 0.9593\n",
      "Epoch [33/50], Gen Loss: 1.0495, Disc Loss: 1.0540\n",
      "Epoch [34/50], Gen Loss: 0.9606, Disc Loss: 0.9628\n",
      "Epoch [35/50], Gen Loss: 0.9390, Disc Loss: 0.9400\n",
      "Epoch [36/50], Gen Loss: 1.0495, Disc Loss: 1.0529\n",
      "Epoch [37/50], Gen Loss: 0.9355, Disc Loss: 0.9417\n",
      "Epoch [38/50], Gen Loss: 0.9476, Disc Loss: 0.9505\n",
      "Epoch [39/50], Gen Loss: 1.0347, Disc Loss: 1.0344\n",
      "Epoch [40/50], Gen Loss: 0.9858, Disc Loss: 0.9874\n",
      "Epoch [41/50], Gen Loss: 0.9516, Disc Loss: 0.9641\n",
      "Epoch [42/50], Gen Loss: 0.9623, Disc Loss: 0.9698\n",
      "Epoch [43/50], Gen Loss: 0.9917, Disc Loss: 1.0055\n",
      "Epoch [44/50], Gen Loss: 0.9033, Disc Loss: 0.9069\n",
      "Epoch [45/50], Gen Loss: 0.9438, Disc Loss: 0.9593\n",
      "Epoch [46/50], Gen Loss: 0.8616, Disc Loss: 0.8588\n",
      "Epoch [47/50], Gen Loss: 0.9609, Disc Loss: 0.9692\n",
      "Epoch [48/50], Gen Loss: 0.9819, Disc Loss: 0.9853\n",
      "Epoch [49/50], Gen Loss: 0.8597, Disc Loss: 0.8638\n",
      "Epoch [50/50], Gen Loss: 0.8627, Disc Loss: 0.8636\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Sample Hyperparameters\n",
    "input_dim = 128   # Dimension of input link features\n",
    "hidden_dim = 64   # Hidden layer size\n",
    "num_link_types = 3  # Number of link types in the heterogeneous network\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Generative Predictor Network (Generator)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),  # Predicts if a link exists (binary classification)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminative Classifier Network (Discriminator)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_link_types):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_link_types),  # Classifies link type\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Adversarial Training: Generator tries to fool the Discriminator\n",
    "def train(generator, discriminator, data_loader, gen_optimizer, disc_optimizer, criterion):\n",
    "    for epoch in range(epochs):\n",
    "        for links, link_types in data_loader:\n",
    "            links, link_types = links.to(device), link_types.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            disc_optimizer.zero_grad()\n",
    "            type_preds = discriminator(links)  # Predict link types\n",
    "            disc_loss = criterion(type_preds, link_types)\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            gen_optimizer.zero_grad()\n",
    "            predicted_links = generator(links)  # Predict missing links\n",
    "            fake_type_preds = discriminator(links)  # Attempt to fool the discriminator\n",
    "\n",
    "            # Adversarial loss: Encourage the generator to learn transferable features\n",
    "            gen_loss = criterion(fake_type_preds, link_types)\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Gen Loss: {gen_loss.item():.4f}, Disc Loss: {disc_loss.item():.4f}')\n",
    "\n",
    "# Create random synthetic data for demonstration (replace with real data)\n",
    "num_samples = 1000\n",
    "link_features = torch.rand(num_samples, input_dim)  # Random link features\n",
    "link_types = torch.randint(0, num_link_types, (num_samples,))  # Random link types\n",
    "\n",
    "# DataLoader for batching\n",
    "dataset = TensorDataset(link_features, link_types)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(input_dim, hidden_dim).to(device)\n",
    "discriminator = Discriminator(input_dim, hidden_dim, num_link_types).to(device)\n",
    "\n",
    "# Optimizers and Loss function\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the MTTM model\n",
    "train(generator, discriminator, data_loader, gen_optimizer, disc_optimizer, criterion)\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c19d6f6-fbb5-43d1-85e6-87a3fe125a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string 'Neural_Networks' to float32 at row 0, column 1435.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Neural_Networks'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m G, features \u001b[38;5;241m=\u001b[39m \u001b[43mload_cora\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CoraDataset(G, features)\n\u001b[0;32m     64\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[25], line 13\u001b[0m, in \u001b[0;36mload_cora\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_cora\u001b[39m():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Load the dataset from a file or URL\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# For simplicity, we use a preprocessed version\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mread_edgelist(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcora.cites\u001b[39m\u001b[38;5;124m'\u001b[39m, create_using\u001b[38;5;241m=\u001b[39mnx\u001b[38;5;241m.\u001b[39mDiGraph(), nodetype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcora.content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m G, features\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:1016\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1016\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string 'Neural_Networks' to float32 at row 0, column 1435."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load the Cora dataset\n",
    "def load_cora():\n",
    "    # Load the dataset from a file or URL\n",
    "    # For simplicity, we use a preprocessed version\n",
    "    G = nx.read_edgelist('cora.cites', create_using=nx.DiGraph(), nodetype=int)\n",
    "    features = np.loadtxt('cora.content', dtype=np.float32)\n",
    "    return G, features\n",
    "\n",
    "# Custom Dataset class for DataLoader\n",
    "class CoraDataset(Dataset):\n",
    "    def __init__(self, G, features):\n",
    "        self.G = G\n",
    "        self.features = features\n",
    "        self.nodes = list(G.nodes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nodes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        node = self.nodes[idx]\n",
    "        neighbors = list(self.G.neighbors(node))\n",
    "        if neighbors:\n",
    "            neighbor = np.random.choice(neighbors)\n",
    "            label = 1\n",
    "        else:\n",
    "            neighbor = np.random.choice(self.nodes)\n",
    "            label = 0\n",
    "        return self.features[node], self.features[neighbor], label\n",
    "\n",
    "# Define the Generative Predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the Discriminative Classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Load dataset\n",
    "G, features = load_cora()\n",
    "dataset = CoraDataset(G, features)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "input_dim = features.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "\n",
    "gen_predictor = GenerativePredictor(input_dim, hidden_dim, output_dim)\n",
    "disc_classifier = DiscriminativeClassifier(input_dim, hidden_dim, 2)\n",
    "\n",
    "# Loss functions and optimizers\n",
    "criterion_gen = nn.BCELoss()\n",
    "criterion_disc = nn.CrossEntropyLoss()\n",
    "optimizer_gen = optim.Adam(gen_predictor.parameters(), lr=0.001)\n",
    "optimizer_disc = optim.Adam(disc_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        node_features, neighbor_features, labels = data\n",
    "        node_features, neighbor_features, labels = node_features.float(), neighbor_features.float(), labels.float()\n",
    "        \n",
    "        # Train Generative Predictor\n",
    "        optimizer_gen.zero_grad()\n",
    "        gen_output = gen_predictor(node_features)\n",
    "        loss_gen = criterion_gen(gen_output, labels.unsqueeze(1))\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # Train Discriminative Classifier\n",
    "        optimizer_disc.zero_grad()\n",
    "        disc_output = disc_classifier(node_features)\n",
    "        loss_disc = criterion_disc(disc_output, labels.long())\n",
    "        loss_disc.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "    # Print losses\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss Gen: {loss_gen.item():.4f}, Loss Disc: {loss_disc.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67108654-38ad-4a75-9e5c-255bbf446062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset: facebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|████████████████████████████████████████████████████████████████████████████| 1396/1396 [00:00<00:00, 5744.47it/s]\n",
      "Generating walks (CPU: 1): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 23.20it/s]\n",
      "Generating edge features: 100%|██████████████████████████████████████████████████████████████████████████████| 975106/975106.0 [00:07<00:00, 126689.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: Index(['id_left id_right type'], dtype='object')\n",
      "'type' column not found. Assigning default value.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id_left'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id_left'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m process_data(dataset_name)  \u001b[38;5;66;03m# This will create the embeddings\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Load your dataset (update the path)\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m node_pairs, link_types \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDatasets/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/realData.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with your CSV file path\u001b[39;00m\n\u001b[0;32m    100\u001b[0m train_data, test_data, train_labels, test_labels \u001b[38;5;241m=\u001b[39m train_test_split(node_pairs, link_types, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 55\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     51\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Prepare node pairs as input features\u001b[39;00m\n\u001b[0;32m     54\u001b[0m node_pairs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m---> 55\u001b[0m     (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid_left\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_right\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node_pairs, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id_left'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))  # Predict missing link probabilities\n",
    "\n",
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # Predict link type (real or fake)\n",
    "\n",
    "# Load and preprocess the Facebook dataset\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(\"Available columns:\", df.columns)  # Debugging step\n",
    "\n",
    "    # Use 'type' column if available, else assign a default value\n",
    "    if 'type' not in df.columns:\n",
    "        print(\"'type' column not found. Assigning default value.\")\n",
    "        df['type'] = 0  # Assign default label if missing\n",
    "\n",
    "    # Encode the type labels\n",
    "    le = LabelEncoder()\n",
    "    df['type'] = le.fit_transform(df['type'])\n",
    "\n",
    "    # Prepare node pairs as input features\n",
    "    node_pairs = np.concatenate(\n",
    "        (df['id_left'].values.reshape(-1, 1), df['id_right'].values.reshape(-1, 1)), axis=1\n",
    "    )\n",
    "    \n",
    "    return node_pairs, df['type'].values\n",
    "\n",
    "\n",
    "# Create graph and generate edge embeddings using Node2Vec\n",
    "def get_graph(filepath):\n",
    "    dataGraph = pd.read_csv(filepath, sep=' ', skiprows=0)\n",
    "    graph = nx.Graph()\n",
    "    edges = np.array(dataGraph)\n",
    "    for edge in edges:\n",
    "        graph.add_edge(edge[0], edge[1])\n",
    "    return graph\n",
    "\n",
    "def get_edge_embeddings(graph, savepath):\n",
    "    node2vec = Node2Vec(graph, dimensions=64, walk_length=5, num_walks=10, workers=1)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=1)\n",
    "    edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "    edges_kv = edges_embs.as_keyed_vectors()\n",
    "    edges_kv.save_word2vec_format(savepath)\n",
    "\n",
    "def process_data(dataset):\n",
    "    print(\"Input dataset:\", dataset)\n",
    "    pathReal = f'Datasets/{dataset}/realData.csv'  # Adjust path as needed\n",
    "    graph = get_graph(pathReal)  # Real graph\n",
    "    savepath = f'Datasets/node2vecFeature/{dataset.lower()}Feature.txt'  # Final data\n",
    "    get_edge_embeddings(graph, savepath)\n",
    "\n",
    "# Initialize models\n",
    "input_dim = 128  # Adjust this according to the edge embedding size\n",
    "generator = Generator(input_dim=input_dim, hidden_dim=64, output_dim=1)\n",
    "discriminator = Discriminator(input_dim=input_dim, hidden_dim=64)\n",
    "\n",
    "# Optimizers and loss\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Load the Facebook dataset and generate embeddings\n",
    "dataset_name = 'facebook'  # Replace with your dataset name\n",
    "process_data(dataset_name)  # This will create the embeddings\n",
    "\n",
    "# Load your dataset (update the path)\n",
    "node_pairs, link_types = load_data(f'Datasets/{dataset_name}/realData.csv')  # Replace with your CSV file path\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(node_pairs, link_types, test_size=0.2)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    for i in range(len(train_data)):\n",
    "        x = torch.FloatTensor(train_data[i]).unsqueeze(0)  # Shape (1, 2)\n",
    "\n",
    "        # Train discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        real_pred = discriminator(x)\n",
    "\n",
    "        # Assuming link is real for training discriminator\n",
    "        d_loss = loss_fn(real_pred, torch.tensor([[1.0]]))  # Real link\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        g_optimizer.zero_grad()\n",
    "        fake_link = generator(x).detach()\n",
    "        fake_pred = discriminator(fake_link)\n",
    "        g_loss = loss_fn(fake_pred, torch.tensor([[0.0]]))  # Fake link\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d158aea7-512c-4c6e-8ea0-0b143125daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-12 18:17:53,122] A new study created in memory with name: no-name-daec97ec-1e92-4901-a163-1c012626a942\n",
      "C:\\Users\\AHAN\\AppData\\Local\\Temp\\ipykernel_3300\\1158932190.py:143: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "[W 2024-10-12 18:18:35,300] Trial 0 failed with parameters: {'batch_size': 32, 'learning_rate': 0.006023102785592278, 'hidden_dim': 8} because of the following error: TypeError(\"default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class '__main__.edgeFeatures'>\").\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\AHAN\\AppData\\Local\\Temp\\ipykernel_3300\\1158932190.py\", line 153, in objective\n",
      "    for data, labels, _ in train_loader:\n",
      "  File \"C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 317, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\AHAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 192, in collate\n",
      "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
      "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class '__main__.edgeFeatures'>\n",
      "[W 2024-10-12 18:18:35,303] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class '__main__.edgeFeatures'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 178\u001b[0m\n\u001b[0;32m    175\u001b[0m predicted_Type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMDB\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYELP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDBLP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}[dataset]\n\u001b[0;32m    177\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m'\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[33], line 153\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    151\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mto_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:192\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class '__main__.edgeFeatures'>"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna  # NEW: For Hyperparameter Optimization\n",
    "import os, time\n",
    "\n",
    "# Set default parameters to optimize\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 16\n",
    "\n",
    "class edgeFeatures(object):\n",
    "    def __init__(self, label=None, type=None, embeddings=None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ', skiprows=0)\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ', skiprows=0)\n",
    "\n",
    "    train_Real_Graph, train_Fake_Graph = nx.Graph(), nx.Graph()\n",
    "    test_Real_Graph, test_Fake_Graph = nx.Graph(), nx.Graph()\n",
    "\n",
    "    real_edges = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edges = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    if dataset.lower() == 'facebook':\n",
    "        dataNewType = [9, 8, 7, 6, 5, 4]\n",
    "    else:\n",
    "        dataNewType = [2]\n",
    "\n",
    "    for edge in real_edges:\n",
    "        (n1, n2, rel) = edge\n",
    "        if rel in dataNewType:\n",
    "            test_Real_Graph.add_edge(n1, n2, relationship=rel)\n",
    "        else:\n",
    "            train_Real_Graph.add_edge(n1, n2, relationship=rel)\n",
    "\n",
    "    for edge in fake_edges:\n",
    "        (n1, n2, rel) = edge\n",
    "        if rel in dataNewType:\n",
    "            test_Fake_Graph.add_edge(n1, n2, relationship=rel)\n",
    "        else:\n",
    "            train_Fake_Graph.add_edge(n1, n2, relationship=rel)\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    realFileName = f'Datasets/{dataset}/realData.csv'\n",
    "    fakeFileName = f'Datasets/{dataset}/fakeData.csv'\n",
    "    train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph = structuralGraph(\n",
    "        realFileName, fakeFileName, dataset\n",
    "    )\n",
    "\n",
    "    data = pd.read_csv(f'Datasets/node2vecFeature/{dataset}Feature.txt', sep=' ', skiprows=1, header=None)\n",
    "    embeddings = np.array(data.iloc[:, 2:66])\n",
    "    nodeL = np.array(data.iloc[:, 0:1])\n",
    "    nodeR = np.array(data.iloc[:, 1:2])\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "    for i, (n1, n2) in enumerate(zip(nodeL, nodeR)):\n",
    "        n1, n2 = int(re.sub(\"\\D\", \"\", n1[0])), int(re.sub(\"\\D\", \"\", n2[0]))\n",
    "        emb = embeddings[i]\n",
    "        edgeFeature = edgeFeatures(embeddings=emb)\n",
    "\n",
    "        if train_Real_Graph.has_edge(n1, n2):\n",
    "            edgeFeature.label = 1\n",
    "            edgeFeature.type = train_Real_Graph[n1][n2]['relationship']\n",
    "            train_data.append(edgeFeature)\n",
    "        elif train_Fake_Graph.has_edge(n1, n2):\n",
    "            edgeFeature.label = 0\n",
    "            edgeFeature.type = train_Fake_Graph[n1][n2]['relationship']\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_Real_Graph.has_edge(n1, n2):\n",
    "            edgeFeature.label = 1\n",
    "            edgeFeature.type = test_Real_Graph[n1][n2]['relationship']\n",
    "            test_data.append(edgeFeature)\n",
    "        elif test_Fake_Graph.has_edge(n1, n2):\n",
    "            edgeFeature.label = 0\n",
    "            edgeFeature.type = test_Fake_Graph[n1][n2]['relationship']\n",
    "            test_data.append(edgeFeature)\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(validate, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validate_loader, test_loader\n",
    "\n",
    "def to_var(x):\n",
    "    return Variable(x.cuda()) if torch.cuda.is_available() else Variable(x)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambd):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "class adversarial_neural_networks(nn.Module):\n",
    "    def __init__(self, predicted_Type, hidden_dim):\n",
    "        super(adversarial_neural_networks, self).__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv1d(1, 1, 10, stride=1),\n",
    "            nn.Linear(55, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.predictor_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 24), nn.ReLU(),\n",
    "            nn.Linear(24, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 2), nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.discriminative_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 16), nn.ReLU(),\n",
    "            nn.Linear(16, predicted_Type), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.predictor(embeddings.unsqueeze(1))\n",
    "        shared_embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        link_output = self.predictor_classifier(shared_embeddings)\n",
    "        reverse_embeddings = GradReverse.apply(shared_embeddings, 1.0)\n",
    "        type_output = self.discriminative_classifier(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "def objective(trial):\n",
    "    global batch_size, learning_rate, hidden_dim\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 8, 64)\n",
    "\n",
    "    model = adversarial_neural_networks(predicted_Type, hidden_dim)\n",
    "    train_loader, validate_loader, _ = get_train_validate_test(dataset)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, labels, _ in train_loader:\n",
    "            data, labels = to_var(data), to_var(labels.long())\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate validation performance\n",
    "    model.eval()\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in validate_loader:\n",
    "            data, labels = to_var(data), to_var(labels.long())\n",
    "            outputs, _ = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_acc += (preds == labels).sum().item() / len(labels)\n",
    "\n",
    "    return val_acc / len(validate_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = 'facebook'\n",
    "    predicted_Type = {'facebook': 4, 'IMDB': 2, 'YELP': 2, 'DBLP': 2}[dataset]\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print('Best hyperparameters:', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02b81095-ea96-4d24-930b-2f2f2f854757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: -189.8821, Link Prediction Loss: 84.9544, Type Classification Loss: 275.4902, Train_Acc: 0.9699, Validate_Acc: 0.9639\n",
      "Epoch [2/50], Loss: -258.4483, Link Prediction Loss: 17.5279, Type Classification Loss: 277.2990, Train_Acc: 0.9901, Validate_Acc: 0.9819\n",
      "Epoch [3/50], Loss: -267.9262, Link Prediction Loss: 9.4637, Type Classification Loss: 279.4643, Train_Acc: 0.9942, Validate_Acc: 0.9880\n",
      "Epoch [4/50], Loss: -269.0319, Link Prediction Loss: 7.7267, Type Classification Loss: 279.0328, Train_Acc: 0.9942, Validate_Acc: 0.9874\n",
      "Epoch [5/50], Loss: -270.4168, Link Prediction Loss: 8.6377, Type Classification Loss: 282.7302, Train_Acc: 0.9945, Validate_Acc: 0.9869\n",
      "Epoch [6/50], Loss: -272.9853, Link Prediction Loss: 8.2383, Type Classification Loss: 285.7269, Train_Acc: 0.9946, Validate_Acc: 0.9874\n",
      "Epoch [7/50], Loss: -297.8789, Link Prediction Loss: 9.5057, Type Classification Loss: 317.9068, Train_Acc: 0.9936, Validate_Acc: 0.9869\n",
      "Epoch [8/50], Loss: -321.2405, Link Prediction Loss: 13.3324, Type Classification Loss: 350.4890, Train_Acc: 0.9945, Validate_Acc: 0.9880\n",
      "Epoch [9/50], Loss: -650.5992, Link Prediction Loss: 20.1113, Type Classification Loss: 729.8003, Train_Acc: 0.9928, Validate_Acc: 0.9869\n",
      "Epoch [10/50], Loss: -1140.0072, Link Prediction Loss: 26.7447, Type Classification Loss: 1233.6598, Train_Acc: 0.9878, Validate_Acc: 0.9814\n",
      "Epoch [11/50], Loss: -1977.9140, Link Prediction Loss: 34.1384, Type Classification Loss: 2084.9329, Train_Acc: 0.9930, Validate_Acc: 0.9880\n",
      "Epoch [12/50], Loss: -3287.9254, Link Prediction Loss: 53.0787, Type Classification Loss: 3590.2496, Train_Acc: 0.9940, Validate_Acc: 0.9895\n",
      "Epoch [13/50], Loss: -4909.5286, Link Prediction Loss: 50.7111, Type Classification Loss: 5321.0018, Train_Acc: 0.9830, Validate_Acc: 0.9784\n",
      "Epoch [14/50], Loss: -6376.9030, Link Prediction Loss: 61.2911, Type Classification Loss: 6818.7794, Train_Acc: 0.9863, Validate_Acc: 0.9814\n",
      "Epoch [15/50], Loss: -10519.3118, Link Prediction Loss: 81.7151, Type Classification Loss: 11200.6186, Train_Acc: 0.9932, Validate_Acc: 0.9869\n",
      "Epoch [16/50], Loss: -12910.8954, Link Prediction Loss: 94.2296, Type Classification Loss: 13904.7392, Train_Acc: 0.9922, Validate_Acc: 0.9880\n",
      "Epoch [17/50], Loss: -16382.2926, Link Prediction Loss: 116.2096, Type Classification Loss: 17413.3508, Train_Acc: 0.9808, Validate_Acc: 0.9709\n",
      "Epoch [18/50], Loss: -13108.1433, Link Prediction Loss: 113.7079, Type Classification Loss: 14260.5647, Train_Acc: 0.9884, Validate_Acc: 0.9839\n",
      "Epoch [19/50], Loss: -20615.7572, Link Prediction Loss: 119.9078, Type Classification Loss: 21841.9217, Train_Acc: 0.9911, Validate_Acc: 0.9834\n",
      "Epoch [20/50], Loss: -23269.3185, Link Prediction Loss: 125.4625, Type Classification Loss: 25423.1086, Train_Acc: 0.9895, Validate_Acc: 0.9824\n",
      "Epoch [21/50], Loss: -28591.7942, Link Prediction Loss: 165.2282, Type Classification Loss: 30607.9783, Train_Acc: 0.9830, Validate_Acc: 0.9749\n",
      "Epoch [22/50], Loss: -30133.2451, Link Prediction Loss: 154.9602, Type Classification Loss: 31827.2394, Train_Acc: 0.9911, Validate_Acc: 0.9874\n",
      "Epoch [23/50], Loss: -33057.8376, Link Prediction Loss: 190.7977, Type Classification Loss: 34873.7030, Train_Acc: 0.9853, Validate_Acc: 0.9794\n",
      "Epoch [24/50], Loss: -28831.8778, Link Prediction Loss: 120.4911, Type Classification Loss: 30983.1755, Train_Acc: 0.9886, Validate_Acc: 0.9829\n",
      "Epoch [25/50], Loss: -43198.8614, Link Prediction Loss: 224.8161, Type Classification Loss: 47070.5379, Train_Acc: 0.9912, Validate_Acc: 0.9859\n",
      "Epoch [26/50], Loss: -40412.4414, Link Prediction Loss: 178.0190, Type Classification Loss: 43292.4648, Train_Acc: 0.9832, Validate_Acc: 0.9759\n",
      "Epoch [27/50], Loss: -52831.1247, Link Prediction Loss: 206.1201, Type Classification Loss: 57141.6620, Train_Acc: 0.9841, Validate_Acc: 0.9769\n",
      "Epoch [28/50], Loss: -56307.6051, Link Prediction Loss: 212.6695, Type Classification Loss: 60565.0897, Train_Acc: 0.9903, Validate_Acc: 0.9854\n",
      "Epoch [29/50], Loss: -80547.7241, Link Prediction Loss: 330.7136, Type Classification Loss: 87866.0925, Train_Acc: 0.9911, Validate_Acc: 0.9844\n",
      "Epoch [30/50], Loss: -78046.2063, Link Prediction Loss: 242.2495, Type Classification Loss: 83449.8143, Train_Acc: 0.9920, Validate_Acc: 0.9859\n",
      "Epoch [31/50], Loss: -64541.1606, Link Prediction Loss: 233.9935, Type Classification Loss: 68484.1200, Train_Acc: 0.9823, Validate_Acc: 0.9759\n",
      "Epoch [32/50], Loss: -70713.3697, Link Prediction Loss: 274.0757, Type Classification Loss: 74994.4573, Train_Acc: 0.9766, Validate_Acc: 0.9734\n",
      "Epoch [33/50], Loss: -66545.4638, Link Prediction Loss: 351.6853, Type Classification Loss: 73818.8717, Train_Acc: 0.9916, Validate_Acc: 0.9849\n",
      "Epoch [34/50], Loss: -82436.8357, Link Prediction Loss: 294.7492, Type Classification Loss: 87654.4834, Train_Acc: 0.9912, Validate_Acc: 0.9844\n",
      "Epoch [35/50], Loss: -116231.8450, Link Prediction Loss: 407.7684, Type Classification Loss: 124676.3406, Train_Acc: 0.9888, Validate_Acc: 0.9819\n",
      "Epoch [36/50], Loss: -107240.9412, Link Prediction Loss: 275.8169, Type Classification Loss: 115440.9996, Train_Acc: 0.9852, Validate_Acc: 0.9844\n",
      "Epoch [37/50], Loss: -104698.5164, Link Prediction Loss: 512.0838, Type Classification Loss: 113316.2625, Train_Acc: 0.9897, Validate_Acc: 0.9849\n",
      "Epoch [38/50], Loss: -109960.4406, Link Prediction Loss: 207.9577, Type Classification Loss: 117830.0191, Train_Acc: 0.9884, Validate_Acc: 0.9839\n",
      "Epoch [39/50], Loss: -146715.2450, Link Prediction Loss: 462.5031, Type Classification Loss: 159078.2688, Train_Acc: 0.9906, Validate_Acc: 0.9829\n",
      "Epoch [40/50], Loss: -159943.1645, Link Prediction Loss: 419.7106, Type Classification Loss: 174322.8740, Train_Acc: 0.9901, Validate_Acc: 0.9859\n",
      "Epoch [41/50], Loss: -127556.6168, Link Prediction Loss: 599.7502, Type Classification Loss: 138058.8483, Train_Acc: 0.9910, Validate_Acc: 0.9814\n",
      "Epoch [42/50], Loss: -142686.0726, Link Prediction Loss: 290.3534, Type Classification Loss: 154204.8950, Train_Acc: 0.9861, Validate_Acc: 0.9774\n",
      "Epoch [43/50], Loss: -172707.1203, Link Prediction Loss: 662.0493, Type Classification Loss: 184265.3322, Train_Acc: 0.9893, Validate_Acc: 0.9814\n",
      "Epoch [44/50], Loss: -184138.3270, Link Prediction Loss: 512.3711, Type Classification Loss: 200334.0532, Train_Acc: 0.9882, Validate_Acc: 0.9829\n",
      "Epoch [45/50], Loss: -154094.4416, Link Prediction Loss: 507.8780, Type Classification Loss: 163875.6357, Train_Acc: 0.9911, Validate_Acc: 0.9844\n",
      "Epoch [46/50], Loss: -166135.0300, Link Prediction Loss: 420.4546, Type Classification Loss: 179509.3193, Train_Acc: 0.9911, Validate_Acc: 0.9829\n",
      "Epoch [47/50], Loss: -185788.0253, Link Prediction Loss: 571.6829, Type Classification Loss: 197598.4246, Train_Acc: 0.9897, Validate_Acc: 0.9819\n",
      "Epoch [48/50], Loss: -209766.1552, Link Prediction Loss: 465.8285, Type Classification Loss: 222907.7351, Train_Acc: 0.9896, Validate_Acc: 0.9824\n",
      "Epoch [49/50], Loss: -198937.4809, Link Prediction Loss: 715.2135, Type Classification Loss: 213576.6457, Train_Acc: 0.9917, Validate_Acc: 0.9854\n",
      "Epoch [50/50], Loss: -205267.7355, Link Prediction Loss: 444.6079, Type Classification Loss: 217586.5007, Train_Acc: 0.9892, Validate_Acc: 0.9804\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the Generative Predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define the Discriminative Classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# MTTM class\n",
    "class MTTM:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_classes, lr=0.001):\n",
    "        self.generator = GenerativePredictor(input_dim, hidden_dim, output_dim)\n",
    "        self.discriminator = DiscriminativeClassifier(output_dim, hidden_dim, num_classes)\n",
    "        self.gen_optimizer = optim.Adam(self.generator.parameters(), lr=lr)\n",
    "        self.disc_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "        self.link_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.type_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.generator.train()\n",
    "            self.discriminator.train()\n",
    "            total_loss = 0\n",
    "            link_pred_loss = 0\n",
    "            type_class_loss = 0\n",
    "            \n",
    "            for batch_features, batch_links, batch_types in train_loader:\n",
    "                # Generate link representations\n",
    "                gen_output = self.generator(batch_features)\n",
    "                \n",
    "                # Predict links\n",
    "                link_pred = gen_output.squeeze()  # Change here\n",
    "                link_loss = self.link_criterion(link_pred, batch_links)\n",
    "                \n",
    "                # Classify link types\n",
    "                type_pred = self.discriminator(gen_output.detach())\n",
    "                disc_loss = self.type_criterion(type_pred, batch_types)\n",
    "                \n",
    "                # Train discriminator\n",
    "                self.disc_optimizer.zero_grad()\n",
    "                disc_loss.backward()\n",
    "                self.disc_optimizer.step()\n",
    "                \n",
    "                # Train generator\n",
    "                self.gen_optimizer.zero_grad()\n",
    "                type_pred = self.discriminator(gen_output)\n",
    "                gen_loss = link_loss - self.type_criterion(type_pred, batch_types)\n",
    "                gen_loss.backward()\n",
    "                self.gen_optimizer.step()\n",
    "                \n",
    "                total_loss += gen_loss.item()\n",
    "                link_pred_loss += link_loss.item()\n",
    "                type_class_loss += disc_loss.item()\n",
    "            \n",
    "            # Validate\n",
    "            train_acc = self.evaluate(train_loader)\n",
    "            val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, '\n",
    "                  f'Link Prediction Loss: {link_pred_loss:.4f}, '\n",
    "                  f'Type Classification Loss: {type_class_loss:.4f}, '\n",
    "                  f'Train_Acc: {train_acc:.4f}, Validate_Acc: {val_acc:.4f}')\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_links, _ in data_loader:\n",
    "                gen_output = self.generator(batch_features)\n",
    "                link_pred = torch.sigmoid(gen_output.squeeze())  # Change here\n",
    "                all_preds.extend(link_pred.numpy() > 0.5)\n",
    "                all_labels.extend(batch_links.numpy())\n",
    "        \n",
    "        return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Generate a hypothetical heterogeneous social network\n",
    "def generate_heterogeneous_social_network(num_nodes=1000, num_edge_types=3):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Generate nodes with random features\n",
    "    for i in range(num_nodes):\n",
    "        G.add_node(i, features=np.random.randn(10))  # 10 random features per node\n",
    "    \n",
    "    # Generate edges of different types\n",
    "    for _ in range(num_nodes * 5):  # Create about 5 edges per node on average\n",
    "        u, v = np.random.choice(num_nodes, 2, replace=False)\n",
    "        edge_type = np.random.randint(num_edge_types)\n",
    "        if not G.has_edge(u, v):\n",
    "            G.add_edge(u, v, type=edge_type)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Prepare data for MTTM\n",
    "def prepare_data(G):\n",
    "    edges = list(G.edges(data=True))\n",
    "    non_edges = list(nx.non_edges(G))\n",
    "    \n",
    "    # Prepare positive samples (existing edges)\n",
    "    X_pos = []\n",
    "    y_pos = []\n",
    "    types_pos = []\n",
    "    for u, v, data in edges:\n",
    "        feature_u = G.nodes[u]['features']\n",
    "        feature_v = G.nodes[v]['features']\n",
    "        X_pos.append(np.concatenate([feature_u, feature_v]))\n",
    "        y_pos.append(1)\n",
    "        types_pos.append(data['type'])\n",
    "    \n",
    "    # Prepare negative samples (non-existing edges)\n",
    "    X_neg = []\n",
    "    y_neg = []\n",
    "    types_neg = []\n",
    "    for u, v in non_edges[:len(edges)]:  # Use the same number of negative samples as positive\n",
    "        feature_u = G.nodes[u]['features']\n",
    "        feature_v = G.nodes[v]['features']\n",
    "        X_neg.append(np.concatenate([feature_u, feature_v]))\n",
    "        y_neg.append(0)\n",
    "        types_neg.append(np.random.randint(len(set(types_pos))))  # Assign random type to negative samples\n",
    "    \n",
    "    X = np.array(X_pos + X_neg)\n",
    "    y = np.array(y_pos + y_neg)\n",
    "    types = np.array(types_pos + types_neg)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, types\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Generate hypothetical social network\n",
    "    G = generate_heterogeneous_social_network()\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y, types = prepare_data(G)\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val, types_train, types_val = train_test_split(X, y, types, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), \n",
    "                                  torch.FloatTensor(y_train), \n",
    "                                  torch.LongTensor(types_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), \n",
    "                                torch.FloatTensor(y_val), \n",
    "                                torch.LongTensor(types_val))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize and train MTTM\n",
    "    input_dim = X_train.shape[1]\n",
    "    hidden_dim = 64\n",
    "    output_dim = 1  # Change here\n",
    "    num_classes = len(np.unique(types_train))\n",
    "    \n",
    "    mttm = MTTM(input_dim, hidden_dim, output_dim, num_classes)\n",
    "    mttm.train(train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d361472-35e7-42ed-a502-287267a1f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: -217.2058, Link Prediction Loss: 62.6947, Type Classification Loss: 281.8336, Train_Acc: 0.9671, Validate_Acc: 0.9639\n",
      "Epoch [2/30], Loss: -108.6710, Link Prediction Loss: 165.5936, Type Classification Loss: 274.7313, Train_Acc: 0.9218, Validate_Acc: 0.9051\n",
      "Epoch [3/30], Loss: -20.4042, Link Prediction Loss: 253.8392, Type Classification Loss: 274.6658, Train_Acc: 0.8850, Validate_Acc: 0.8780\n",
      "Epoch [4/30], Loss: -201.4445, Link Prediction Loss: 72.8965, Type Classification Loss: 274.7862, Train_Acc: 0.8988, Validate_Acc: 0.8855\n",
      "Epoch [5/30], Loss: -194.0928, Link Prediction Loss: 80.1438, Type Classification Loss: 274.6488, Train_Acc: 0.9208, Validate_Acc: 0.9071\n",
      "Epoch [6/30], Loss: -178.4438, Link Prediction Loss: 95.8258, Type Classification Loss: 274.7057, Train_Acc: 0.9152, Validate_Acc: 0.9111\n",
      "Epoch [7/30], Loss: -166.6529, Link Prediction Loss: 107.7116, Type Classification Loss: 274.8753, Train_Acc: 0.9326, Validate_Acc: 0.9257\n",
      "Epoch [8/30], Loss: -132.7105, Link Prediction Loss: 141.5575, Type Classification Loss: 274.7113, Train_Acc: 0.9134, Validate_Acc: 0.9051\n",
      "Epoch [9/30], Loss: -182.9425, Link Prediction Loss: 92.2063, Type Classification Loss: 275.7691, Train_Acc: 0.9100, Validate_Acc: 0.9021\n",
      "Epoch [10/30], Loss: -90.8006, Link Prediction Loss: 183.8190, Type Classification Loss: 275.1374, Train_Acc: 0.8513, Validate_Acc: 0.8389\n",
      "Epoch [11/30], Loss: 225.9744, Link Prediction Loss: 500.2896, Type Classification Loss: 274.8322, Train_Acc: 0.8881, Validate_Acc: 0.8800\n",
      "Epoch [12/30], Loss: 900.5962, Link Prediction Loss: 1174.9496, Type Classification Loss: 274.8057, Train_Acc: 0.9051, Validate_Acc: 0.8931\n",
      "Epoch [13/30], Loss: -107.2283, Link Prediction Loss: 167.3583, Type Classification Loss: 275.0765, Train_Acc: 0.9037, Validate_Acc: 0.8926\n",
      "Epoch [14/30], Loss: -124.7478, Link Prediction Loss: 149.6295, Type Classification Loss: 274.8290, Train_Acc: 0.8657, Validate_Acc: 0.8479\n",
      "Epoch [15/30], Loss: -197.3999, Link Prediction Loss: 77.0630, Type Classification Loss: 274.9358, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [16/30], Loss: -206.7356, Link Prediction Loss: 67.5039, Type Classification Loss: 274.6845, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [17/30], Loss: -206.7635, Link Prediction Loss: 67.4475, Type Classification Loss: 274.6856, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [18/30], Loss: -206.8590, Link Prediction Loss: 67.6230, Type Classification Loss: 274.9579, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [19/30], Loss: -206.5884, Link Prediction Loss: 67.6868, Type Classification Loss: 274.7720, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [20/30], Loss: -206.8553, Link Prediction Loss: 67.6107, Type Classification Loss: 274.9462, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [21/30], Loss: -206.4389, Link Prediction Loss: 67.6598, Type Classification Loss: 274.4493, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [22/30], Loss: -206.6054, Link Prediction Loss: 67.5849, Type Classification Loss: 274.6526, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [23/30], Loss: -206.9127, Link Prediction Loss: 67.5389, Type Classification Loss: 274.9075, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [24/30], Loss: -206.7905, Link Prediction Loss: 67.5980, Type Classification Loss: 274.8513, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [25/30], Loss: -206.9727, Link Prediction Loss: 67.3184, Type Classification Loss: 274.7353, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [26/30], Loss: -206.8936, Link Prediction Loss: 67.6406, Type Classification Loss: 275.0385, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [27/30], Loss: -207.0374, Link Prediction Loss: 67.6935, Type Classification Loss: 275.3046, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [28/30], Loss: -206.4648, Link Prediction Loss: 67.5492, Type Classification Loss: 274.4238, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [29/30], Loss: -206.7554, Link Prediction Loss: 67.5126, Type Classification Loss: 274.7542, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [30/30], Loss: -206.6488, Link Prediction Loss: 67.5537, Type Classification Loss: 274.6391, Train_Acc: 0.9005, Validate_Acc: 0.8820\n",
      "Epoch [1/30], Loss: -243.4453, Link Prediction Loss: 39.5492, Type Classification Loss: 286.4567, Train_Acc: 0.9852, Validate_Acc: 0.9814\n",
      "Epoch [2/30], Loss: -259.5935, Link Prediction Loss: 14.4162, Type Classification Loss: 274.2765, Train_Acc: 0.9783, Validate_Acc: 0.9729\n",
      "Epoch [3/30], Loss: -255.6381, Link Prediction Loss: 18.3923, Type Classification Loss: 274.3140, Train_Acc: 0.9922, Validate_Acc: 0.9854\n",
      "Epoch [4/30], Loss: -262.2122, Link Prediction Loss: 11.7183, Type Classification Loss: 274.2337, Train_Acc: 0.9927, Validate_Acc: 0.9885\n",
      "Epoch [5/30], Loss: -251.4856, Link Prediction Loss: 22.5570, Type Classification Loss: 274.2986, Train_Acc: 0.9854, Validate_Acc: 0.9809\n",
      "Epoch [6/30], Loss: -248.3189, Link Prediction Loss: 25.6525, Type Classification Loss: 274.1958, Train_Acc: 0.9892, Validate_Acc: 0.9849\n",
      "Epoch [7/30], Loss: -242.5973, Link Prediction Loss: 31.4181, Type Classification Loss: 274.2485, Train_Acc: 0.9868, Validate_Acc: 0.9859\n",
      "Epoch [8/30], Loss: -140.9805, Link Prediction Loss: 133.0326, Type Classification Loss: 274.2656, Train_Acc: 0.9836, Validate_Acc: 0.9794\n",
      "Epoch [9/30], Loss: -212.4931, Link Prediction Loss: 61.5131, Type Classification Loss: 274.2761, Train_Acc: 0.9851, Validate_Acc: 0.9819\n",
      "Epoch [10/30], Loss: -256.5216, Link Prediction Loss: 17.5410, Type Classification Loss: 274.3105, Train_Acc: 0.9877, Validate_Acc: 0.9839\n",
      "Epoch [11/30], Loss: -258.4847, Link Prediction Loss: 15.2584, Type Classification Loss: 273.9688, Train_Acc: 0.9874, Validate_Acc: 0.9839\n",
      "Epoch [12/30], Loss: -188.8933, Link Prediction Loss: 85.1575, Type Classification Loss: 274.3288, Train_Acc: 0.9631, Validate_Acc: 0.9608\n",
      "Epoch [13/30], Loss: -227.8536, Link Prediction Loss: 46.1660, Type Classification Loss: 274.2699, Train_Acc: 0.9794, Validate_Acc: 0.9779\n",
      "Epoch [14/30], Loss: -236.6324, Link Prediction Loss: 37.3814, Type Classification Loss: 274.2736, Train_Acc: 0.9827, Validate_Acc: 0.9804\n",
      "Epoch [15/30], Loss: -231.5613, Link Prediction Loss: 42.2839, Type Classification Loss: 274.0575, Train_Acc: 0.9907, Validate_Acc: 0.9849\n",
      "Epoch [16/30], Loss: -258.2262, Link Prediction Loss: 15.8516, Type Classification Loss: 274.3349, Train_Acc: 0.9883, Validate_Acc: 0.9839\n",
      "Epoch [17/30], Loss: -251.3469, Link Prediction Loss: 22.7101, Type Classification Loss: 274.3010, Train_Acc: 0.9866, Validate_Acc: 0.9834\n",
      "Epoch [18/30], Loss: -255.9120, Link Prediction Loss: 17.9685, Type Classification Loss: 274.0843, Train_Acc: 0.9903, Validate_Acc: 0.9859\n",
      "Epoch [19/30], Loss: -262.0457, Link Prediction Loss: 12.0011, Type Classification Loss: 274.3202, Train_Acc: 0.9912, Validate_Acc: 0.9869\n",
      "Epoch [20/30], Loss: -262.1188, Link Prediction Loss: 11.8568, Type Classification Loss: 274.2541, Train_Acc: 0.9910, Validate_Acc: 0.9859\n",
      "Epoch [21/30], Loss: -255.5037, Link Prediction Loss: 18.5222, Type Classification Loss: 274.3042, Train_Acc: 0.9892, Validate_Acc: 0.9854\n",
      "Epoch [22/30], Loss: -202.6796, Link Prediction Loss: 71.2224, Type Classification Loss: 274.1541, Train_Acc: 0.9819, Validate_Acc: 0.9799\n",
      "Epoch [23/30], Loss: -219.7560, Link Prediction Loss: 54.1378, Type Classification Loss: 274.1298, Train_Acc: 0.9880, Validate_Acc: 0.9874\n",
      "Epoch [24/30], Loss: -233.0634, Link Prediction Loss: 40.8031, Type Classification Loss: 274.0960, Train_Acc: 0.9814, Validate_Acc: 0.9804\n",
      "Epoch [25/30], Loss: -236.5104, Link Prediction Loss: 37.6144, Type Classification Loss: 274.4222, Train_Acc: 0.9882, Validate_Acc: 0.9859\n",
      "Epoch [26/30], Loss: -252.0696, Link Prediction Loss: 21.9699, Type Classification Loss: 274.2998, Train_Acc: 0.9872, Validate_Acc: 0.9849\n",
      "Epoch [27/30], Loss: -247.9321, Link Prediction Loss: 25.9181, Type Classification Loss: 274.0845, Train_Acc: 0.9797, Validate_Acc: 0.9779\n",
      "Epoch [28/30], Loss: -242.4403, Link Prediction Loss: 31.3140, Type Classification Loss: 274.0036, Train_Acc: 0.9753, Validate_Acc: 0.9709\n",
      "Epoch [29/30], Loss: -222.1612, Link Prediction Loss: 51.6828, Type Classification Loss: 274.0877, Train_Acc: 0.9775, Validate_Acc: 0.9744\n",
      "Epoch [30/30], Loss: -221.4141, Link Prediction Loss: 52.3070, Type Classification Loss: 273.9490, Train_Acc: 0.9810, Validate_Acc: 0.9804\n",
      "Epoch [1/30], Loss: -250.6842, Link Prediction Loss: 26.1182, Type Classification Loss: 278.8685, Train_Acc: 0.9925, Validate_Acc: 0.9885\n",
      "Epoch [2/30], Loss: -262.7926, Link Prediction Loss: 11.6335, Type Classification Loss: 275.0162, Train_Acc: 0.9933, Validate_Acc: 0.9890\n",
      "Epoch [3/30], Loss: -264.9187, Link Prediction Loss: 8.9044, Type Classification Loss: 273.9998, Train_Acc: 0.9937, Validate_Acc: 0.9874\n",
      "Epoch [4/30], Loss: -262.0081, Link Prediction Loss: 11.6451, Type Classification Loss: 273.7581, Train_Acc: 0.9921, Validate_Acc: 0.9854\n",
      "Epoch [5/30], Loss: -264.4213, Link Prediction Loss: 9.2391, Type Classification Loss: 273.7627, Train_Acc: 0.9930, Validate_Acc: 0.9874\n",
      "Epoch [6/30], Loss: -265.2521, Link Prediction Loss: 8.2789, Type Classification Loss: 273.6382, Train_Acc: 0.9928, Validate_Acc: 0.9885\n",
      "Epoch [7/30], Loss: -263.7824, Link Prediction Loss: 9.8170, Type Classification Loss: 273.6944, Train_Acc: 0.9918, Validate_Acc: 0.9864\n",
      "Epoch [8/30], Loss: -264.2918, Link Prediction Loss: 9.2948, Type Classification Loss: 273.6801, Train_Acc: 0.9892, Validate_Acc: 0.9814\n",
      "Epoch [9/30], Loss: -266.3877, Link Prediction Loss: 7.1406, Type Classification Loss: 273.6223, Train_Acc: 0.9935, Validate_Acc: 0.9885\n",
      "Epoch [10/30], Loss: -266.8889, Link Prediction Loss: 6.6957, Type Classification Loss: 273.6664, Train_Acc: 0.9939, Validate_Acc: 0.9880\n",
      "Epoch [11/30], Loss: -266.2587, Link Prediction Loss: 7.3861, Type Classification Loss: 273.7555, Train_Acc: 0.9933, Validate_Acc: 0.9885\n",
      "Epoch [12/30], Loss: -244.2871, Link Prediction Loss: 29.3774, Type Classification Loss: 273.7530, Train_Acc: 0.9755, Validate_Acc: 0.9704\n",
      "Epoch [13/30], Loss: -263.9869, Link Prediction Loss: 9.6034, Type Classification Loss: 273.6820, Train_Acc: 0.9932, Validate_Acc: 0.9880\n",
      "Epoch [14/30], Loss: -266.6298, Link Prediction Loss: 7.0858, Type Classification Loss: 273.8260, Train_Acc: 0.9936, Validate_Acc: 0.9885\n",
      "Epoch [15/30], Loss: -266.8725, Link Prediction Loss: 6.8572, Type Classification Loss: 273.8345, Train_Acc: 0.9937, Validate_Acc: 0.9885\n",
      "Epoch [16/30], Loss: -266.9264, Link Prediction Loss: 6.6808, Type Classification Loss: 273.6941, Train_Acc: 0.9936, Validate_Acc: 0.9885\n",
      "Epoch [17/30], Loss: -267.8832, Link Prediction Loss: 5.7752, Type Classification Loss: 273.7653, Train_Acc: 0.9940, Validate_Acc: 0.9890\n",
      "Epoch [18/30], Loss: -267.0272, Link Prediction Loss: 6.5166, Type Classification Loss: 273.6408, Train_Acc: 0.9937, Validate_Acc: 0.9874\n",
      "Epoch [19/30], Loss: -268.2441, Link Prediction Loss: 5.3046, Type Classification Loss: 273.6277, Train_Acc: 0.9940, Validate_Acc: 0.9885\n",
      "Epoch [20/30], Loss: -268.2083, Link Prediction Loss: 5.3785, Type Classification Loss: 273.6812, Train_Acc: 0.9944, Validate_Acc: 0.9885\n",
      "Epoch [21/30], Loss: -267.6339, Link Prediction Loss: 5.9549, Type Classification Loss: 273.6749, Train_Acc: 0.9931, Validate_Acc: 0.9880\n",
      "Epoch [22/30], Loss: -267.5634, Link Prediction Loss: 6.0307, Type Classification Loss: 273.7281, Train_Acc: 0.9939, Validate_Acc: 0.9885\n",
      "Epoch [23/30], Loss: -261.3075, Link Prediction Loss: 12.3998, Type Classification Loss: 273.8143, Train_Acc: 0.9839, Validate_Acc: 0.9794\n",
      "Epoch [24/30], Loss: -266.5473, Link Prediction Loss: 7.0478, Type Classification Loss: 273.7071, Train_Acc: 0.9936, Validate_Acc: 0.9880\n",
      "Epoch [25/30], Loss: -268.6529, Link Prediction Loss: 4.9717, Type Classification Loss: 273.7216, Train_Acc: 0.9940, Validate_Acc: 0.9885\n",
      "Epoch [26/30], Loss: -269.0892, Link Prediction Loss: 4.3880, Type Classification Loss: 273.5591, Train_Acc: 0.9939, Validate_Acc: 0.9885\n",
      "Epoch [27/30], Loss: -269.4496, Link Prediction Loss: 4.2431, Type Classification Loss: 273.8006, Train_Acc: 0.9946, Validate_Acc: 0.9859\n",
      "Epoch [28/30], Loss: -268.6914, Link Prediction Loss: 4.9535, Type Classification Loss: 273.7476, Train_Acc: 0.9941, Validate_Acc: 0.9869\n",
      "Epoch [29/30], Loss: -268.2269, Link Prediction Loss: 5.2905, Type Classification Loss: 273.6147, Train_Acc: 0.9940, Validate_Acc: 0.9854\n",
      "Epoch [30/30], Loss: -259.8141, Link Prediction Loss: 13.8692, Type Classification Loss: 273.7898, Train_Acc: 0.9876, Validate_Acc: 0.9809\n",
      "Epoch [1/30], Loss: -245.4808, Link Prediction Loss: 30.3869, Type Classification Loss: 277.0751, Train_Acc: 0.9908, Validate_Acc: 0.9864\n",
      "Epoch [2/30], Loss: -258.1751, Link Prediction Loss: 15.7198, Type Classification Loss: 274.1063, Train_Acc: 0.9874, Validate_Acc: 0.9834\n",
      "Epoch [3/30], Loss: -259.7494, Link Prediction Loss: 14.1633, Type Classification Loss: 274.1530, Train_Acc: 0.9868, Validate_Acc: 0.9839\n",
      "Epoch [4/30], Loss: -258.2851, Link Prediction Loss: 15.3766, Type Classification Loss: 273.8928, Train_Acc: 0.9908, Validate_Acc: 0.9849\n",
      "Epoch [5/30], Loss: -262.6544, Link Prediction Loss: 11.3408, Type Classification Loss: 274.2103, Train_Acc: 0.9918, Validate_Acc: 0.9864\n",
      "Epoch [6/30], Loss: -263.2014, Link Prediction Loss: 10.7162, Type Classification Loss: 274.1270, Train_Acc: 0.9932, Validate_Acc: 0.9885\n",
      "Epoch [7/30], Loss: -264.4576, Link Prediction Loss: 9.4488, Type Classification Loss: 274.1199, Train_Acc: 0.9922, Validate_Acc: 0.9874\n",
      "Epoch [8/30], Loss: -244.0423, Link Prediction Loss: 29.6314, Type Classification Loss: 273.8726, Train_Acc: 0.9878, Validate_Acc: 0.9839\n",
      "Epoch [9/30], Loss: -258.6818, Link Prediction Loss: 15.2691, Type Classification Loss: 274.1463, Train_Acc: 0.9888, Validate_Acc: 0.9859\n",
      "Epoch [10/30], Loss: -262.6001, Link Prediction Loss: 11.3564, Type Classification Loss: 274.1663, Train_Acc: 0.9917, Validate_Acc: 0.9874\n",
      "Epoch [11/30], Loss: -256.0786, Link Prediction Loss: 17.7601, Type Classification Loss: 274.0098, Train_Acc: 0.9912, Validate_Acc: 0.9859\n",
      "Epoch [12/30], Loss: -262.9350, Link Prediction Loss: 10.9811, Type Classification Loss: 274.1588, Train_Acc: 0.9895, Validate_Acc: 0.9864\n",
      "Epoch [13/30], Loss: -259.2417, Link Prediction Loss: 14.6863, Type Classification Loss: 274.1487, Train_Acc: 0.9869, Validate_Acc: 0.9824\n",
      "Epoch [14/30], Loss: -262.9270, Link Prediction Loss: 10.9174, Type Classification Loss: 274.0447, Train_Acc: 0.9923, Validate_Acc: 0.9864\n",
      "Epoch [15/30], Loss: -264.1287, Link Prediction Loss: 10.0660, Type Classification Loss: 274.4481, Train_Acc: 0.9936, Validate_Acc: 0.9895\n",
      "Epoch [16/30], Loss: -243.0723, Link Prediction Loss: 30.8249, Type Classification Loss: 274.1090, Train_Acc: 0.9874, Validate_Acc: 0.9869\n",
      "Epoch [17/30], Loss: -256.4777, Link Prediction Loss: 17.4183, Type Classification Loss: 274.1004, Train_Acc: 0.9937, Validate_Acc: 0.9874\n",
      "Epoch [18/30], Loss: -261.7738, Link Prediction Loss: 12.2971, Type Classification Loss: 274.2990, Train_Acc: 0.9935, Validate_Acc: 0.9880\n",
      "Epoch [19/30], Loss: -265.2484, Link Prediction Loss: 8.5538, Type Classification Loss: 274.0068, Train_Acc: 0.9936, Validate_Acc: 0.9880\n",
      "Epoch [20/30], Loss: -265.3510, Link Prediction Loss: 8.5372, Type Classification Loss: 274.1037, Train_Acc: 0.9936, Validate_Acc: 0.9880\n",
      "Epoch [21/30], Loss: -264.1061, Link Prediction Loss: 9.5970, Type Classification Loss: 273.8764, Train_Acc: 0.9933, Validate_Acc: 0.9880\n",
      "Epoch [22/30], Loss: -262.9467, Link Prediction Loss: 11.0373, Type Classification Loss: 274.2006, Train_Acc: 0.9886, Validate_Acc: 0.9844\n",
      "Epoch [23/30], Loss: -259.7753, Link Prediction Loss: 14.0924, Type Classification Loss: 274.0861, Train_Acc: 0.9922, Validate_Acc: 0.9869\n",
      "Epoch [24/30], Loss: -262.8518, Link Prediction Loss: 11.0544, Type Classification Loss: 274.0900, Train_Acc: 0.9917, Validate_Acc: 0.9859\n",
      "Epoch [25/30], Loss: -169.1034, Link Prediction Loss: 104.8314, Type Classification Loss: 274.1519, Train_Acc: 0.9610, Validate_Acc: 0.9573\n",
      "Epoch [26/30], Loss: -175.4571, Link Prediction Loss: 98.3126, Type Classification Loss: 273.9648, Train_Acc: 0.9765, Validate_Acc: 0.9744\n",
      "Epoch [27/30], Loss: -211.8512, Link Prediction Loss: 62.1648, Type Classification Loss: 274.2353, Train_Acc: 0.9885, Validate_Acc: 0.9854\n",
      "Epoch [28/30], Loss: -238.2669, Link Prediction Loss: 35.6748, Type Classification Loss: 274.1492, Train_Acc: 0.9882, Validate_Acc: 0.9829\n",
      "Epoch [29/30], Loss: -254.4233, Link Prediction Loss: 19.5050, Type Classification Loss: 274.1402, Train_Acc: 0.9871, Validate_Acc: 0.9834\n",
      "Epoch [30/30], Loss: -254.6958, Link Prediction Loss: 19.3628, Type Classification Loss: 274.3054, Train_Acc: 0.9907, Validate_Acc: 0.9839\n",
      "Epoch [1/30], Loss: -170.6981, Link Prediction Loss: 109.1316, Type Classification Loss: 280.5539, Train_Acc: 0.9206, Validate_Acc: 0.9187\n",
      "Epoch [2/30], Loss: -6.2697, Link Prediction Loss: 267.7501, Type Classification Loss: 274.3755, Train_Acc: 0.9316, Validate_Acc: 0.9217\n",
      "Epoch [3/30], Loss: -193.0699, Link Prediction Loss: 81.1476, Type Classification Loss: 274.6110, Train_Acc: 0.9620, Validate_Acc: 0.9548\n",
      "Epoch [4/30], Loss: -191.6795, Link Prediction Loss: 82.1674, Type Classification Loss: 274.2231, Train_Acc: 0.9615, Validate_Acc: 0.9578\n",
      "Epoch [5/30], Loss: -224.5132, Link Prediction Loss: 49.4843, Type Classification Loss: 274.3838, Train_Acc: 0.9767, Validate_Acc: 0.9744\n",
      "Epoch [6/30], Loss: -181.3688, Link Prediction Loss: 92.7677, Type Classification Loss: 274.5282, Train_Acc: 0.8929, Validate_Acc: 0.8835\n",
      "Epoch [7/30], Loss: -38.6564, Link Prediction Loss: 235.7350, Type Classification Loss: 274.8362, Train_Acc: 0.8960, Validate_Acc: 0.8850\n",
      "Epoch [8/30], Loss: -112.6736, Link Prediction Loss: 161.2778, Type Classification Loss: 274.3348, Train_Acc: 0.9301, Validate_Acc: 0.9207\n",
      "Epoch [9/30], Loss: -157.8600, Link Prediction Loss: 116.3654, Type Classification Loss: 274.6673, Train_Acc: 0.9236, Validate_Acc: 0.9157\n",
      "Epoch [10/30], Loss: -212.8810, Link Prediction Loss: 61.6427, Type Classification Loss: 275.0231, Train_Acc: 0.9268, Validate_Acc: 0.9182\n",
      "Epoch [11/30], Loss: -210.1282, Link Prediction Loss: 63.9757, Type Classification Loss: 274.5610, Train_Acc: 0.9280, Validate_Acc: 0.9182\n",
      "Epoch [12/30], Loss: -200.8062, Link Prediction Loss: 73.6669, Type Classification Loss: 274.8860, Train_Acc: 0.8978, Validate_Acc: 0.8906\n",
      "Epoch [13/30], Loss: 136.5339, Link Prediction Loss: 410.7364, Type Classification Loss: 274.6120, Train_Acc: 0.9206, Validate_Acc: 0.9192\n",
      "Epoch [14/30], Loss: -51.9530, Link Prediction Loss: 222.2541, Type Classification Loss: 274.6214, Train_Acc: 0.8906, Validate_Acc: 0.8785\n",
      "Epoch [15/30], Loss: -188.6314, Link Prediction Loss: 85.9693, Type Classification Loss: 275.1125, Train_Acc: 0.8746, Validate_Acc: 0.8559\n",
      "Epoch [16/30], Loss: -146.8509, Link Prediction Loss: 127.1333, Type Classification Loss: 274.4142, Train_Acc: 0.8873, Validate_Acc: 0.8715\n",
      "Epoch [17/30], Loss: 33.7474, Link Prediction Loss: 307.9911, Type Classification Loss: 274.6948, Train_Acc: 0.8963, Validate_Acc: 0.8946\n",
      "Epoch [18/30], Loss: -81.5493, Link Prediction Loss: 192.9954, Type Classification Loss: 275.0257, Train_Acc: 0.9066, Validate_Acc: 0.9006\n",
      "Epoch [19/30], Loss: 72.1818, Link Prediction Loss: 346.2452, Type Classification Loss: 274.4611, Train_Acc: 0.9233, Validate_Acc: 0.9162\n",
      "Epoch [20/30], Loss: 77.8851, Link Prediction Loss: 351.9189, Type Classification Loss: 274.4215, Train_Acc: 0.9080, Validate_Acc: 0.8991\n",
      "Epoch [21/30], Loss: 362.7427, Link Prediction Loss: 636.8484, Type Classification Loss: 274.5359, Train_Acc: 0.8124, Validate_Acc: 0.8082\n",
      "Epoch [22/30], Loss: -33.0968, Link Prediction Loss: 241.1483, Type Classification Loss: 274.6810, Train_Acc: 0.8761, Validate_Acc: 0.8650\n",
      "Epoch [23/30], Loss: -55.8981, Link Prediction Loss: 218.0751, Type Classification Loss: 274.3563, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [24/30], Loss: -188.4528, Link Prediction Loss: 85.6367, Type Classification Loss: 274.4805, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [25/30], Loss: -188.7986, Link Prediction Loss: 85.5121, Type Classification Loss: 274.7822, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [26/30], Loss: -188.7267, Link Prediction Loss: 85.4937, Type Classification Loss: 274.6185, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [27/30], Loss: -188.7204, Link Prediction Loss: 85.4394, Type Classification Loss: 274.6085, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [28/30], Loss: -188.3593, Link Prediction Loss: 85.4293, Type Classification Loss: 274.1756, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [29/30], Loss: -188.8048, Link Prediction Loss: 85.4754, Type Classification Loss: 274.7491, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [30/30], Loss: -188.8945, Link Prediction Loss: 85.4479, Type Classification Loss: 274.8057, Train_Acc: 0.8803, Validate_Acc: 0.8660\n",
      "Epoch [1/30], Loss: -247.6985, Link Prediction Loss: 26.5372, Type Classification Loss: 274.8167, Train_Acc: 0.9900, Validate_Acc: 0.9839\n",
      "Epoch [2/30], Loss: -263.9634, Link Prediction Loss: 9.8563, Type Classification Loss: 273.9736, Train_Acc: 0.9933, Validate_Acc: 0.9895\n",
      "Epoch [3/30], Loss: -264.5209, Link Prediction Loss: 9.2216, Type Classification Loss: 273.8691, Train_Acc: 0.9926, Validate_Acc: 0.9869\n",
      "Epoch [4/30], Loss: -264.9691, Link Prediction Loss: 8.5867, Type Classification Loss: 273.6862, Train_Acc: 0.9935, Validate_Acc: 0.9880\n",
      "Epoch [5/30], Loss: -261.0082, Link Prediction Loss: 12.7083, Type Classification Loss: 273.8592, Train_Acc: 0.9868, Validate_Acc: 0.9834\n",
      "Epoch [6/30], Loss: -264.7492, Link Prediction Loss: 8.9251, Type Classification Loss: 273.8098, Train_Acc: 0.9936, Validate_Acc: 0.9895\n",
      "Epoch [7/30], Loss: -264.6034, Link Prediction Loss: 9.2699, Type Classification Loss: 274.0364, Train_Acc: 0.9926, Validate_Acc: 0.9859\n",
      "Epoch [8/30], Loss: -265.0341, Link Prediction Loss: 8.6428, Type Classification Loss: 273.8264, Train_Acc: 0.9937, Validate_Acc: 0.9885\n",
      "Epoch [9/30], Loss: -265.4119, Link Prediction Loss: 8.3255, Type Classification Loss: 273.8331, Train_Acc: 0.9930, Validate_Acc: 0.9874\n",
      "Epoch [10/30], Loss: -266.0423, Link Prediction Loss: 7.6658, Type Classification Loss: 273.8187, Train_Acc: 0.9935, Validate_Acc: 0.9890\n",
      "Epoch [11/30], Loss: -266.1851, Link Prediction Loss: 7.5725, Type Classification Loss: 273.8724, Train_Acc: 0.9927, Validate_Acc: 0.9869\n",
      "Epoch [12/30], Loss: -266.3214, Link Prediction Loss: 7.4010, Type Classification Loss: 273.8228, Train_Acc: 0.9925, Validate_Acc: 0.9874\n",
      "Epoch [13/30], Loss: -263.2632, Link Prediction Loss: 10.3851, Type Classification Loss: 273.7552, Train_Acc: 0.9844, Validate_Acc: 0.9809\n",
      "Epoch [14/30], Loss: -263.7697, Link Prediction Loss: 9.9857, Type Classification Loss: 273.8586, Train_Acc: 0.9930, Validate_Acc: 0.9869\n",
      "Epoch [15/30], Loss: -265.9140, Link Prediction Loss: 7.8888, Type Classification Loss: 273.9081, Train_Acc: 0.9936, Validate_Acc: 0.9885\n",
      "Epoch [16/30], Loss: -266.3967, Link Prediction Loss: 7.2346, Type Classification Loss: 273.7223, Train_Acc: 0.9935, Validate_Acc: 0.9880\n",
      "Epoch [17/30], Loss: -266.8963, Link Prediction Loss: 6.7970, Type Classification Loss: 273.8233, Train_Acc: 0.9936, Validate_Acc: 0.9880\n",
      "Epoch [18/30], Loss: -266.8685, Link Prediction Loss: 6.9202, Type Classification Loss: 273.9180, Train_Acc: 0.9925, Validate_Acc: 0.9849\n",
      "Epoch [19/30], Loss: -266.5593, Link Prediction Loss: 7.2009, Type Classification Loss: 273.8797, Train_Acc: 0.9937, Validate_Acc: 0.9890\n",
      "Epoch [20/30], Loss: -266.6459, Link Prediction Loss: 7.1795, Type Classification Loss: 273.9359, Train_Acc: 0.9932, Validate_Acc: 0.9885\n",
      "Epoch [21/30], Loss: -267.1037, Link Prediction Loss: 6.5500, Type Classification Loss: 273.7606, Train_Acc: 0.9933, Validate_Acc: 0.9869\n",
      "Epoch [22/30], Loss: -266.7671, Link Prediction Loss: 6.8985, Type Classification Loss: 273.7717, Train_Acc: 0.9923, Validate_Acc: 0.9864\n",
      "Epoch [23/30], Loss: -262.8980, Link Prediction Loss: 10.9375, Type Classification Loss: 273.9513, Train_Acc: 0.9925, Validate_Acc: 0.9874\n",
      "Epoch [24/30], Loss: -267.0814, Link Prediction Loss: 6.6731, Type Classification Loss: 273.8708, Train_Acc: 0.9939, Validate_Acc: 0.9890\n",
      "Epoch [25/30], Loss: -260.4868, Link Prediction Loss: 13.3305, Type Classification Loss: 273.9226, Train_Acc: 0.9910, Validate_Acc: 0.9849\n",
      "Epoch [26/30], Loss: -265.7615, Link Prediction Loss: 7.9243, Type Classification Loss: 273.7843, Train_Acc: 0.9936, Validate_Acc: 0.9895\n",
      "Epoch [27/30], Loss: -267.2938, Link Prediction Loss: 6.3684, Type Classification Loss: 273.7669, Train_Acc: 0.9936, Validate_Acc: 0.9895\n",
      "Epoch [28/30], Loss: -267.6520, Link Prediction Loss: 6.1714, Type Classification Loss: 273.9406, Train_Acc: 0.9937, Validate_Acc: 0.9890\n",
      "Epoch [29/30], Loss: -267.5382, Link Prediction Loss: 6.1266, Type Classification Loss: 273.7652, Train_Acc: 0.9937, Validate_Acc: 0.9895\n",
      "Epoch [30/30], Loss: -268.0114, Link Prediction Loss: 5.7439, Type Classification Loss: 273.8882, Train_Acc: 0.9937, Validate_Acc: 0.9890\n",
      "Epoch [1/30], Loss: -243.0087, Link Prediction Loss: 36.0465, Type Classification Loss: 280.7090, Train_Acc: 0.9852, Validate_Acc: 0.9819\n",
      "Epoch [2/30], Loss: -220.5099, Link Prediction Loss: 53.5540, Type Classification Loss: 274.3441, Train_Acc: 0.9612, Validate_Acc: 0.9613\n",
      "Epoch [3/30], Loss: -138.7628, Link Prediction Loss: 135.1216, Type Classification Loss: 274.1883, Train_Acc: 0.9686, Validate_Acc: 0.9669\n",
      "Epoch [4/30], Loss: -248.2459, Link Prediction Loss: 25.9317, Type Classification Loss: 274.5222, Train_Acc: 0.9834, Validate_Acc: 0.9814\n",
      "Epoch [5/30], Loss: -233.5961, Link Prediction Loss: 40.4527, Type Classification Loss: 274.3787, Train_Acc: 0.9754, Validate_Acc: 0.9719\n",
      "Epoch [6/30], Loss: -226.0541, Link Prediction Loss: 47.9963, Type Classification Loss: 274.4007, Train_Acc: 0.9701, Validate_Acc: 0.9659\n",
      "Epoch [7/30], Loss: -233.1231, Link Prediction Loss: 41.0357, Type Classification Loss: 274.5013, Train_Acc: 0.9821, Validate_Acc: 0.9759\n",
      "Epoch [8/30], Loss: 156.4469, Link Prediction Loss: 430.3981, Type Classification Loss: 274.2888, Train_Acc: 0.9408, Validate_Acc: 0.9332\n",
      "Epoch [9/30], Loss: -188.5606, Link Prediction Loss: 86.0156, Type Classification Loss: 275.0020, Train_Acc: 0.9347, Validate_Acc: 0.9352\n",
      "Epoch [10/30], Loss: -142.7043, Link Prediction Loss: 130.8642, Type Classification Loss: 273.8899, Train_Acc: 0.9419, Validate_Acc: 0.9433\n",
      "Epoch [11/30], Loss: -209.0556, Link Prediction Loss: 65.1802, Type Classification Loss: 274.6398, Train_Acc: 0.9561, Validate_Acc: 0.9573\n",
      "Epoch [12/30], Loss: -183.8110, Link Prediction Loss: 89.9253, Type Classification Loss: 274.0466, Train_Acc: 0.9674, Validate_Acc: 0.9664\n",
      "Epoch [13/30], Loss: -65.0540, Link Prediction Loss: 209.2361, Type Classification Loss: 274.7295, Train_Acc: 0.9646, Validate_Acc: 0.9618\n",
      "Epoch [14/30], Loss: -118.1032, Link Prediction Loss: 155.9612, Type Classification Loss: 274.3675, Train_Acc: 0.9239, Validate_Acc: 0.9152\n",
      "Epoch [15/30], Loss: -209.0234, Link Prediction Loss: 65.0562, Type Classification Loss: 274.4816, Train_Acc: 0.9602, Validate_Acc: 0.9593\n",
      "Epoch [16/30], Loss: -148.5514, Link Prediction Loss: 125.5529, Type Classification Loss: 274.4376, Train_Acc: 0.9463, Validate_Acc: 0.9438\n",
      "Epoch [17/30], Loss: -221.8435, Link Prediction Loss: 52.4661, Type Classification Loss: 274.7221, Train_Acc: 0.9434, Validate_Acc: 0.9393\n",
      "Epoch [18/30], Loss: -216.8457, Link Prediction Loss: 57.4453, Type Classification Loss: 274.6991, Train_Acc: 0.9654, Validate_Acc: 0.9603\n",
      "Epoch [19/30], Loss: -198.7002, Link Prediction Loss: 75.5147, Type Classification Loss: 274.5689, Train_Acc: 0.9618, Validate_Acc: 0.9583\n",
      "Epoch [20/30], Loss: -235.9606, Link Prediction Loss: 38.0456, Type Classification Loss: 274.3915, Train_Acc: 0.9639, Validate_Acc: 0.9598\n",
      "Epoch [21/30], Loss: -236.2660, Link Prediction Loss: 38.1027, Type Classification Loss: 274.7660, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [22/30], Loss: -246.1208, Link Prediction Loss: 27.7704, Type Classification Loss: 274.1985, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [23/30], Loss: -246.0334, Link Prediction Loss: 27.8514, Type Classification Loss: 274.2352, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [24/30], Loss: -246.1021, Link Prediction Loss: 27.8198, Type Classification Loss: 274.2820, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [25/30], Loss: -246.1095, Link Prediction Loss: 27.8139, Type Classification Loss: 274.2963, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [26/30], Loss: -246.1150, Link Prediction Loss: 27.8288, Type Classification Loss: 274.2668, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [27/30], Loss: -246.5112, Link Prediction Loss: 27.7472, Type Classification Loss: 274.6321, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [28/30], Loss: -246.1596, Link Prediction Loss: 27.7723, Type Classification Loss: 274.2517, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [29/30], Loss: -246.3947, Link Prediction Loss: 27.8740, Type Classification Loss: 274.7035, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [30/30], Loss: -246.2805, Link Prediction Loss: 27.8054, Type Classification Loss: 274.4350, Train_Acc: 0.9755, Validate_Acc: 0.9724\n",
      "Epoch [1/30], Loss: -247.5172, Link Prediction Loss: 27.0544, Type Classification Loss: 275.5289, Train_Acc: 0.9852, Validate_Acc: 0.9799\n",
      "Epoch [2/30], Loss: -261.3932, Link Prediction Loss: 12.4558, Type Classification Loss: 274.0253, Train_Acc: 0.9932, Validate_Acc: 0.9890\n",
      "Epoch [3/30], Loss: -262.9570, Link Prediction Loss: 10.8241, Type Classification Loss: 273.9598, Train_Acc: 0.9916, Validate_Acc: 0.9874\n",
      "Epoch [4/30], Loss: -258.1295, Link Prediction Loss: 15.7913, Type Classification Loss: 274.1048, Train_Acc: 0.9933, Validate_Acc: 0.9880\n",
      "Epoch [5/30], Loss: -263.0504, Link Prediction Loss: 10.8582, Type Classification Loss: 274.0987, Train_Acc: 0.9896, Validate_Acc: 0.9854\n",
      "Epoch [6/30], Loss: -264.8578, Link Prediction Loss: 8.9974, Type Classification Loss: 274.0153, Train_Acc: 0.9932, Validate_Acc: 0.9874\n",
      "Epoch [7/30], Loss: -261.0990, Link Prediction Loss: 12.7547, Type Classification Loss: 274.0297, Train_Acc: 0.9746, Validate_Acc: 0.9734\n",
      "Epoch [8/30], Loss: -261.4007, Link Prediction Loss: 12.3269, Type Classification Loss: 273.8807, Train_Acc: 0.9925, Validate_Acc: 0.9874\n",
      "Epoch [9/30], Loss: -264.7332, Link Prediction Loss: 9.0789, Type Classification Loss: 273.9885, Train_Acc: 0.9935, Validate_Acc: 0.9895\n",
      "Epoch [10/30], Loss: -265.5625, Link Prediction Loss: 8.3434, Type Classification Loss: 274.1026, Train_Acc: 0.9935, Validate_Acc: 0.9895\n",
      "Epoch [11/30], Loss: -266.1408, Link Prediction Loss: 7.8671, Type Classification Loss: 274.2011, Train_Acc: 0.9932, Validate_Acc: 0.9890\n",
      "Epoch [12/30], Loss: -264.9144, Link Prediction Loss: 8.7898, Type Classification Loss: 273.8643, Train_Acc: 0.9923, Validate_Acc: 0.9864\n",
      "Epoch [13/30], Loss: -265.6012, Link Prediction Loss: 8.3302, Type Classification Loss: 274.1199, Train_Acc: 0.9936, Validate_Acc: 0.9895\n",
      "Epoch [14/30], Loss: -262.6531, Link Prediction Loss: 11.1117, Type Classification Loss: 273.9023, Train_Acc: 0.9921, Validate_Acc: 0.9869\n",
      "Epoch [15/30], Loss: -249.8279, Link Prediction Loss: 23.9288, Type Classification Loss: 273.9141, Train_Acc: 0.9920, Validate_Acc: 0.9854\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 289\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal model validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 255\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    252\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m individual \u001b[38;5;129;01min\u001b[39;00m population:\n\u001b[1;32m--> 255\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_mttm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     fitness_scores\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[1;32mIn[41], line 226\u001b[0m, in \u001b[0;36mtrain_and_evaluate_mttm\u001b[1;34m(X_train, y_train, types_train, X_val, y_val, types_val, input_dim, hidden_dim, output_dim, learning_rate, num_classes, num_epochs)\u001b[0m\n\u001b[0;32m    223\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    225\u001b[0m mttm \u001b[38;5;241m=\u001b[39m MTTM(input_dim, hidden_dim, output_dim, num_classes, lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m--> 226\u001b[0m \u001b[43mmttm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mttm\u001b[38;5;241m.\u001b[39mevaluate(val_loader)\n",
      "Cell \u001b[1;32mIn[41], line 87\u001b[0m, in \u001b[0;36mMTTM.train\u001b[1;34m(self, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     type_class_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m disc_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(val_loader)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink Prediction Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink_pred_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType Classification Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_class_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain_Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validate_Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 105\u001b[0m, in \u001b[0;36mMTTM.evaluate\u001b[1;34m(self, data_loader)\u001b[0m\n\u001b[0;32m    103\u001b[0m         gen_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(batch_features)\n\u001b[0;32m    104\u001b[0m         all_preds\u001b[38;5;241m.\u001b[39mextend(gen_output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mextend(batch_links\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(all_labels, all_preds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "# Define the Generative Predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()  # Add sigmoid activation for binary prediction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define the Discriminative Classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# MTTM class\n",
    "class MTTM:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_classes, lr=0.001):\n",
    "        self.generator = GenerativePredictor(input_dim, hidden_dim, output_dim)\n",
    "        self.discriminator = DiscriminativeClassifier(output_dim, hidden_dim, num_classes)\n",
    "        self.gen_optimizer = optim.Adam(self.generator.parameters(), lr=lr)\n",
    "        self.disc_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "        self.link_criterion = nn.BCELoss()  # Changed to BCELoss\n",
    "        self.type_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.generator.train()\n",
    "            self.discriminator.train()\n",
    "            total_loss = 0\n",
    "            link_pred_loss = 0\n",
    "            type_class_loss = 0\n",
    "            \n",
    "            for batch_features, batch_links, batch_types in train_loader:\n",
    "                # Generate link representations\n",
    "                gen_output = self.generator(batch_features)\n",
    "                \n",
    "                # Predict links\n",
    "                link_loss = self.link_criterion(gen_output, batch_links.unsqueeze(1).float())\n",
    "                \n",
    "                # Classify link types\n",
    "                type_pred = self.discriminator(gen_output.detach())\n",
    "                disc_loss = self.type_criterion(type_pred, batch_types)\n",
    "                \n",
    "                # Train discriminator\n",
    "                self.disc_optimizer.zero_grad()\n",
    "                disc_loss.backward()\n",
    "                self.disc_optimizer.step()\n",
    "                \n",
    "                # Train generator\n",
    "                self.gen_optimizer.zero_grad()\n",
    "                type_pred = self.discriminator(gen_output)\n",
    "                gen_loss = link_loss - self.type_criterion(type_pred, batch_types)\n",
    "                gen_loss.backward()\n",
    "                self.gen_optimizer.step()\n",
    "                \n",
    "                total_loss += gen_loss.item()\n",
    "                link_pred_loss += link_loss.item()\n",
    "                type_class_loss += disc_loss.item()\n",
    "            \n",
    "            # Validate\n",
    "            train_acc = self.evaluate(train_loader)\n",
    "            val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, '\n",
    "                  f'Link Prediction Loss: {link_pred_loss:.4f}, '\n",
    "                  f'Type Classification Loss: {type_class_loss:.4f}, '\n",
    "                  f'Train_Acc: {train_acc:.4f}, Validate_Acc: {val_acc:.4f}')\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_links, _ in data_loader:\n",
    "                gen_output = self.generator(batch_features)\n",
    "                all_preds.extend(gen_output.squeeze().numpy() > 0.5)\n",
    "                all_labels.extend(batch_links.numpy())\n",
    "        \n",
    "        return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Generate a hypothetical heterogeneous social network\n",
    "def generate_heterogeneous_social_network(num_nodes=1000, num_edge_types=3):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Generate nodes with random features\n",
    "    for i in range(num_nodes):\n",
    "        G.add_node(i, features=np.random.randn(10))  # 10 random features per node\n",
    "    \n",
    "    # Generate edges of different types\n",
    "    for _ in range(num_nodes * 5):  # Create about 5 edges per node on average\n",
    "        u, v = np.random.choice(num_nodes, 2, replace=False)\n",
    "        edge_type = np.random.randint(num_edge_types)\n",
    "        if not G.has_edge(u, v):\n",
    "            G.add_edge(u, v, type=edge_type)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Prepare data for MTTM\n",
    "def prepare_data(G):\n",
    "    edges = list(G.edges(data=True))\n",
    "    non_edges = list(nx.non_edges(G))\n",
    "    \n",
    "    # Prepare positive samples (existing edges)\n",
    "    X_pos = []\n",
    "    y_pos = []\n",
    "    types_pos = []\n",
    "    for u, v, data in edges:\n",
    "        feature_u = G.nodes[u]['features']\n",
    "        feature_v = G.nodes[v]['features']\n",
    "        X_pos.append(np.concatenate([feature_u, feature_v]))\n",
    "        y_pos.append(1)\n",
    "        types_pos.append(data['type'])\n",
    "    \n",
    "    # Prepare negative samples (non-existing edges)\n",
    "    X_neg = []\n",
    "    y_neg = []\n",
    "    types_neg = []\n",
    "    for u, v in non_edges[:len(edges)]:  # Use the same number of negative samples as positive\n",
    "        feature_u = G.nodes[u]['features']\n",
    "        feature_v = G.nodes[v]['features']\n",
    "        X_neg.append(np.concatenate([feature_u, feature_v]))\n",
    "        y_neg.append(0)\n",
    "        types_neg.append(np.random.randint(len(set(types_pos))))  # Assign random type to negative samples\n",
    "    \n",
    "    X = np.array(X_pos + X_neg)\n",
    "    y = np.array(y_pos + y_neg)\n",
    "    types = np.array(types_pos + types_neg)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, types\n",
    "\n",
    "# Genetic Algorithm for hyperparameter optimization\n",
    "class GeneticAlgorithm:\n",
    "    def __init__(self, population_size, num_generations, mutation_rate):\n",
    "        self.population_size = population_size\n",
    "        self.num_generations = num_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "\n",
    "    def initialize_population(self):\n",
    "        population = []\n",
    "        for _ in range(self.population_size):\n",
    "            individual = {\n",
    "                'hidden_dim': random.randint(32, 256),\n",
    "                'learning_rate': random.uniform(0.0001, 0.1)\n",
    "            }\n",
    "            population.append(individual)\n",
    "        return population\n",
    "\n",
    "    def crossover(self, parent1, parent2):\n",
    "        child = {}\n",
    "        for key in parent1.keys():\n",
    "            if random.random() < 0.5:\n",
    "                child[key] = parent1[key]\n",
    "            else:\n",
    "                child[key] = parent2[key]\n",
    "        return child\n",
    "\n",
    "    def mutate(self, individual):\n",
    "        if random.random() < self.mutation_rate:\n",
    "            key = random.choice(list(individual.keys()))\n",
    "            if key == 'hidden_dim':\n",
    "                individual[key] = random.randint(32, 256)\n",
    "            else:  # learning_rate\n",
    "                individual[key] = random.uniform(0.0001, 0.1)\n",
    "        return individual\n",
    "\n",
    "    def evolve(self, population, fitness_scores):\n",
    "        new_population = []\n",
    "        sorted_population = [x for _, x in sorted(zip(fitness_scores, population), key=lambda pair: pair[0], reverse=True)]\n",
    "        \n",
    "        # Elitism: keep the best two individuals\n",
    "        new_population.extend(sorted_population[:2])\n",
    "        \n",
    "        while len(new_population) < self.population_size:\n",
    "            parent1 = random.choice(sorted_population[:len(sorted_population)//2])\n",
    "            parent2 = random.choice(sorted_population[:len(sorted_population)//2])\n",
    "            child = self.crossover(parent1, parent2)\n",
    "            child = self.mutate(child)\n",
    "            new_population.append(child)\n",
    "        \n",
    "        return new_population\n",
    "\n",
    "def train_and_evaluate_mttm(X_train, y_train, types_train, X_val, y_val, types_val, input_dim, hidden_dim, output_dim, learning_rate, num_classes, num_epochs=30):\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), \n",
    "                                  torch.FloatTensor(y_train), \n",
    "                                  torch.LongTensor(types_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), \n",
    "                                torch.FloatTensor(y_val), \n",
    "                                torch.LongTensor(types_val))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    mttm = MTTM(input_dim, hidden_dim, output_dim, num_classes, lr=learning_rate)\n",
    "    mttm.train(train_loader, val_loader, num_epochs=num_epochs)\n",
    "    \n",
    "    return mttm.evaluate(val_loader)\n",
    "\n",
    "def main():\n",
    "    # Generate hypothetical social network\n",
    "    G = generate_heterogeneous_social_network()\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y, types = prepare_data(G)\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val, types_train, types_val = train_test_split(X, y, types, test_size=0.2, random_state=42)\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = 1  # Fixed output dimension for binary classification\n",
    "    num_classes = len(np.unique(types_train))\n",
    "\n",
    "    # Initialize Genetic Algorithm\n",
    "    ga = GeneticAlgorithm(population_size=10, num_generations=5, mutation_rate=0.1)\n",
    "    population = ga.initialize_population()\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_hyperparameters = None\n",
    "\n",
    "    for generation in range(ga.num_generations):\n",
    "        fitness_scores = []\n",
    "        \n",
    "        for individual in population:\n",
    "            accuracy = train_and_evaluate_mttm(\n",
    "                X_train, y_train, types_train, X_val, y_val, types_val,\n",
    "                input_dim, individual['hidden_dim'], output_dim,\n",
    "                individual['learning_rate'], num_classes\n",
    "            )\n",
    "            fitness_scores.append(accuracy)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_hyperparameters = individual\n",
    "        \n",
    "        print(f\"Generation {generation + 1}: Best accuracy = {best_accuracy}\")\n",
    "        population = ga.evolve(population, fitness_scores)\n",
    "\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(best_hyperparameters)\n",
    "    print(f\"Best validation accuracy: {best_accuracy}\")\n",
    "\n",
    "    # Train final model with best hyperparameters\n",
    "    final_mttm = MTTM(input_dim, best_hyperparameters['hidden_dim'], output_dim, num_classes, lr=best_hyperparameters['learning_rate'])\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), \n",
    "                                  torch.FloatTensor(y_train), \n",
    "                                  torch.LongTensor(types_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), \n",
    "                                torch.FloatTensor(y_val), \n",
    "                                torch.LongTensor(types_val))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    final_mttm.train(train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "    final_accuracy = final_mttm.evaluate(val_loader)\n",
    "    print(f\"Final model validation accuracy: {final_accuracy}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e92bffd1-13bc-4eef-b67a-0aee10333f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.8162, Link Prediction Loss: 0.6979, Type Classification Loss: 1.1183, Train_Acc: 0.4700, Validate_Acc: 0.5350\n",
      "Epoch [2/200], Loss: 1.7934, Link Prediction Loss: 0.6914, Type Classification Loss: 1.1020, Train_Acc: 0.5125, Validate_Acc: 0.5300\n",
      "Epoch [3/200], Loss: 1.7737, Link Prediction Loss: 0.6856, Type Classification Loss: 1.0881, Train_Acc: 0.5325, Validate_Acc: 0.5250\n",
      "Epoch [4/200], Loss: 1.7563, Link Prediction Loss: 0.6802, Type Classification Loss: 1.0761, Train_Acc: 0.5463, Validate_Acc: 0.5450\n",
      "Epoch [5/200], Loss: 1.7405, Link Prediction Loss: 0.6750, Type Classification Loss: 1.0655, Train_Acc: 0.5650, Validate_Acc: 0.5300\n",
      "Epoch [6/200], Loss: 1.7256, Link Prediction Loss: 0.6698, Type Classification Loss: 1.0558, Train_Acc: 0.5863, Validate_Acc: 0.5250\n",
      "Epoch [7/200], Loss: 1.7113, Link Prediction Loss: 0.6648, Type Classification Loss: 1.0466, Train_Acc: 0.6038, Validate_Acc: 0.5250\n",
      "Epoch [8/200], Loss: 1.6972, Link Prediction Loss: 0.6596, Type Classification Loss: 1.0376, Train_Acc: 0.6138, Validate_Acc: 0.5200\n",
      "Epoch [9/200], Loss: 1.6830, Link Prediction Loss: 0.6545, Type Classification Loss: 1.0286, Train_Acc: 0.6225, Validate_Acc: 0.5250\n",
      "Epoch [10/200], Loss: 1.6687, Link Prediction Loss: 0.6493, Type Classification Loss: 1.0194, Train_Acc: 0.6250, Validate_Acc: 0.5200\n",
      "Epoch [11/200], Loss: 1.6543, Link Prediction Loss: 0.6442, Type Classification Loss: 1.0101, Train_Acc: 0.6350, Validate_Acc: 0.5250\n",
      "Epoch [12/200], Loss: 1.6399, Link Prediction Loss: 0.6391, Type Classification Loss: 1.0008, Train_Acc: 0.6450, Validate_Acc: 0.5250\n",
      "Epoch [13/200], Loss: 1.6255, Link Prediction Loss: 0.6340, Type Classification Loss: 0.9915, Train_Acc: 0.6512, Validate_Acc: 0.5250\n",
      "Epoch [14/200], Loss: 1.6110, Link Prediction Loss: 0.6289, Type Classification Loss: 0.9821, Train_Acc: 0.6600, Validate_Acc: 0.5400\n",
      "Epoch [15/200], Loss: 1.5965, Link Prediction Loss: 0.6237, Type Classification Loss: 0.9728, Train_Acc: 0.6650, Validate_Acc: 0.5450\n",
      "Epoch [16/200], Loss: 1.5820, Link Prediction Loss: 0.6185, Type Classification Loss: 0.9635, Train_Acc: 0.6750, Validate_Acc: 0.5300\n",
      "Epoch [17/200], Loss: 1.5676, Link Prediction Loss: 0.6133, Type Classification Loss: 0.9543, Train_Acc: 0.6863, Validate_Acc: 0.5300\n",
      "Epoch [18/200], Loss: 1.5531, Link Prediction Loss: 0.6080, Type Classification Loss: 0.9451, Train_Acc: 0.6950, Validate_Acc: 0.5400\n",
      "Epoch [19/200], Loss: 1.5387, Link Prediction Loss: 0.6027, Type Classification Loss: 0.9360, Train_Acc: 0.7000, Validate_Acc: 0.5350\n",
      "Epoch [20/200], Loss: 1.5244, Link Prediction Loss: 0.5975, Type Classification Loss: 0.9269, Train_Acc: 0.7037, Validate_Acc: 0.5300\n",
      "Epoch [21/200], Loss: 1.5102, Link Prediction Loss: 0.5922, Type Classification Loss: 0.9181, Train_Acc: 0.7037, Validate_Acc: 0.5250\n",
      "Epoch [22/200], Loss: 1.4961, Link Prediction Loss: 0.5868, Type Classification Loss: 0.9093, Train_Acc: 0.7113, Validate_Acc: 0.5300\n",
      "Epoch [23/200], Loss: 1.4819, Link Prediction Loss: 0.5813, Type Classification Loss: 0.9006, Train_Acc: 0.7137, Validate_Acc: 0.5200\n",
      "Epoch [24/200], Loss: 1.4678, Link Prediction Loss: 0.5757, Type Classification Loss: 0.8920, Train_Acc: 0.7200, Validate_Acc: 0.5250\n",
      "Epoch [25/200], Loss: 1.4535, Link Prediction Loss: 0.5700, Type Classification Loss: 0.8835, Train_Acc: 0.7300, Validate_Acc: 0.5150\n",
      "Epoch [26/200], Loss: 1.4391, Link Prediction Loss: 0.5641, Type Classification Loss: 0.8750, Train_Acc: 0.7362, Validate_Acc: 0.5100\n",
      "Epoch [27/200], Loss: 1.4245, Link Prediction Loss: 0.5581, Type Classification Loss: 0.8664, Train_Acc: 0.7438, Validate_Acc: 0.5200\n",
      "Epoch [28/200], Loss: 1.4098, Link Prediction Loss: 0.5521, Type Classification Loss: 0.8577, Train_Acc: 0.7475, Validate_Acc: 0.5250\n",
      "Epoch [29/200], Loss: 1.3950, Link Prediction Loss: 0.5461, Type Classification Loss: 0.8489, Train_Acc: 0.7562, Validate_Acc: 0.5200\n",
      "Epoch [30/200], Loss: 1.3802, Link Prediction Loss: 0.5400, Type Classification Loss: 0.8401, Train_Acc: 0.7688, Validate_Acc: 0.5150\n",
      "Epoch [31/200], Loss: 1.3651, Link Prediction Loss: 0.5338, Type Classification Loss: 0.8313, Train_Acc: 0.7725, Validate_Acc: 0.5200\n",
      "Epoch [32/200], Loss: 1.3500, Link Prediction Loss: 0.5276, Type Classification Loss: 0.8224, Train_Acc: 0.7700, Validate_Acc: 0.5150\n",
      "Epoch [33/200], Loss: 1.3348, Link Prediction Loss: 0.5213, Type Classification Loss: 0.8134, Train_Acc: 0.7863, Validate_Acc: 0.5150\n",
      "Epoch [34/200], Loss: 1.3195, Link Prediction Loss: 0.5150, Type Classification Loss: 0.8044, Train_Acc: 0.7913, Validate_Acc: 0.5150\n",
      "Epoch [35/200], Loss: 1.3040, Link Prediction Loss: 0.5085, Type Classification Loss: 0.7954, Train_Acc: 0.7963, Validate_Acc: 0.5100\n",
      "Epoch [36/200], Loss: 1.2883, Link Prediction Loss: 0.5020, Type Classification Loss: 0.7864, Train_Acc: 0.7987, Validate_Acc: 0.5100\n",
      "Epoch [37/200], Loss: 1.2726, Link Prediction Loss: 0.4953, Type Classification Loss: 0.7773, Train_Acc: 0.8000, Validate_Acc: 0.5050\n",
      "Epoch [38/200], Loss: 1.2567, Link Prediction Loss: 0.4885, Type Classification Loss: 0.7681, Train_Acc: 0.8075, Validate_Acc: 0.5050\n",
      "Epoch [39/200], Loss: 1.2408, Link Prediction Loss: 0.4817, Type Classification Loss: 0.7590, Train_Acc: 0.8137, Validate_Acc: 0.5100\n",
      "Epoch [40/200], Loss: 1.2248, Link Prediction Loss: 0.4749, Type Classification Loss: 0.7499, Train_Acc: 0.8187, Validate_Acc: 0.5100\n",
      "Epoch [41/200], Loss: 1.2089, Link Prediction Loss: 0.4680, Type Classification Loss: 0.7408, Train_Acc: 0.8275, Validate_Acc: 0.5100\n",
      "Epoch [42/200], Loss: 1.1929, Link Prediction Loss: 0.4611, Type Classification Loss: 0.7318, Train_Acc: 0.8300, Validate_Acc: 0.5150\n",
      "Epoch [43/200], Loss: 1.1770, Link Prediction Loss: 0.4541, Type Classification Loss: 0.7228, Train_Acc: 0.8300, Validate_Acc: 0.5250\n",
      "Epoch [44/200], Loss: 1.1611, Link Prediction Loss: 0.4472, Type Classification Loss: 0.7140, Train_Acc: 0.8313, Validate_Acc: 0.5300\n",
      "Epoch [45/200], Loss: 1.1453, Link Prediction Loss: 0.4402, Type Classification Loss: 0.7050, Train_Acc: 0.8363, Validate_Acc: 0.5450\n",
      "Epoch [46/200], Loss: 1.1292, Link Prediction Loss: 0.4332, Type Classification Loss: 0.6960, Train_Acc: 0.8387, Validate_Acc: 0.5450\n",
      "Epoch [47/200], Loss: 1.1130, Link Prediction Loss: 0.4262, Type Classification Loss: 0.6868, Train_Acc: 0.8413, Validate_Acc: 0.5500\n",
      "Epoch [48/200], Loss: 1.0967, Link Prediction Loss: 0.4191, Type Classification Loss: 0.6776, Train_Acc: 0.8462, Validate_Acc: 0.5450\n",
      "Epoch [49/200], Loss: 1.0804, Link Prediction Loss: 0.4120, Type Classification Loss: 0.6684, Train_Acc: 0.8500, Validate_Acc: 0.5350\n",
      "Epoch [50/200], Loss: 1.0641, Link Prediction Loss: 0.4049, Type Classification Loss: 0.6592, Train_Acc: 0.8575, Validate_Acc: 0.5300\n",
      "Epoch [51/200], Loss: 1.0479, Link Prediction Loss: 0.3979, Type Classification Loss: 0.6500, Train_Acc: 0.8625, Validate_Acc: 0.5250\n",
      "Epoch [52/200], Loss: 1.0317, Link Prediction Loss: 0.3910, Type Classification Loss: 0.6407, Train_Acc: 0.8675, Validate_Acc: 0.5250\n",
      "Epoch [53/200], Loss: 1.0157, Link Prediction Loss: 0.3842, Type Classification Loss: 0.6316, Train_Acc: 0.8712, Validate_Acc: 0.5100\n",
      "Epoch [54/200], Loss: 1.0000, Link Prediction Loss: 0.3775, Type Classification Loss: 0.6225, Train_Acc: 0.8750, Validate_Acc: 0.5100\n",
      "Epoch [55/200], Loss: 0.9844, Link Prediction Loss: 0.3708, Type Classification Loss: 0.6136, Train_Acc: 0.8812, Validate_Acc: 0.5050\n",
      "Epoch [56/200], Loss: 0.9690, Link Prediction Loss: 0.3641, Type Classification Loss: 0.6048, Train_Acc: 0.8888, Validate_Acc: 0.5050\n",
      "Epoch [57/200], Loss: 0.9537, Link Prediction Loss: 0.3575, Type Classification Loss: 0.5962, Train_Acc: 0.8925, Validate_Acc: 0.5000\n",
      "Epoch [58/200], Loss: 0.9384, Link Prediction Loss: 0.3508, Type Classification Loss: 0.5876, Train_Acc: 0.8975, Validate_Acc: 0.4950\n",
      "Epoch [59/200], Loss: 0.9234, Link Prediction Loss: 0.3443, Type Classification Loss: 0.5791, Train_Acc: 0.9038, Validate_Acc: 0.4950\n",
      "Epoch [60/200], Loss: 0.9086, Link Prediction Loss: 0.3378, Type Classification Loss: 0.5708, Train_Acc: 0.9100, Validate_Acc: 0.4950\n",
      "Epoch [61/200], Loss: 0.8940, Link Prediction Loss: 0.3315, Type Classification Loss: 0.5625, Train_Acc: 0.9075, Validate_Acc: 0.5000\n",
      "Epoch [62/200], Loss: 0.8796, Link Prediction Loss: 0.3252, Type Classification Loss: 0.5543, Train_Acc: 0.9125, Validate_Acc: 0.4950\n",
      "Epoch [63/200], Loss: 0.8653, Link Prediction Loss: 0.3191, Type Classification Loss: 0.5462, Train_Acc: 0.9125, Validate_Acc: 0.4900\n",
      "Epoch [64/200], Loss: 0.8512, Link Prediction Loss: 0.3131, Type Classification Loss: 0.5381, Train_Acc: 0.9175, Validate_Acc: 0.4900\n",
      "Epoch [65/200], Loss: 0.8373, Link Prediction Loss: 0.3072, Type Classification Loss: 0.5301, Train_Acc: 0.9175, Validate_Acc: 0.4900\n",
      "Epoch [66/200], Loss: 0.8238, Link Prediction Loss: 0.3014, Type Classification Loss: 0.5224, Train_Acc: 0.9200, Validate_Acc: 0.4900\n",
      "Epoch [67/200], Loss: 0.8104, Link Prediction Loss: 0.2957, Type Classification Loss: 0.5147, Train_Acc: 0.9225, Validate_Acc: 0.4950\n",
      "Epoch [68/200], Loss: 0.7971, Link Prediction Loss: 0.2900, Type Classification Loss: 0.5072, Train_Acc: 0.9263, Validate_Acc: 0.4950\n",
      "Epoch [69/200], Loss: 0.7841, Link Prediction Loss: 0.2844, Type Classification Loss: 0.4997, Train_Acc: 0.9300, Validate_Acc: 0.4950\n",
      "Epoch [70/200], Loss: 0.7711, Link Prediction Loss: 0.2790, Type Classification Loss: 0.4922, Train_Acc: 0.9313, Validate_Acc: 0.4900\n",
      "Epoch [71/200], Loss: 0.7582, Link Prediction Loss: 0.2736, Type Classification Loss: 0.4846, Train_Acc: 0.9337, Validate_Acc: 0.4900\n",
      "Epoch [72/200], Loss: 0.7454, Link Prediction Loss: 0.2683, Type Classification Loss: 0.4771, Train_Acc: 0.9387, Validate_Acc: 0.4950\n",
      "Epoch [73/200], Loss: 0.7329, Link Prediction Loss: 0.2632, Type Classification Loss: 0.4697, Train_Acc: 0.9437, Validate_Acc: 0.4950\n",
      "Epoch [74/200], Loss: 0.7205, Link Prediction Loss: 0.2581, Type Classification Loss: 0.4625, Train_Acc: 0.9450, Validate_Acc: 0.4800\n",
      "Epoch [75/200], Loss: 0.7083, Link Prediction Loss: 0.2530, Type Classification Loss: 0.4553, Train_Acc: 0.9463, Validate_Acc: 0.4800\n",
      "Epoch [76/200], Loss: 0.6962, Link Prediction Loss: 0.2480, Type Classification Loss: 0.4482, Train_Acc: 0.9463, Validate_Acc: 0.4800\n",
      "Epoch [77/200], Loss: 0.6844, Link Prediction Loss: 0.2431, Type Classification Loss: 0.4413, Train_Acc: 0.9463, Validate_Acc: 0.4800\n",
      "Epoch [78/200], Loss: 0.6728, Link Prediction Loss: 0.2383, Type Classification Loss: 0.4345, Train_Acc: 0.9475, Validate_Acc: 0.4850\n",
      "Epoch [79/200], Loss: 0.6613, Link Prediction Loss: 0.2336, Type Classification Loss: 0.4277, Train_Acc: 0.9500, Validate_Acc: 0.4900\n",
      "Epoch [80/200], Loss: 0.6499, Link Prediction Loss: 0.2289, Type Classification Loss: 0.4210, Train_Acc: 0.9513, Validate_Acc: 0.5000\n",
      "Epoch [81/200], Loss: 0.6388, Link Prediction Loss: 0.2243, Type Classification Loss: 0.4145, Train_Acc: 0.9525, Validate_Acc: 0.4950\n",
      "Epoch [82/200], Loss: 0.6280, Link Prediction Loss: 0.2199, Type Classification Loss: 0.4081, Train_Acc: 0.9537, Validate_Acc: 0.4950\n",
      "Epoch [83/200], Loss: 0.6173, Link Prediction Loss: 0.2155, Type Classification Loss: 0.4018, Train_Acc: 0.9575, Validate_Acc: 0.5000\n",
      "Epoch [84/200], Loss: 0.6068, Link Prediction Loss: 0.2112, Type Classification Loss: 0.3956, Train_Acc: 0.9587, Validate_Acc: 0.5100\n",
      "Epoch [85/200], Loss: 0.5964, Link Prediction Loss: 0.2069, Type Classification Loss: 0.3895, Train_Acc: 0.9637, Validate_Acc: 0.5050\n",
      "Epoch [86/200], Loss: 0.5862, Link Prediction Loss: 0.2027, Type Classification Loss: 0.3835, Train_Acc: 0.9663, Validate_Acc: 0.5000\n",
      "Epoch [87/200], Loss: 0.5762, Link Prediction Loss: 0.1986, Type Classification Loss: 0.3775, Train_Acc: 0.9663, Validate_Acc: 0.5000\n",
      "Epoch [88/200], Loss: 0.5663, Link Prediction Loss: 0.1946, Type Classification Loss: 0.3717, Train_Acc: 0.9675, Validate_Acc: 0.4900\n",
      "Epoch [89/200], Loss: 0.5566, Link Prediction Loss: 0.1907, Type Classification Loss: 0.3659, Train_Acc: 0.9712, Validate_Acc: 0.4900\n",
      "Epoch [90/200], Loss: 0.5470, Link Prediction Loss: 0.1868, Type Classification Loss: 0.3601, Train_Acc: 0.9712, Validate_Acc: 0.4900\n",
      "Epoch [91/200], Loss: 0.5377, Link Prediction Loss: 0.1831, Type Classification Loss: 0.3546, Train_Acc: 0.9712, Validate_Acc: 0.4900\n",
      "Epoch [92/200], Loss: 0.5286, Link Prediction Loss: 0.1793, Type Classification Loss: 0.3492, Train_Acc: 0.9725, Validate_Acc: 0.4950\n",
      "Epoch [93/200], Loss: 0.5197, Link Prediction Loss: 0.1757, Type Classification Loss: 0.3440, Train_Acc: 0.9738, Validate_Acc: 0.4950\n",
      "Epoch [94/200], Loss: 0.5111, Link Prediction Loss: 0.1722, Type Classification Loss: 0.3388, Train_Acc: 0.9750, Validate_Acc: 0.4900\n",
      "Epoch [95/200], Loss: 0.5024, Link Prediction Loss: 0.1688, Type Classification Loss: 0.3337, Train_Acc: 0.9775, Validate_Acc: 0.4900\n",
      "Epoch [96/200], Loss: 0.4940, Link Prediction Loss: 0.1654, Type Classification Loss: 0.3285, Train_Acc: 0.9800, Validate_Acc: 0.4900\n",
      "Epoch [97/200], Loss: 0.4856, Link Prediction Loss: 0.1621, Type Classification Loss: 0.3235, Train_Acc: 0.9800, Validate_Acc: 0.4850\n",
      "Epoch [98/200], Loss: 0.4775, Link Prediction Loss: 0.1589, Type Classification Loss: 0.3186, Train_Acc: 0.9800, Validate_Acc: 0.4850\n",
      "Epoch [99/200], Loss: 0.4695, Link Prediction Loss: 0.1558, Type Classification Loss: 0.3137, Train_Acc: 0.9800, Validate_Acc: 0.4850\n",
      "Epoch [100/200], Loss: 0.4617, Link Prediction Loss: 0.1528, Type Classification Loss: 0.3090, Train_Acc: 0.9825, Validate_Acc: 0.4850\n",
      "Epoch [101/200], Loss: 0.4541, Link Prediction Loss: 0.1498, Type Classification Loss: 0.3042, Train_Acc: 0.9838, Validate_Acc: 0.4850\n",
      "Epoch [102/200], Loss: 0.4465, Link Prediction Loss: 0.1469, Type Classification Loss: 0.2996, Train_Acc: 0.9838, Validate_Acc: 0.4800\n",
      "Epoch [103/200], Loss: 0.4390, Link Prediction Loss: 0.1440, Type Classification Loss: 0.2951, Train_Acc: 0.9838, Validate_Acc: 0.4700\n",
      "Epoch [104/200], Loss: 0.4318, Link Prediction Loss: 0.1412, Type Classification Loss: 0.2907, Train_Acc: 0.9838, Validate_Acc: 0.4700\n",
      "Epoch [105/200], Loss: 0.4247, Link Prediction Loss: 0.1384, Type Classification Loss: 0.2863, Train_Acc: 0.9838, Validate_Acc: 0.4650\n",
      "Epoch [106/200], Loss: 0.4176, Link Prediction Loss: 0.1356, Type Classification Loss: 0.2820, Train_Acc: 0.9838, Validate_Acc: 0.4700\n",
      "Epoch [107/200], Loss: 0.4106, Link Prediction Loss: 0.1328, Type Classification Loss: 0.2778, Train_Acc: 0.9838, Validate_Acc: 0.4700\n",
      "Epoch [108/200], Loss: 0.4038, Link Prediction Loss: 0.1302, Type Classification Loss: 0.2736, Train_Acc: 0.9850, Validate_Acc: 0.4700\n",
      "Epoch [109/200], Loss: 0.3971, Link Prediction Loss: 0.1276, Type Classification Loss: 0.2695, Train_Acc: 0.9850, Validate_Acc: 0.4700\n",
      "Epoch [110/200], Loss: 0.3905, Link Prediction Loss: 0.1250, Type Classification Loss: 0.2655, Train_Acc: 0.9862, Validate_Acc: 0.4700\n",
      "Epoch [111/200], Loss: 0.3841, Link Prediction Loss: 0.1225, Type Classification Loss: 0.2617, Train_Acc: 0.9862, Validate_Acc: 0.4650\n",
      "Epoch [112/200], Loss: 0.3779, Link Prediction Loss: 0.1200, Type Classification Loss: 0.2579, Train_Acc: 0.9862, Validate_Acc: 0.4650\n",
      "Epoch [113/200], Loss: 0.3717, Link Prediction Loss: 0.1175, Type Classification Loss: 0.2542, Train_Acc: 0.9900, Validate_Acc: 0.4700\n",
      "Epoch [114/200], Loss: 0.3655, Link Prediction Loss: 0.1151, Type Classification Loss: 0.2504, Train_Acc: 0.9900, Validate_Acc: 0.4750\n",
      "Epoch [115/200], Loss: 0.3597, Link Prediction Loss: 0.1128, Type Classification Loss: 0.2468, Train_Acc: 0.9900, Validate_Acc: 0.4800\n",
      "Epoch [116/200], Loss: 0.3539, Link Prediction Loss: 0.1105, Type Classification Loss: 0.2434, Train_Acc: 0.9925, Validate_Acc: 0.4800\n",
      "Epoch [117/200], Loss: 0.3483, Link Prediction Loss: 0.1083, Type Classification Loss: 0.2400, Train_Acc: 0.9925, Validate_Acc: 0.4750\n",
      "Epoch [118/200], Loss: 0.3427, Link Prediction Loss: 0.1062, Type Classification Loss: 0.2366, Train_Acc: 0.9950, Validate_Acc: 0.4750\n",
      "Epoch [119/200], Loss: 0.3373, Link Prediction Loss: 0.1040, Type Classification Loss: 0.2333, Train_Acc: 0.9962, Validate_Acc: 0.4750\n",
      "Epoch [120/200], Loss: 0.3319, Link Prediction Loss: 0.1020, Type Classification Loss: 0.2300, Train_Acc: 0.9962, Validate_Acc: 0.4750\n",
      "Epoch [121/200], Loss: 0.3267, Link Prediction Loss: 0.0999, Type Classification Loss: 0.2268, Train_Acc: 0.9975, Validate_Acc: 0.4750\n",
      "Epoch [122/200], Loss: 0.3215, Link Prediction Loss: 0.0979, Type Classification Loss: 0.2236, Train_Acc: 0.9975, Validate_Acc: 0.4750\n",
      "Epoch [123/200], Loss: 0.3164, Link Prediction Loss: 0.0960, Type Classification Loss: 0.2204, Train_Acc: 0.9975, Validate_Acc: 0.4750\n",
      "Epoch [124/200], Loss: 0.3113, Link Prediction Loss: 0.0940, Type Classification Loss: 0.2173, Train_Acc: 0.9975, Validate_Acc: 0.4750\n",
      "Epoch [125/200], Loss: 0.3062, Link Prediction Loss: 0.0921, Type Classification Loss: 0.2141, Train_Acc: 0.9975, Validate_Acc: 0.4750\n",
      "Epoch [126/200], Loss: 0.3013, Link Prediction Loss: 0.0902, Type Classification Loss: 0.2111, Train_Acc: 0.9988, Validate_Acc: 0.4750\n",
      "Epoch [127/200], Loss: 0.2964, Link Prediction Loss: 0.0884, Type Classification Loss: 0.2081, Train_Acc: 0.9988, Validate_Acc: 0.4750\n",
      "Epoch [128/200], Loss: 0.2917, Link Prediction Loss: 0.0866, Type Classification Loss: 0.2051, Train_Acc: 0.9988, Validate_Acc: 0.4750\n",
      "Epoch [129/200], Loss: 0.2870, Link Prediction Loss: 0.0848, Type Classification Loss: 0.2022, Train_Acc: 0.9988, Validate_Acc: 0.4800\n",
      "Epoch [130/200], Loss: 0.2823, Link Prediction Loss: 0.0831, Type Classification Loss: 0.1992, Train_Acc: 0.9988, Validate_Acc: 0.4800\n",
      "Epoch [131/200], Loss: 0.2778, Link Prediction Loss: 0.0815, Type Classification Loss: 0.1963, Train_Acc: 0.9988, Validate_Acc: 0.4850\n",
      "Epoch [132/200], Loss: 0.2734, Link Prediction Loss: 0.0799, Type Classification Loss: 0.1935, Train_Acc: 0.9988, Validate_Acc: 0.4900\n",
      "Epoch [133/200], Loss: 0.2690, Link Prediction Loss: 0.0783, Type Classification Loss: 0.1907, Train_Acc: 0.9988, Validate_Acc: 0.4900\n",
      "Epoch [134/200], Loss: 0.2647, Link Prediction Loss: 0.0767, Type Classification Loss: 0.1880, Train_Acc: 0.9988, Validate_Acc: 0.4850\n",
      "Epoch [135/200], Loss: 0.2604, Link Prediction Loss: 0.0751, Type Classification Loss: 0.1853, Train_Acc: 0.9988, Validate_Acc: 0.4850\n",
      "Epoch [136/200], Loss: 0.2562, Link Prediction Loss: 0.0736, Type Classification Loss: 0.1827, Train_Acc: 0.9988, Validate_Acc: 0.4850\n",
      "Epoch [137/200], Loss: 0.2520, Link Prediction Loss: 0.0720, Type Classification Loss: 0.1800, Train_Acc: 0.9988, Validate_Acc: 0.4800\n",
      "Epoch [138/200], Loss: 0.2478, Link Prediction Loss: 0.0705, Type Classification Loss: 0.1774, Train_Acc: 0.9988, Validate_Acc: 0.4800\n",
      "Epoch [139/200], Loss: 0.2438, Link Prediction Loss: 0.0690, Type Classification Loss: 0.1748, Train_Acc: 0.9988, Validate_Acc: 0.4800\n",
      "Epoch [140/200], Loss: 0.2397, Link Prediction Loss: 0.0675, Type Classification Loss: 0.1722, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [141/200], Loss: 0.2358, Link Prediction Loss: 0.0661, Type Classification Loss: 0.1697, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [142/200], Loss: 0.2320, Link Prediction Loss: 0.0647, Type Classification Loss: 0.1672, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [143/200], Loss: 0.2282, Link Prediction Loss: 0.0634, Type Classification Loss: 0.1648, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [144/200], Loss: 0.2246, Link Prediction Loss: 0.0621, Type Classification Loss: 0.1626, Train_Acc: 1.0000, Validate_Acc: 0.4900\n",
      "Epoch [145/200], Loss: 0.2210, Link Prediction Loss: 0.0608, Type Classification Loss: 0.1602, Train_Acc: 1.0000, Validate_Acc: 0.4900\n",
      "Epoch [146/200], Loss: 0.2174, Link Prediction Loss: 0.0595, Type Classification Loss: 0.1579, Train_Acc: 1.0000, Validate_Acc: 0.4900\n",
      "Epoch [147/200], Loss: 0.2140, Link Prediction Loss: 0.0583, Type Classification Loss: 0.1557, Train_Acc: 1.0000, Validate_Acc: 0.4900\n",
      "Epoch [148/200], Loss: 0.2107, Link Prediction Loss: 0.0571, Type Classification Loss: 0.1535, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [149/200], Loss: 0.2073, Link Prediction Loss: 0.0560, Type Classification Loss: 0.1513, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [150/200], Loss: 0.2040, Link Prediction Loss: 0.0549, Type Classification Loss: 0.1491, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [151/200], Loss: 0.2008, Link Prediction Loss: 0.0538, Type Classification Loss: 0.1470, Train_Acc: 1.0000, Validate_Acc: 0.4850\n",
      "Epoch [152/200], Loss: 0.1977, Link Prediction Loss: 0.0528, Type Classification Loss: 0.1449, Train_Acc: 1.0000, Validate_Acc: 0.4900\n",
      "Epoch [153/200], Loss: 0.1946, Link Prediction Loss: 0.0518, Type Classification Loss: 0.1429, Train_Acc: 1.0000, Validate_Acc: 0.4900\n",
      "Epoch [154/200], Loss: 0.1916, Link Prediction Loss: 0.0508, Type Classification Loss: 0.1408, Train_Acc: 1.0000, Validate_Acc: 0.4900\n",
      "Epoch [155/200], Loss: 0.1886, Link Prediction Loss: 0.0498, Type Classification Loss: 0.1388, Train_Acc: 1.0000, Validate_Acc: 0.4950\n",
      "Epoch [156/200], Loss: 0.1857, Link Prediction Loss: 0.0489, Type Classification Loss: 0.1368, Train_Acc: 1.0000, Validate_Acc: 0.4950\n",
      "Epoch [157/200], Loss: 0.1828, Link Prediction Loss: 0.0480, Type Classification Loss: 0.1348, Train_Acc: 1.0000, Validate_Acc: 0.4950\n",
      "Epoch [158/200], Loss: 0.1800, Link Prediction Loss: 0.0471, Type Classification Loss: 0.1329, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [159/200], Loss: 0.1772, Link Prediction Loss: 0.0463, Type Classification Loss: 0.1310, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [160/200], Loss: 0.1745, Link Prediction Loss: 0.0455, Type Classification Loss: 0.1290, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [161/200], Loss: 0.1719, Link Prediction Loss: 0.0447, Type Classification Loss: 0.1272, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [162/200], Loss: 0.1693, Link Prediction Loss: 0.0439, Type Classification Loss: 0.1254, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [163/200], Loss: 0.1668, Link Prediction Loss: 0.0431, Type Classification Loss: 0.1236, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [164/200], Loss: 0.1643, Link Prediction Loss: 0.0424, Type Classification Loss: 0.1219, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [165/200], Loss: 0.1619, Link Prediction Loss: 0.0417, Type Classification Loss: 0.1202, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [166/200], Loss: 0.1595, Link Prediction Loss: 0.0410, Type Classification Loss: 0.1185, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [167/200], Loss: 0.1571, Link Prediction Loss: 0.0403, Type Classification Loss: 0.1168, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [168/200], Loss: 0.1549, Link Prediction Loss: 0.0396, Type Classification Loss: 0.1152, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [169/200], Loss: 0.1526, Link Prediction Loss: 0.0390, Type Classification Loss: 0.1136, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [170/200], Loss: 0.1503, Link Prediction Loss: 0.0383, Type Classification Loss: 0.1120, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [171/200], Loss: 0.1481, Link Prediction Loss: 0.0377, Type Classification Loss: 0.1104, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [172/200], Loss: 0.1460, Link Prediction Loss: 0.0371, Type Classification Loss: 0.1089, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [173/200], Loss: 0.1439, Link Prediction Loss: 0.0365, Type Classification Loss: 0.1074, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [174/200], Loss: 0.1418, Link Prediction Loss: 0.0359, Type Classification Loss: 0.1059, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [175/200], Loss: 0.1397, Link Prediction Loss: 0.0353, Type Classification Loss: 0.1044, Train_Acc: 1.0000, Validate_Acc: 0.5000\n",
      "Epoch [176/200], Loss: 0.1377, Link Prediction Loss: 0.0348, Type Classification Loss: 0.1030, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [177/200], Loss: 0.1357, Link Prediction Loss: 0.0342, Type Classification Loss: 0.1015, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [178/200], Loss: 0.1338, Link Prediction Loss: 0.0337, Type Classification Loss: 0.1001, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [179/200], Loss: 0.1319, Link Prediction Loss: 0.0332, Type Classification Loss: 0.0987, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [180/200], Loss: 0.1300, Link Prediction Loss: 0.0327, Type Classification Loss: 0.0974, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [181/200], Loss: 0.1282, Link Prediction Loss: 0.0322, Type Classification Loss: 0.0961, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [182/200], Loss: 0.1264, Link Prediction Loss: 0.0317, Type Classification Loss: 0.0948, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [183/200], Loss: 0.1247, Link Prediction Loss: 0.0312, Type Classification Loss: 0.0935, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [184/200], Loss: 0.1230, Link Prediction Loss: 0.0307, Type Classification Loss: 0.0922, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [185/200], Loss: 0.1213, Link Prediction Loss: 0.0303, Type Classification Loss: 0.0910, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [186/200], Loss: 0.1196, Link Prediction Loss: 0.0298, Type Classification Loss: 0.0898, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [187/200], Loss: 0.1179, Link Prediction Loss: 0.0294, Type Classification Loss: 0.0885, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [188/200], Loss: 0.1163, Link Prediction Loss: 0.0290, Type Classification Loss: 0.0874, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [189/200], Loss: 0.1147, Link Prediction Loss: 0.0286, Type Classification Loss: 0.0862, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [190/200], Loss: 0.1131, Link Prediction Loss: 0.0282, Type Classification Loss: 0.0850, Train_Acc: 1.0000, Validate_Acc: 0.5100\n",
      "Epoch [191/200], Loss: 0.1116, Link Prediction Loss: 0.0278, Type Classification Loss: 0.0839, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [192/200], Loss: 0.1102, Link Prediction Loss: 0.0274, Type Classification Loss: 0.0828, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [193/200], Loss: 0.1087, Link Prediction Loss: 0.0270, Type Classification Loss: 0.0817, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [194/200], Loss: 0.1073, Link Prediction Loss: 0.0266, Type Classification Loss: 0.0806, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [195/200], Loss: 0.1059, Link Prediction Loss: 0.0263, Type Classification Loss: 0.0796, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [196/200], Loss: 0.1046, Link Prediction Loss: 0.0259, Type Classification Loss: 0.0786, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [197/200], Loss: 0.1032, Link Prediction Loss: 0.0256, Type Classification Loss: 0.0776, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [198/200], Loss: 0.1019, Link Prediction Loss: 0.0253, Type Classification Loss: 0.0766, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [199/200], Loss: 0.1006, Link Prediction Loss: 0.0249, Type Classification Loss: 0.0757, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Epoch [200/200], Loss: 0.0993, Link Prediction Loss: 0.0246, Type Classification Loss: 0.0747, Train_Acc: 1.0000, Validate_Acc: 0.5050\n",
      "Final Result: AUC -- 0.4932 Precision -- 0.4592 Accuracy -- 0.5050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 64     # Example size of input feature vector\n",
    "hidden_size = 13    # Hidden layer size\n",
    "link_types = 3      # Number of link types in the network\n",
    "num_epochs = 200     # Total epochs\n",
    "batch_size = 32     # Batch size\n",
    "learning_rate = 0.0061029293178949725\n",
    "\n",
    "# Synthetic data loader (replace with your data loader)\n",
    "def generate_fake_data(num_samples=1000):\n",
    "    \"\"\"Generates synthetic data: (features, labels, types).\"\"\"\n",
    "    X = np.random.randn(num_samples, input_size).astype(np.float32)\n",
    "    y = np.random.randint(0, 2, size=(num_samples,))  # Link existence (binary)\n",
    "    types = np.random.randint(0, link_types, size=(num_samples,))  # Link types\n",
    "    return torch.tensor(X), torch.tensor(y), torch.tensor(types)\n",
    "\n",
    "# Define the Generative Predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Binary prediction: missing link or not\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Define the Discriminative Classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, link_types)  # Predict link type\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "# Initialize models\n",
    "generator = GenerativePredictor().to(device)\n",
    "discriminator = DiscriminativeClassifier().to(device)\n",
    "\n",
    "# Loss functions and optimizers\n",
    "criterion_link = nn.BCELoss()  # Binary Cross Entropy for link prediction\n",
    "criterion_type = nn.CrossEntropyLoss()  # Cross Entropy for type classification\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "X_train, y_train, type_train = generate_fake_data(800)\n",
    "X_val, y_val, type_val = generate_fake_data(200)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set models to training mode\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # Forward pass: Generative Predictor\n",
    "    outputs = generator(X_train.to(device))\n",
    "    link_loss = criterion_link(outputs, y_train.float().unsqueeze(1).to(device))\n",
    "\n",
    "    # Forward pass: Discriminative Classifier\n",
    "    type_logits = discriminator(X_train.to(device))\n",
    "    type_loss = criterion_type(type_logits, type_train.long().to(device))  # Ensure type_train is Long\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = link_loss + type_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer_g.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer_g.step()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    # Calculate metrics on training set\n",
    "    train_preds = (outputs > 0.5).cpu().numpy()\n",
    "    train_acc = accuracy_score(y_train.numpy(), train_preds)\n",
    "\n",
    "    # Validation step\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = generator(X_val.to(device))\n",
    "        val_preds = (val_outputs > 0.5).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_val.numpy(), val_preds)\n",
    "\n",
    "    print('Epoch [%d/%d], Loss: %.4f, Link Prediction Loss: %.4f, Type Classification Loss: %.4f, Train_Acc: %.4f, Validate_Acc: %.4f'\n",
    "          % (epoch + 1, num_epochs, total_loss.item(), link_loss.item(), type_loss.item(), train_acc, val_acc))\n",
    "\n",
    "# Evaluation metrics on the final validation set\n",
    "val_outputs = generator(X_val.to(device)).cpu().detach().numpy()\n",
    "val_preds = (val_outputs > 0.5).astype(int)\n",
    "\n",
    "auc = roc_auc_score(y_val.numpy(), val_outputs)\n",
    "precision = precision_score(y_val.numpy(), val_preds)\n",
    "accuracy = accuracy_score(y_val.numpy(), val_preds)\n",
    "\n",
    "print(\"Final Result: AUC -- %.4f\" % (auc), \"Precision -- %.4f\" % (precision), 'Accuracy -- %.4f' % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4b5083e-8203-477b-9967-f66af4a1e918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping search: maximum iterations reached --> 5\n",
      "Optimal hidden_size: 13, Optimal learning_rate: 0.0061029293178949725\n",
      "Final Model Accuracy with optimized hyperparameters: 0.4750\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score\n",
    "from pyswarm import pso  # PSO algorithm for optimization\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters to be optimized by PSO\n",
    "input_size = 64  # Fixed input size for the dataset\n",
    "link_types = 3  # Number of link types\n",
    "num_epochs = 200  # Total epochs\n",
    "batch_size = 32  # Fixed batch size\n",
    "\n",
    "# Synthetic data loader (replace with your data loader)\n",
    "def generate_fake_data(num_samples=1000):\n",
    "    \"\"\"Generates synthetic data: (features, labels, types).\"\"\"\n",
    "    X = np.random.randn(num_samples, input_size).astype(np.float32)\n",
    "    y = np.random.randint(0, 2, size=(num_samples,))  # Link existence (binary)\n",
    "    types = np.random.randint(0, link_types, size=(num_samples,))  # Link types\n",
    "    return torch.tensor(X), torch.tensor(y), torch.tensor(types)\n",
    "\n",
    "# Define the Generative Predictor\n",
    "class GenerativePredictor(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(GenerativePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Binary prediction: missing link or not\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Define the Discriminative Classifier\n",
    "class DiscriminativeClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DiscriminativeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, link_types)  # Predict link type\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "# Function to train and evaluate the model with given hyperparameters\n",
    "def train_and_evaluate(params):\n",
    "    hidden_size, learning_rate = int(params[0]), params[1]  # Unpack parameters\n",
    "\n",
    "    # Initialize models\n",
    "    generator = GenerativePredictor(hidden_size).to(device)\n",
    "    discriminator = DiscriminativeClassifier(hidden_size).to(device)\n",
    "\n",
    "    # Loss functions and optimizers\n",
    "    criterion_link = nn.BCELoss()  # Binary Cross Entropy for link prediction\n",
    "    criterion_type = nn.CrossEntropyLoss()  # Cross Entropy for type classification\n",
    "\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Load data\n",
    "    X_train, y_train, type_train = generate_fake_data(800)\n",
    "    X_val, y_val, type_val = generate_fake_data(200)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        # Forward pass: Generative Predictor\n",
    "        outputs = generator(X_train.to(device))\n",
    "        link_loss = criterion_link(outputs, y_train.float().unsqueeze(1).to(device))\n",
    "\n",
    "        # Forward pass: Discriminative Classifier\n",
    "        type_logits = discriminator(X_train.to(device))\n",
    "        type_loss = criterion_type(type_logits, type_train.long().to(device))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = link_loss + type_loss\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer_g.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        optimizer_d.step()\n",
    "\n",
    "    # Validation step\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = generator(X_val.to(device)).cpu().detach().numpy()\n",
    "        val_preds = (val_outputs > 0.5).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        auc = roc_auc_score(y_val.numpy(), val_outputs)\n",
    "        precision = precision_score(y_val.numpy(), val_preds)\n",
    "        accuracy = accuracy_score(y_val.numpy(), val_preds)\n",
    "\n",
    "    # PSO will try to maximize this metric (return negative for minimization)\n",
    "    return -accuracy\n",
    "\n",
    "# PSO optimization for hyperparameters: hidden_size and learning_rate\n",
    "def optimize_hyperparameters():\n",
    "    # Define the search space for PSO\n",
    "    lb = [10, 0.0001]  # Lower bounds: hidden_size and learning_rate\n",
    "    ub = [100, 0.01]   # Upper bounds: hidden_size and learning_rate\n",
    "\n",
    "    # Run PSO to optimize the hyperparameters\n",
    "    optimal_params, _ = pso(train_and_evaluate, lb, ub, swarmsize=10, maxiter=5)\n",
    "\n",
    "    hidden_size_optimal = int(optimal_params[0])\n",
    "    learning_rate_optimal = optimal_params[1]\n",
    "\n",
    "    print(f\"Optimal hidden_size: {hidden_size_optimal}, Optimal learning_rate: {learning_rate_optimal}\")\n",
    "    return hidden_size_optimal, learning_rate_optimal\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Optimize hyperparameters using PSO\n",
    "    hidden_size_optimal, learning_rate_optimal = optimize_hyperparameters()\n",
    "\n",
    "    # Train the final model with optimized hyperparameters\n",
    "    final_accuracy = -train_and_evaluate([hidden_size_optimal, learning_rate_optimal])\n",
    "\n",
    "    print(f\"Final Model Accuracy with optimized hyperparameters: {final_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b30a4e6-1e5b-41c5-9070-650914e17b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type-Specific Information at Epoch 25:\n",
      "Type Labels: [3, 2, 3, 0, 3, 3, 2, 3, 1, 2, 1, 1, 0, 3, 1, 1, 3, 3, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 3, 3, 0]\n",
      "Type Labels: [2, 1, 1, 0, 1, 3, 1, 1, 3, 3, 1, 3, 1, 1, 1, 3, 1, 3, 1, 1, 1, 2, 2, 3, 1, 1, 1, 3, 1, 1, 1, 1]\n",
      "Type Labels: [1, 1, 3, 1, 1, 3, 2, 1, 1, 0, 2, 2, 3, 3, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 3, 2, 2, 3, 2, 0, 1, 0]\n",
      "Type Labels: [2, 0, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 0, 3, 1, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 3, 2]\n",
      "Type Labels: [1, 1, 1, 0, 3, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 3, 0, 2, 1, 1, 2, 3, 1, 1, 1, 1, 2, 1, 3, 2]\n",
      "Type Labels: [1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 0, 1, 3, 2, 0, 3]\n",
      "Type Labels: [3, 3, 1, 3, 2, 1, 1, 1, 1, 2, 1, 3, 2, 0, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1]\n",
      "Type Labels: [1, 1, 2, 0, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3]\n",
      "Type Labels: [3, 2, 1, 1, 1, 0, 2, 3, 1, 2, 1, 3, 2, 1, 1, 3, 1, 3, 1, 1, 2, 3, 1, 3, 3, 1, 1, 1, 2, 2, 2, 2]\n",
      "Type Labels: [0, 3, 1, 1, 1, 1, 1, 1, 0, 0, 3, 3, 1, 1, 2, 1, 3, 2, 1, 1, 1, 3, 0, 1, 2, 3, 3, 3, 1, 1, 1, 3]\n",
      "Type Labels: [1, 0, 1, 1, 3, 1, 1, 2, 1, 3, 1, 3, 0, 1, 0, 0, 3, 3, 1, 3, 3, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1]\n",
      "Type Labels: [3, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 0, 0, 2, 2, 3, 2, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 2, 3]\n",
      "Type Labels: [1, 1, 1, 3, 3, 1, 1, 2, 0, 0, 1, 1, 2, 1, 3, 3, 1, 3, 1, 0, 1, 1, 0, 3, 1, 1, 3, 1, 1, 3, 1, 2]\n",
      "Type Labels: [1, 1, 1, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "Type Labels: [1, 2, 2, 2, 2, 1, 1, 1, 2, 0, 1, 1, 3, 0, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1]\n",
      "Type Labels: [1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 2, 3, 1, 2, 1, 1, 1, 1, 2, 1, 3, 2, 0, 2, 1, 1, 1, 1, 1, 2]\n",
      "Type Labels: [1, 0, 1, 3, 1, 1, 1, 3, 1, 3, 3, 3, 3, 1, 1, 2, 3, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 0, 3, 3]\n",
      "Type Labels: [3, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1]\n",
      "Type Labels: [3, 1, 1, 1, 3, 1, 3, 3, 1, 2, 2, 1, 3, 0, 3, 0, 3, 2, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [1, 3, 1, 1, 1, 0, 1, 2, 3, 0, 2, 3, 1, 3, 1, 1, 0, 1, 1, 1, 1, 0, 3, 1, 2, 1, 0, 1, 1, 3, 2, 0]\n",
      "Type Labels: [1, 3, 1, 1, 2, 1, 0, 3, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 1, 1, 2, 2, 2, 3, 1, 3, 2, 1, 3]\n",
      "Type Labels: [1, 2, 2, 3, 1, 1, 2, 1, 3, 1, 3, 3, 1, 1, 3, 1, 0, 2, 1, 2, 2, 2, 2, 3, 1, 3, 1, 3, 1, 1, 2, 0]\n",
      "Type Labels: [3, 1, 1, 0, 1, 1, 0, 3, 3, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 2, 1, 3, 2, 2, 1, 1, 2, 2, 1, 1, 1, 3]\n",
      "Type Labels: [1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 3, 2, 1, 0, 3, 2, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 2, 1]\n",
      "Type Labels: [1, 3, 1, 1, 1, 3, 1, 0, 2, 1, 1, 3, 3, 3, 1, 1, 0, 1, 3, 1, 3, 1, 3, 1, 0, 3, 3, 1, 1, 1, 1, 2]\n",
      "Type Labels: [1, 1, 0, 0, 2, 0, 2, 3, 2, 3, 1, 0, 1, 1, 2, 3, 3, 2, 1, 2, 3, 1, 1, 2, 2, 1, 1, 3, 2, 3, 2, 0]\n",
      "Type Labels: [1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 0, 3, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1]\n",
      "Type Labels: [1, 0, 1, 1, 3, 1, 3, 3, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 3, 0, 1, 1, 2, 1, 1, 3]\n",
      "Type Labels: [1, 1, 2, 2, 2, 1, 2, 1, 2, 3, 2, 3, 3, 2, 3, 2, 0, 2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 2, 1, 1]\n",
      "Type Labels: [1, 3, 1, 0, 1, 3, 0, 1, 2, 1, 1, 1, 1, 1, 0, 2, 2, 1, 3, 1, 2, 1, 1, 1, 1, 3, 1, 2, 0, 3, 1, 1]\n",
      "Type Labels: [3, 3, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 1, 3, 1, 1, 1, 3, 1, 3, 3, 1, 1, 1, 0, 1, 1, 3, 1, 1]\n",
      "Type Labels: [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 3, 1, 0, 1, 1, 1, 2, 1, 3, 0, 3, 3, 2, 1, 3, 3, 1, 1]\n",
      "Type Labels: [1, 0, 0, 1, 2, 3, 1, 0, 1, 0, 1, 3, 0, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 2, 1, 2, 3, 1]\n",
      "Type Labels: [1, 3, 1, 1, 3, 3, 2, 1, 3, 1, 1, 1, 1, 2, 3, 3, 2, 1, 1, 0, 3, 1, 1, 3, 1, 0, 0, 1, 3, 1, 2, 1]\n",
      "Type Labels: [1, 3, 3, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 3, 3, 2, 2, 3, 3, 3, 3, 2, 1, 2, 1, 1, 3, 2]\n",
      "Type Labels: [1, 1, 0, 1, 2, 1, 3, 2, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 2, 1, 1, 2, 3, 3, 1, 2]\n",
      "Type Labels: [1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 3, 1, 3, 3, 2, 1, 3, 3, 0, 0, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1]\n",
      "Type Labels: [3, 2, 1, 3, 2, 1, 2, 1, 1, 3, 1, 1, 0, 2, 1, 3, 1, 2, 1, 3, 2, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3]\n",
      "Type Labels: [0, 2, 1, 2, 1, 1, 1, 2, 3, 3, 1, 3, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 3, 0, 1, 1, 1, 3]\n",
      "Type Labels: [1, 1, 1, 3, 1, 0, 1, 2, 1, 1, 3, 1, 1, 3, 1, 0, 1, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 0, 1, 1, 3, 1]\n",
      "Type Labels: [1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 1, 0, 2, 1, 0, 3, 1, 3, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [1, 1, 2, 1, 3, 3, 1, 1, 2, 1, 3, 1, 2, 2, 3, 1, 1, 3, 1, 0, 3, 2, 1, 2, 3, 0, 1, 1, 1, 2, 1, 1]\n",
      "Type Labels: [1, 1, 3, 1, 3, 1, 1, 0, 1, 3, 1, 1, 1, 1, 1, 2, 1, 2, 2, 3, 3, 2, 2, 1, 2, 1, 1, 1, 3, 1, 3, 1]\n",
      "Type Labels: [1, 1, 3, 3, 1, 0, 3, 1, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1, 1, 0, 3, 3, 1, 2, 1, 1, 2, 1, 2, 3]\n",
      "Type Labels: [1, 1, 1, 1, 1, 3, 3, 3, 0, 1, 1, 1, 1, 1, 2, 2, 3, 1, 1, 3, 3, 1, 2, 3, 1, 1, 2, 2, 1, 2, 1, 1]\n",
      "Type Labels: [1, 3, 1, 3, 1, 3, 3, 1, 3, 1, 2, 3, 1, 3, 1, 2, 1, 2, 1, 1, 3, 1, 1, 3, 2, 3, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [1, 0, 0, 1, 0, 1, 3, 1, 2, 3, 1, 0, 2, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 3, 2, 3, 1, 1, 1, 1, 3, 1]\n",
      "Type Labels: [1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 0, 1, 1, 1, 1, 2, 1, 1, 3, 1, 0, 1, 2, 3, 2, 2, 1, 3, 1, 1, 1, 2]\n",
      "Type Labels: [3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 0, 3, 3, 3, 3, 3, 1, 2, 3, 1, 3, 3, 1]\n",
      "Type Labels: [3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 0, 0, 0, 1, 1, 1, 1, 1, 0, 3, 2, 2, 2, 3, 1, 1, 2, 1]\n",
      "Type Labels: [1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 0, 0, 0, 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 2, 1, 1, 3, 1]\n",
      "Type Labels: [1, 1, 2, 2, 1, 2, 1, 3, 0, 0, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1]\n",
      "Type Labels: [1, 1, 1, 1, 2, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 3, 2, 1, 3, 2, 3, 1, 3, 1, 2, 1, 1, 1, 3, 2, 3, 1]\n",
      "Type Labels: [1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 3, 2, 1, 3, 1, 1, 3, 1, 1, 1, 2, 0, 1, 1, 2, 1, 3, 3, 3, 3]\n",
      "Type Labels: [1, 3, 1, 1, 0, 1, 3, 1, 3, 1, 2, 2, 0, 1, 1, 2, 2, 1, 2, 1, 3, 3, 3, 1, 1, 1, 3, 3, 0, 1, 3, 1]\n",
      "Type Labels: [1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]\n",
      "Type Labels: [1, 1, 3, 3, 3, 3, 1, 1, 1, 0, 1, 3, 3, 0, 1, 0, 3, 1, 3, 0, 1, 1, 0, 2, 1, 1, 2, 0, 1, 3, 1, 1]\n",
      "Type Labels: [1, 1, 3, 3, 2, 1, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 3, 2, 1, 1, 3, 0, 1, 1, 3, 1, 1, 1, 0, 2, 2, 2]\n",
      "Type Labels: [3, 1, 2, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 0, 1, 1, 1, 1, 2, 3, 1, 0, 2, 1, 1, 1, 3]\n",
      "Type Labels: [0, 1, 1, 1, 3, 1, 2, 1]\n",
      "Final result: AUC -- 0.9687 Precision -- 0.7609 Accuracy -- 0.9500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 16\n",
    "\n",
    "class edgeFeatures:\n",
    "    def __init__(self, label=None, type=None, embeddings=None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ', skiprows=0)\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ', skiprows=0)\n",
    "\n",
    "    train_Real_Graph = nx.Graph()\n",
    "    train_Fake_Graph = nx.Graph()\n",
    "    test_Real_Graph = nx.Graph()\n",
    "    test_Fake_Graph = nx.Graph()\n",
    "\n",
    "    real_edge_Attritube = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edge_Attritube = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    dataNewType = [9, 8, 7, 6, 5, 4] if dataset.lower() == 'facebook' else [2]\n",
    "\n",
    "    for edge in real_edge_Attritube:\n",
    "        relation = edge[2]\n",
    "        if relation in dataNewType:\n",
    "            test_Real_Graph.add_edge(edge[0], edge[1], relationship=relation)\n",
    "        else:\n",
    "            train_Real_Graph.add_edge(edge[0], edge[1], relationship=relation)\n",
    "\n",
    "    for edge in fake_edge_Attritube:\n",
    "        relation = edge[2]\n",
    "        if relation in dataNewType:\n",
    "            test_Fake_Graph.add_edge(edge[0], edge[1], relationship=relation)\n",
    "        else:\n",
    "            train_Fake_Graph.add_edge(edge[0], edge[1], relationship=relation)\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    realFileName = f'Datasets/{dataset}/realData.csv'\n",
    "    fakeFileName = f'Datasets/{dataset}/fakeData.csv'\n",
    "    node2vecReFile = f'Datasets/node2vecFeature/{dataset}Feature.txt'\n",
    "\n",
    "    train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph = structuralGraph(\n",
    "        realFileName, fakeFileName, dataset)\n",
    "\n",
    "    data = pd.read_csv(node2vecReFile, sep=' ', skiprows=1, header=None)\n",
    "    embeddings = np.array(data.iloc[:, 2:66])\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "\n",
    "    for i, (nodeL, nodeR) in enumerate(zip(data.iloc[:, 0], data.iloc[:, 1])):\n",
    "        nodel, noder = int(re.sub(\"\\D\", \"\", nodeL)), int(re.sub(\"\\D\", \"\", nodeR))\n",
    "        edgeFeature = edgeFeatures(embeddings=embeddings[i])\n",
    "\n",
    "        if train_Real_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 1, train_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            train_data.append(edgeFeature)\n",
    "        elif train_Fake_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 0, train_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_Real_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 1, test_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            test_data.append(edgeFeature)\n",
    "        elif test_Fake_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 0, test_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            test_data.append(edgeFeature)\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "    def create_dataset(data):\n",
    "        return [[torch.tensor(f.embeddings, dtype=torch.float32),\n",
    "                 torch.tensor(f.label, dtype=torch.long),\n",
    "                 torch.tensor(f.type, dtype=torch.long)] for f in data]\n",
    "\n",
    "    return (\n",
    "        DataLoader(create_dataset(train), batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(create_dataset(validate), batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(create_dataset(test_data), batch_size=batch_size, shuffle=False),\n",
    "    )\n",
    "\n",
    "def to_var(x):\n",
    "    return Variable(x.cuda() if torch.cuda.is_available() else x)\n",
    "\n",
    "class re_shape(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view(len(x), -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.view(len(grad_output), 1, -1), None\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambd):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "class adversarial_neural_networks(nn.Module):\n",
    "    def __init__(self, predicted_Type):\n",
    "        super().__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv1d(1, 1, 10, stride=1, padding=0),\n",
    "            nn.Linear(55, 32),\n",
    "        )\n",
    "        self.predictor_classifier = nn.Sequential(\n",
    "            nn.Linear(32, 24), nn.ReLU(),\n",
    "            nn.Linear(24, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 2), nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.discriminative_classifier = nn.Sequential(\n",
    "            nn.Linear(32, 16), nn.ReLU(),\n",
    "            nn.Linear(16, predicted_Type), nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        shared_embeddings = re_shape.apply(self.predictor(embeddings))\n",
    "        link_output = self.predictor_classifier(shared_embeddings)\n",
    "        reverse_embeddings = GradReverse.apply(shared_embeddings, 1.0)\n",
    "        type_output = self.discriminative_classifier(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "def evaluate_model(loader, model):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in loader:\n",
    "            outputs, _ = model(to_var(data).unsqueeze(1))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    auc = metrics.roc_auc_score(all_labels, all_preds)\n",
    "    precision = metrics.precision_score(all_labels, all_preds)\n",
    "    accuracy = metrics.accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(\"Final result: AUC -- %.4f\" % auc, \n",
    "          \"Precision -- %.4f\" % precision, \n",
    "          \"Accuracy -- %.4f\" % accuracy)\n",
    "\n",
    "def train_adversarial_neural_networks(train_loader, validate_loader, model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for train_data, train_labels, type_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            link_outputs, type_outputs = model(to_var(train_data).unsqueeze(1))\n",
    "            loss = (criterion(link_outputs, to_var(train_labels)) +\n",
    "                    criterion(type_outputs, to_var(type_labels)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch == 24:\n",
    "            print(\"\\nType-Specific Information at Epoch 25:\")\n",
    "            for _, (_, _, type_labels) in enumerate(train_loader):\n",
    "                print(f'Type Labels: {type_labels.tolist()}')\n",
    "\n",
    "def main(predicted_Type, dataset):\n",
    "    train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "    model = adversarial_neural_networks(predicted_Type)\n",
    "    train_adversarial_neural_networks(train_loader, validate_loader, model)\n",
    "    evaluate_model(test_loader, model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = 'Facebook'\n",
    "    predicted_Type = 4\n",
    "    main(predicted_Type, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c597f3c-275f-46bf-a359-09ffff1f7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms  # GA library\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "# 1. Define the search space for hyperparameters (ranges)\n",
    "HYPERPARAMETER_SPACE = {\n",
    "    \"batch_size\": [16, 32, 64, 128],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"hidden_dim\": [8, 16, 32, 64],\n",
    "    \"num_epochs\": [5]\n",
    "}\n",
    "\n",
    "# 2. Initialize your model (using your existing code)\n",
    "class adversarial_neural_networks(nn.Module):\n",
    "    def __init__(self, predicted_Type, hidden_dim):\n",
    "        super(adversarial_neural_networks, self).__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv1d(1, 1, 10, stride=1, padding=0),\n",
    "            nn.Linear(55, 32),\n",
    "        )\n",
    "        self.predictor_classifier = nn.Sequential(\n",
    "            nn.Linear(32, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.discriminative_classifier = nn.Sequential(\n",
    "            nn.Linear(32, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, predicted_Type),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = self.predictor(embeddings)\n",
    "        shared_embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        link_output = self.predictor_classifier(shared_embeddings)\n",
    "        reverse_embeddings = shared_embeddings * -1  # Gradient reversal\n",
    "        type_output = self.discriminative_classifier(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "# 3. Fitness function to maximize accuracy\n",
    "def evaluate_hyperparams(individual):\n",
    "    batch_size, lr, hidden_dim, epochs = individual\n",
    "    try:\n",
    "        # Convert hyperparameters to usable format\n",
    "        batch_size = int(batch_size)\n",
    "        learning_rate = float(lr)\n",
    "        hidden_dim = int(hidden_dim)\n",
    "        num_epochs = int(epochs)\n",
    "\n",
    "        # Initialize model and train it with given hyperparameters\n",
    "        model = adversarial_neural_networks(predicted_Type=2, hidden_dim=hidden_dim)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Load data (assuming train_loader and validate_loader are defined)\n",
    "        train_loader, validate_loader, _ = get_train_validate_test('Facebook')\n",
    "\n",
    "        # Train the model for a few epochs to get a quick performance estimate\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for train_data, train_labels, _ in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs, _ = model(train_data.unsqueeze(1))\n",
    "                loss = criterion(outputs, train_labels.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validate the model to get accuracy\n",
    "        model.eval()\n",
    "        validate_acc = []\n",
    "        with torch.no_grad():\n",
    "            for validate_data, validate_labels, _ in validate_loader:\n",
    "                outputs, _ = model(validate_data.unsqueeze(1))\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                accuracy = (predicted == validate_labels).float().mean().item()\n",
    "                validate_acc.append(accuracy)\n",
    "        \n",
    "        # Return mean validation accuracy as fitness score\n",
    "        return np.mean(validate_acc),\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return 0.0,  # Return worst score if anything fails\n",
    "\n",
    "# 4. Set up GA using DEAP\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize accuracy\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_batch_size\", random.choice, HYPERPARAMETER_SPACE[\"batch_size\"])\n",
    "toolbox.register(\"attr_learning_rate\", random.choice, HYPERPARAMETER_SPACE[\"learning_rate\"])\n",
    "toolbox.register(\"attr_hidden_dim\", random.choice, HYPERPARAMETER_SPACE[\"hidden_dim\"])\n",
    "toolbox.register(\"attr_num_epochs\", random.choice, HYPERPARAMETER_SPACE[\"num_epochs\"])\n",
    "\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_batch_size, toolbox.attr_learning_rate,\n",
    "                  toolbox.attr_hidden_dim, toolbox.attr_num_epochs), n=1)\n",
    "\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate_hyperparams)\n",
    "\n",
    "# 5. Run the GA\n",
    "def run_ga():\n",
    "    population = toolbox.population(n=10)  # Start with 10 individuals\n",
    "    generations = 5  # Run for 5 generations to limit execution time\n",
    "\n",
    "    # Use built-in DEAP algorithms to run the evolution\n",
    "    result_population = algorithms.eaSimple(population, toolbox,\n",
    "                                            cxpb=0.5, mutpb=0.2,\n",
    "                                            ngen=generations, verbose=True)\n",
    "\n",
    "    # Get the best individual from the final population\n",
    "    best_individual = tools.selBest(result_population[0], k=1)[0]\n",
    "    print(f\"Best Hyperparameters: {best_individual}\")\n",
    "    return best_individual\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_hyperparams = run_ga()\n",
    "    print(f\"Optimized Hyperparameters: Batch Size={best_hyperparams[0]}, \"\n",
    "          f\"Learning Rate={best_hyperparams[1]}, Hidden Dim={best_hyperparams[2]}, \"\n",
    "          f\"Num Epochs={best_hyperparams[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01941fdc-6510-45f1-b66c-3e44b0e2387b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 119.8992, Val Accuracy: 0.6989\n",
      "Epoch 2/200, Loss: 112.5956, Val Accuracy: 0.6989\n",
      "Epoch 3/200, Loss: 110.2756, Val Accuracy: 0.6989\n",
      "Epoch 4/200, Loss: 106.7270, Val Accuracy: 0.6989\n",
      "Epoch 5/200, Loss: 98.5492, Val Accuracy: 0.9516\n",
      "Epoch 6/200, Loss: 93.4917, Val Accuracy: 0.9642\n",
      "Epoch 7/200, Loss: 92.6824, Val Accuracy: 0.9705\n",
      "Epoch 8/200, Loss: 92.6486, Val Accuracy: 0.9726\n",
      "Epoch 9/200, Loss: 93.4535, Val Accuracy: 0.9684\n",
      "Epoch 10/200, Loss: 92.2028, Val Accuracy: 0.9663\n",
      "Epoch 11/200, Loss: 91.6648, Val Accuracy: 0.9726\n",
      "Epoch 12/200, Loss: 91.7991, Val Accuracy: 0.9705\n",
      "Epoch 13/200, Loss: 91.6376, Val Accuracy: 0.9726\n",
      "Epoch 14/200, Loss: 91.8012, Val Accuracy: 0.9726\n",
      "Epoch 15/200, Loss: 91.4553, Val Accuracy: 0.9726\n",
      "Epoch 16/200, Loss: 91.6807, Val Accuracy: 0.9705\n",
      "Epoch 17/200, Loss: 91.3795, Val Accuracy: 0.9726\n",
      "Epoch 18/200, Loss: 91.3435, Val Accuracy: 0.9579\n",
      "Epoch 19/200, Loss: 91.6393, Val Accuracy: 0.9726\n",
      "Epoch 20/200, Loss: 91.2516, Val Accuracy: 0.9747\n",
      "Epoch 21/200, Loss: 91.4317, Val Accuracy: 0.9747\n",
      "Epoch 22/200, Loss: 91.4845, Val Accuracy: 0.9726\n",
      "Epoch 23/200, Loss: 91.5844, Val Accuracy: 0.9726\n",
      "Epoch 24/200, Loss: 91.2758, Val Accuracy: 0.9684\n",
      "\n",
      "Type-Specific Information at Epoch 25:\n",
      "Type Labels: [3, 1, 3, 0, 1, 0, 2, 1, 1, 1, 3, 3, 1, 1, 1, 0, 1, 1, 1, 2, 1, 3, 2, 1, 0, 1, 1, 1, 3, 1, 1, 2]\n",
      "Type Labels: [1, 1, 2, 1, 1, 1, 2, 2, 3, 0, 1, 1, 1, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 3, 3, 1, 2, 3, 1, 1]\n",
      "Type Labels: [3, 0, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 3, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 3, 3]\n",
      "Type Labels: [1, 1, 3, 1, 1, 1, 2, 1, 3, 2, 1, 3, 3, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 3, 1, 1, 2, 3, 1, 2]\n",
      "Type Labels: [1, 1, 1, 3, 3, 2, 3, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3, 1, 2, 1, 1, 1]\n",
      "Type Labels: [0, 1, 3, 3, 1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 2, 0, 1, 3, 2, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 3]\n",
      "Type Labels: [2, 2, 1, 3, 1, 3, 1, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 3, 2, 3, 2, 1]\n",
      "Type Labels: [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 2]\n",
      "Type Labels: [1, 2, 1, 2, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3]\n",
      "Type Labels: [1, 0, 2, 0, 1, 1, 1, 0, 1, 3, 3, 1, 0, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 2, 3, 3, 3, 2, 1, 1, 1, 1]\n",
      "Type Labels: [1, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 2, 1, 3, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 3, 1, 1, 2, 2]\n",
      "Type Labels: [3, 3, 0, 1, 1, 1, 2, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2, 3, 3, 0, 0, 1, 0, 1, 1]\n",
      "Type Labels: [2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 0, 3, 3, 3, 3, 1, 1, 1, 3, 2, 1, 1, 3, 1, 0, 1, 1, 3, 0, 3, 3, 2]\n",
      "Type Labels: [1, 1, 1, 1, 1, 1, 3, 1, 1, 0, 1, 1, 3, 3, 2, 1, 1, 1, 1, 2, 1, 3, 1, 3, 1, 1, 1, 3, 2, 3, 3, 1]\n",
      "Type Labels: [1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 2, 1, 3, 1, 1, 1, 1, 3, 0, 2, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [1, 2, 1, 0, 3, 0, 2, 1, 1, 3, 3, 3, 1, 2, 3, 0, 3, 1, 2, 1, 3, 3, 3, 2, 3, 1, 1, 3, 2, 1, 1, 3]\n",
      "Type Labels: [1, 0, 2, 1, 2, 2, 3, 1, 1, 1, 3, 0, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 0, 1, 3, 1, 1, 0, 1]\n",
      "Type Labels: [1, 3, 1, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 2, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 3, 1, 1]\n",
      "Type Labels: [1, 1, 1, 1, 1, 1, 2, 0, 3, 3, 1, 1, 1, 1, 3, 1, 3, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 0, 3, 0, 1, 1, 1, 3, 2, 0, 3, 1, 2, 1, 1, 0, 1]\n",
      "Type Labels: [1, 1, 1, 3, 1, 1, 1, 2, 1, 2, 1, 1, 3, 1, 0, 1, 1, 1, 3, 1, 1, 2, 1, 3, 1, 2, 3, 1, 1, 1, 3, 1]\n",
      "Type Labels: [1, 3, 1, 1, 3, 2, 1, 1, 1, 3, 3, 3, 3, 1, 1, 0, 3, 2, 3, 1, 1, 1, 1, 2, 1, 2, 1, 3, 2, 1, 1, 3]\n",
      "Type Labels: [1, 3, 3, 3, 1, 1, 0, 1, 3, 0, 0, 1, 3, 2, 1, 1, 3, 2, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2]\n",
      "Type Labels: [1, 1, 1, 1, 1, 3, 1, 1, 2, 3, 3, 2, 1, 0, 1, 3, 1, 2, 2, 3, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [1, 0, 1, 2, 1, 0, 3, 1, 3, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 3, 1, 2, 1, 0, 1, 1, 3, 3, 1]\n",
      "Type Labels: [2, 0, 3, 1, 3, 0, 3, 1, 3, 1, 3, 2, 0, 1, 2, 3, 1, 1, 1, 1, 3, 3, 1, 3, 1, 3, 1, 0, 2, 3, 0, 2]\n",
      "Type Labels: [1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 3, 1, 0, 1, 1, 2, 3, 1, 1]\n",
      "Type Labels: [1, 1, 3, 3, 3, 3, 1, 1, 3, 1, 3, 1, 3, 1, 0, 0, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1]\n",
      "Type Labels: [2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1, 2, 2, 3, 2, 2, 1, 1, 1]\n",
      "Type Labels: [1, 0, 3, 0, 1, 2, 1, 1, 1, 3, 3, 3, 2, 1, 3, 1, 1, 1, 3, 1, 1, 2, 2, 3, 3, 3, 2, 0, 1, 0, 3, 1]\n",
      "Type Labels: [1, 1, 1, 1, 2, 1, 2, 1, 3, 1, 2, 3, 1, 3, 3, 3, 1, 1, 2, 0, 1, 3, 1, 3, 1, 1, 1, 2, 1, 3, 1, 1]\n",
      "Type Labels: [2, 3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 3, 1, 3, 2, 1, 1, 1, 3, 1, 1, 2, 1, 1, 0, 1]\n",
      "Type Labels: [1, 1, 3, 3, 1, 1, 3, 2, 2, 1, 3, 1, 0, 1, 0, 1, 3, 1, 0, 2, 3, 3, 0, 2, 1, 1, 3, 3, 1, 1, 2, 0]\n",
      "Type Labels: [1, 2, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [3, 0, 2, 0, 3, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 0, 1, 0, 1]\n",
      "Type Labels: [1, 1, 0, 3, 0, 1, 3, 1, 3, 3, 3, 2, 2, 1, 1, 1, 1, 1, 2, 1, 3, 3, 1, 2, 1, 1, 3, 1, 1, 3, 1, 2]\n",
      "Type Labels: [1, 2, 1, 2, 3, 1, 3, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 3, 1, 2, 2, 2, 1, 1, 1]\n",
      "Type Labels: [3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 0, 2, 1, 3, 3, 1, 2, 1, 1, 3, 3, 1, 1, 2, 1, 1, 1, 1]\n",
      "Type Labels: [3, 3, 3, 3, 1, 1, 1, 1, 3, 1, 3, 3, 2, 0, 1, 2, 1, 1, 3, 1, 1, 2, 1, 0, 3, 1, 1, 3, 3, 1, 0, 1]\n",
      "Type Labels: [2, 1, 1, 1, 1, 1, 1, 3, 1, 0, 2, 2, 1, 2, 3, 3, 1, 1, 1, 3, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 3, 2]\n",
      "Type Labels: [1, 1, 1, 0, 1, 3, 1, 2, 3, 1, 1, 1, 3, 3, 1, 1, 0, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1]\n",
      "Type Labels: [1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 3, 1]\n",
      "Type Labels: [1, 3, 0, 1, 1, 0, 2, 0, 1, 1, 1, 2, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 0, 3, 0, 1, 1, 3]\n",
      "Type Labels: [1, 2, 1, 2, 2, 2, 1, 3, 0, 1, 2, 1, 3, 3, 1, 3, 1, 2, 3, 3, 3, 2, 1, 2, 3, 1, 1, 3, 1, 3, 3, 2]\n",
      "Type Labels: [2, 3, 3, 0, 1, 1, 1, 2, 2, 3, 1, 1, 3, 1, 1, 2, 3, 1, 3, 3, 1, 1, 3, 1, 3, 1, 1, 3, 2, 1, 2, 1]\n",
      "Type Labels: [0, 3, 3, 3, 1, 1, 2, 3, 1, 3, 3, 0, 1, 1, 2, 3, 3, 1, 1, 2, 1, 2, 1, 3, 3, 1, 1, 0, 0, 1, 1, 1]\n",
      "Type Labels: [3, 1, 1, 1, 0, 1, 3, 3, 3, 1, 1, 1, 1, 2, 3, 1, 1, 0, 2, 1, 3, 3, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1]\n",
      "Type Labels: [2, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 0, 3, 1, 3, 2, 1, 1, 3, 3, 1, 2, 1, 3, 1, 2, 0, 3]\n",
      "Type Labels: [0, 1, 2, 3, 2, 0, 2, 1, 3, 2, 3, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2]\n",
      "Type Labels: [1, 3, 1, 0, 1, 3, 3, 1, 3, 0, 1, 1, 1, 3, 3, 1, 1, 1, 1, 0, 1, 1, 3, 2, 3, 3, 1, 1, 2, 2, 1, 1]\n",
      "Type Labels: [2, 3, 3, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 2, 1, 1]\n",
      "Type Labels: [2, 3, 0, 2, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 1, 0, 3, 1, 1, 3, 1, 2, 3, 2, 1, 3, 1, 1, 1, 1, 1, 2]\n",
      "Type Labels: [0, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 0, 1, 1, 1, 2, 3, 0, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 3, 1, 1, 1]\n",
      "Type Labels: [3, 1, 3, 1, 2, 1, 2, 1, 3, 1, 3, 1, 2, 1, 3, 1, 3, 2, 2, 0, 1, 2, 1, 1, 2, 2, 3, 1, 1, 3, 2, 1]\n",
      "Type Labels: [1, 1, 3, 2, 1, 3, 3, 3, 1, 1, 2, 1, 0, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1]\n",
      "Type Labels: [1, 1, 2, 3, 1, 2, 3, 3, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 3, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1]\n",
      "Type Labels: [1, 1, 2, 1, 1, 3, 2, 2, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1]\n",
      "Type Labels: [3, 1, 3, 1, 1, 2, 1, 1, 3, 2, 3, 0, 1, 3, 2, 0, 1, 0, 2, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Type Labels: [1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 3, 1, 2, 3, 1, 3, 3, 3, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1]\n",
      "Type Labels: [0, 1, 1, 1, 3, 2, 3, 2]\n",
      "Epoch 25/200, Loss: 91.3434, Val Accuracy: 0.9726\n",
      "Epoch 26/200, Loss: 91.3625, Val Accuracy: 0.9705\n",
      "Epoch 27/200, Loss: 91.2518, Val Accuracy: 0.9663\n",
      "Epoch 28/200, Loss: 90.9681, Val Accuracy: 0.9684\n",
      "Epoch 29/200, Loss: 91.5879, Val Accuracy: 0.9684\n",
      "Epoch 30/200, Loss: 91.1019, Val Accuracy: 0.9705\n",
      "Epoch 31/200, Loss: 91.4023, Val Accuracy: 0.9684\n",
      "Epoch 32/200, Loss: 91.3300, Val Accuracy: 0.9537\n",
      "Epoch 33/200, Loss: 91.2568, Val Accuracy: 0.9705\n",
      "Epoch 34/200, Loss: 90.9517, Val Accuracy: 0.9705\n",
      "Epoch 35/200, Loss: 91.3498, Val Accuracy: 0.9705\n",
      "Epoch 36/200, Loss: 91.0264, Val Accuracy: 0.9684\n",
      "Epoch 37/200, Loss: 91.2570, Val Accuracy: 0.9684\n",
      "Epoch 38/200, Loss: 90.9896, Val Accuracy: 0.9684\n",
      "Epoch 39/200, Loss: 91.1283, Val Accuracy: 0.9663\n",
      "Epoch 40/200, Loss: 91.1441, Val Accuracy: 0.9663\n",
      "Epoch 41/200, Loss: 90.8547, Val Accuracy: 0.9642\n",
      "Epoch 42/200, Loss: 91.2740, Val Accuracy: 0.9642\n",
      "Epoch 43/200, Loss: 91.1375, Val Accuracy: 0.9684\n",
      "Epoch 44/200, Loss: 91.0544, Val Accuracy: 0.9621\n",
      "Epoch 45/200, Loss: 91.5013, Val Accuracy: 0.9705\n",
      "Epoch 46/200, Loss: 91.1033, Val Accuracy: 0.9705\n",
      "Epoch 47/200, Loss: 91.3095, Val Accuracy: 0.9684\n",
      "Epoch 48/200, Loss: 91.3330, Val Accuracy: 0.9705\n",
      "Epoch 49/200, Loss: 91.0009, Val Accuracy: 0.9684\n",
      "Epoch 50/200, Loss: 91.2459, Val Accuracy: 0.9600\n",
      "Epoch 51/200, Loss: 91.0187, Val Accuracy: 0.9600\n",
      "Epoch 52/200, Loss: 91.4498, Val Accuracy: 0.9621\n",
      "Epoch 53/200, Loss: 91.2838, Val Accuracy: 0.9663\n",
      "Epoch 54/200, Loss: 91.1506, Val Accuracy: 0.9642\n",
      "Epoch 55/200, Loss: 91.1781, Val Accuracy: 0.9684\n",
      "Epoch 56/200, Loss: 91.0375, Val Accuracy: 0.9663\n",
      "Epoch 57/200, Loss: 91.0806, Val Accuracy: 0.9663\n",
      "Epoch 58/200, Loss: 91.1559, Val Accuracy: 0.9705\n",
      "Epoch 59/200, Loss: 91.0768, Val Accuracy: 0.9684\n",
      "Epoch 60/200, Loss: 91.0661, Val Accuracy: 0.9684\n",
      "Epoch 61/200, Loss: 90.9573, Val Accuracy: 0.9600\n",
      "Epoch 62/200, Loss: 91.0743, Val Accuracy: 0.9663\n",
      "Epoch 63/200, Loss: 91.1902, Val Accuracy: 0.9600\n",
      "Epoch 64/200, Loss: 91.0742, Val Accuracy: 0.9684\n",
      "Epoch 65/200, Loss: 90.9921, Val Accuracy: 0.9705\n",
      "Epoch 66/200, Loss: 90.8981, Val Accuracy: 0.9663\n",
      "Epoch 67/200, Loss: 90.9564, Val Accuracy: 0.9705\n",
      "Epoch 68/200, Loss: 91.1380, Val Accuracy: 0.9684\n",
      "Epoch 69/200, Loss: 90.9200, Val Accuracy: 0.9663\n",
      "Epoch 70/200, Loss: 91.4053, Val Accuracy: 0.9684\n",
      "Epoch 71/200, Loss: 91.0504, Val Accuracy: 0.9705\n",
      "Epoch 72/200, Loss: 91.2242, Val Accuracy: 0.9684\n",
      "Epoch 73/200, Loss: 90.9506, Val Accuracy: 0.9663\n",
      "Epoch 74/200, Loss: 91.1240, Val Accuracy: 0.9642\n",
      "Epoch 75/200, Loss: 90.7828, Val Accuracy: 0.9621\n",
      "Epoch 76/200, Loss: 90.9838, Val Accuracy: 0.9663\n",
      "Epoch 77/200, Loss: 90.8957, Val Accuracy: 0.9684\n",
      "Epoch 78/200, Loss: 91.0829, Val Accuracy: 0.9684\n",
      "Epoch 79/200, Loss: 90.9836, Val Accuracy: 0.9705\n",
      "Epoch 80/200, Loss: 90.9352, Val Accuracy: 0.9726\n",
      "Epoch 81/200, Loss: 90.9586, Val Accuracy: 0.9663\n",
      "Epoch 82/200, Loss: 90.9247, Val Accuracy: 0.9663\n",
      "Epoch 83/200, Loss: 90.7889, Val Accuracy: 0.9684\n",
      "Epoch 84/200, Loss: 91.0820, Val Accuracy: 0.9663\n",
      "Epoch 85/200, Loss: 90.7698, Val Accuracy: 0.9726\n",
      "Epoch 86/200, Loss: 90.2703, Val Accuracy: 0.9705\n",
      "Epoch 87/200, Loss: 90.8148, Val Accuracy: 0.9642\n",
      "Epoch 88/200, Loss: 90.5831, Val Accuracy: 0.9663\n",
      "Epoch 89/200, Loss: 90.8827, Val Accuracy: 0.9663\n",
      "Epoch 90/200, Loss: 90.7952, Val Accuracy: 0.9663\n",
      "Epoch 91/200, Loss: 91.0530, Val Accuracy: 0.9684\n",
      "Epoch 92/200, Loss: 90.7987, Val Accuracy: 0.9684\n",
      "Epoch 93/200, Loss: 90.5547, Val Accuracy: 0.9726\n",
      "Epoch 94/200, Loss: 90.8683, Val Accuracy: 0.9684\n",
      "Epoch 95/200, Loss: 90.8700, Val Accuracy: 0.9705\n",
      "Epoch 96/200, Loss: 91.0697, Val Accuracy: 0.9705\n",
      "Epoch 97/200, Loss: 90.7169, Val Accuracy: 0.9705\n",
      "Epoch 98/200, Loss: 90.7431, Val Accuracy: 0.9684\n",
      "Epoch 99/200, Loss: 90.7339, Val Accuracy: 0.9705\n",
      "Epoch 100/200, Loss: 91.0527, Val Accuracy: 0.9705\n",
      "Epoch 101/200, Loss: 90.4763, Val Accuracy: 0.9726\n",
      "Epoch 102/200, Loss: 90.8717, Val Accuracy: 0.9726\n",
      "Epoch 103/200, Loss: 90.8218, Val Accuracy: 0.9726\n",
      "Epoch 104/200, Loss: 90.8737, Val Accuracy: 0.9726\n",
      "Epoch 105/200, Loss: 90.8051, Val Accuracy: 0.9726\n",
      "Epoch 106/200, Loss: 90.7588, Val Accuracy: 0.9705\n",
      "Epoch 107/200, Loss: 90.2069, Val Accuracy: 0.9705\n",
      "Epoch 108/200, Loss: 90.7705, Val Accuracy: 0.9705\n",
      "Epoch 109/200, Loss: 90.4780, Val Accuracy: 0.9684\n",
      "Epoch 110/200, Loss: 90.3268, Val Accuracy: 0.9726\n",
      "Epoch 111/200, Loss: 90.2578, Val Accuracy: 0.9663\n",
      "Epoch 112/200, Loss: 90.8732, Val Accuracy: 0.9726\n",
      "Epoch 113/200, Loss: 91.0999, Val Accuracy: 0.9726\n",
      "Epoch 114/200, Loss: 90.6234, Val Accuracy: 0.9747\n",
      "Epoch 115/200, Loss: 90.7298, Val Accuracy: 0.9726\n",
      "Epoch 116/200, Loss: 90.6905, Val Accuracy: 0.9747\n",
      "Epoch 117/200, Loss: 90.5184, Val Accuracy: 0.9705\n",
      "Epoch 118/200, Loss: 90.8951, Val Accuracy: 0.9747\n",
      "Epoch 119/200, Loss: 91.2430, Val Accuracy: 0.9705\n",
      "Epoch 120/200, Loss: 91.2428, Val Accuracy: 0.9726\n",
      "Epoch 121/200, Loss: 91.4046, Val Accuracy: 0.9600\n",
      "Epoch 122/200, Loss: 91.1194, Val Accuracy: 0.9768\n",
      "Epoch 123/200, Loss: 90.8896, Val Accuracy: 0.9747\n",
      "Epoch 124/200, Loss: 90.9521, Val Accuracy: 0.9747\n",
      "Epoch 125/200, Loss: 90.7014, Val Accuracy: 0.9705\n",
      "Epoch 126/200, Loss: 90.8919, Val Accuracy: 0.9558\n",
      "Epoch 127/200, Loss: 90.7428, Val Accuracy: 0.9747\n",
      "Epoch 128/200, Loss: 90.3110, Val Accuracy: 0.9747\n",
      "Epoch 129/200, Loss: 89.9760, Val Accuracy: 0.9726\n",
      "Epoch 130/200, Loss: 90.4180, Val Accuracy: 0.9726\n",
      "Epoch 131/200, Loss: 90.7153, Val Accuracy: 0.9747\n",
      "Epoch 132/200, Loss: 90.8145, Val Accuracy: 0.9747\n",
      "Epoch 133/200, Loss: 90.6432, Val Accuracy: 0.9747\n",
      "Epoch 134/200, Loss: 91.0289, Val Accuracy: 0.9726\n",
      "Epoch 135/200, Loss: 90.7998, Val Accuracy: 0.9747\n",
      "Epoch 136/200, Loss: 90.5601, Val Accuracy: 0.9705\n",
      "Epoch 137/200, Loss: 90.6037, Val Accuracy: 0.9684\n",
      "Epoch 138/200, Loss: 90.7765, Val Accuracy: 0.9684\n",
      "Epoch 139/200, Loss: 90.6623, Val Accuracy: 0.9705\n",
      "Epoch 140/200, Loss: 90.7795, Val Accuracy: 0.9705\n",
      "Epoch 141/200, Loss: 90.9594, Val Accuracy: 0.9726\n",
      "Epoch 142/200, Loss: 90.7849, Val Accuracy: 0.9705\n",
      "Epoch 143/200, Loss: 90.9356, Val Accuracy: 0.9684\n",
      "Epoch 144/200, Loss: 91.0019, Val Accuracy: 0.9684\n",
      "Epoch 145/200, Loss: 90.8057, Val Accuracy: 0.9705\n",
      "Epoch 146/200, Loss: 90.7452, Val Accuracy: 0.9705\n",
      "Epoch 147/200, Loss: 90.6846, Val Accuracy: 0.9705\n",
      "Epoch 148/200, Loss: 90.7512, Val Accuracy: 0.9726\n",
      "Epoch 149/200, Loss: 91.0429, Val Accuracy: 0.9726\n",
      "Epoch 150/200, Loss: 90.7165, Val Accuracy: 0.9642\n",
      "Epoch 151/200, Loss: 90.7240, Val Accuracy: 0.9726\n",
      "Epoch 152/200, Loss: 90.7420, Val Accuracy: 0.9684\n",
      "Epoch 153/200, Loss: 89.8622, Val Accuracy: 0.9726\n",
      "Epoch 154/200, Loss: 90.2285, Val Accuracy: 0.9768\n",
      "Epoch 155/200, Loss: 90.2342, Val Accuracy: 0.9642\n",
      "Epoch 156/200, Loss: 89.8152, Val Accuracy: 0.9747\n",
      "Epoch 157/200, Loss: 90.0407, Val Accuracy: 0.9726\n",
      "Epoch 158/200, Loss: 90.3859, Val Accuracy: 0.9705\n",
      "Epoch 159/200, Loss: 90.9894, Val Accuracy: 0.9747\n",
      "Epoch 160/200, Loss: 91.1809, Val Accuracy: 0.9726\n",
      "Epoch 161/200, Loss: 90.9695, Val Accuracy: 0.9768\n",
      "Epoch 162/200, Loss: 90.7864, Val Accuracy: 0.9747\n",
      "Epoch 163/200, Loss: 90.8489, Val Accuracy: 0.9726\n",
      "Epoch 164/200, Loss: 90.7673, Val Accuracy: 0.9747\n",
      "Epoch 165/200, Loss: 90.6283, Val Accuracy: 0.9726\n",
      "Epoch 166/200, Loss: 90.6134, Val Accuracy: 0.9705\n",
      "Epoch 167/200, Loss: 90.7421, Val Accuracy: 0.9705\n",
      "Epoch 168/200, Loss: 90.6630, Val Accuracy: 0.9747\n",
      "Epoch 169/200, Loss: 90.6978, Val Accuracy: 0.9684\n",
      "Epoch 170/200, Loss: 90.5781, Val Accuracy: 0.9747\n",
      "Epoch 171/200, Loss: 90.5798, Val Accuracy: 0.9768\n",
      "Epoch 172/200, Loss: 90.8060, Val Accuracy: 0.9747\n",
      "Epoch 173/200, Loss: 90.6195, Val Accuracy: 0.9768\n",
      "Epoch 174/200, Loss: 90.6675, Val Accuracy: 0.9705\n",
      "Epoch 175/200, Loss: 90.4405, Val Accuracy: 0.9747\n",
      "Epoch 176/200, Loss: 90.0907, Val Accuracy: 0.9747\n",
      "Epoch 177/200, Loss: 90.0982, Val Accuracy: 0.9768\n",
      "Epoch 178/200, Loss: 90.5715, Val Accuracy: 0.9768\n",
      "Epoch 179/200, Loss: 90.4182, Val Accuracy: 0.9726\n",
      "Epoch 180/200, Loss: 90.7592, Val Accuracy: 0.9747\n",
      "Epoch 181/200, Loss: 91.1852, Val Accuracy: 0.9663\n",
      "Epoch 182/200, Loss: 90.7399, Val Accuracy: 0.9705\n",
      "Epoch 183/200, Loss: 90.4283, Val Accuracy: 0.9705\n",
      "Epoch 184/200, Loss: 90.4324, Val Accuracy: 0.9579\n",
      "Epoch 185/200, Loss: 90.5717, Val Accuracy: 0.9684\n",
      "Epoch 186/200, Loss: 89.8174, Val Accuracy: 0.9621\n",
      "Epoch 187/200, Loss: 90.7420, Val Accuracy: 0.9663\n",
      "Epoch 188/200, Loss: 90.5678, Val Accuracy: 0.9726\n",
      "Epoch 189/200, Loss: 90.7045, Val Accuracy: 0.9684\n",
      "Epoch 190/200, Loss: 90.5894, Val Accuracy: 0.9684\n",
      "Epoch 191/200, Loss: 90.5575, Val Accuracy: 0.9768\n",
      "Epoch 192/200, Loss: 90.3937, Val Accuracy: 0.9705\n",
      "Epoch 193/200, Loss: 90.8187, Val Accuracy: 0.9663\n",
      "Epoch 194/200, Loss: 91.0854, Val Accuracy: 0.9474\n",
      "Epoch 195/200, Loss: 91.3833, Val Accuracy: 0.9726\n",
      "Epoch 196/200, Loss: 90.9682, Val Accuracy: 0.9705\n",
      "Epoch 197/200, Loss: 90.8174, Val Accuracy: 0.9642\n",
      "Epoch 198/200, Loss: 90.7116, Val Accuracy: 0.9684\n",
      "Epoch 199/200, Loss: 90.0964, Val Accuracy: 0.9684\n",
      "Epoch 200/200, Loss: 90.1897, Val Accuracy: 0.9684\n",
      "Final result: AUC -- 0.9654, Precision -- 0.8464, Accuracy -- 0.9667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "import matplotlib.pyplot as plt  # Import for plotting\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 200  # Reduced for quicker visualization\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 16\n",
    "\n",
    "def plot_metrics(epoch_range, train_loss, val_accuracy, auc_score, precision, cm):\n",
    "    \"\"\"Generates 4 plots: Loss, Accuracy, AUC vs Precision, Confusion Matrix.\"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Training Loss over Epochs\n",
    "    axs[0, 0].plot(epoch_range, train_loss, label='Training Loss')\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].set_title('Training Loss over Epochs')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Plot 2: Validation Accuracy over Epochs\n",
    "    axs[0, 1].plot(epoch_range, val_accuracy, label='Validation Accuracy', color='green')\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('Accuracy')\n",
    "    axs[0, 1].set_title('Validation Accuracy over Epochs')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plot 3: AUC vs Precision Comparison\n",
    "    axs[1, 0].bar(['AUC', 'Precision'], [auc_score, precision], color=['blue', 'orange'])\n",
    "    axs[1, 0].set_ylim([0, 1])\n",
    "    axs[1, 0].set_title('AUC vs Precision')\n",
    "\n",
    "    # Plot 4: Confusion Matrix\n",
    "    axs[1, 1].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axs[1, 1].set_title('Confusion Matrix')\n",
    "    axs[1, 1].set_xticks(np.arange(2))\n",
    "    axs[1, 1].set_yticks(np.arange(2))\n",
    "    axs[1, 1].set_xticklabels(['Pred 0', 'Pred 1'])\n",
    "    axs[1, 1].set_yticklabels(['True 0', 'True 1'])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axs[1, 1].text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class edgeFeatures:\n",
    "    def __init__(self, label=None, type=None, embeddings=None):\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "def structuralGraph(realFileName, fakeFileName, dataset):\n",
    "    # Process and load graphs\n",
    "    dataReal = pd.read_csv(realFileName, sep=' ', skiprows=0)\n",
    "    dataFake = pd.read_csv(fakeFileName, sep=' ', skiprows=0)\n",
    "\n",
    "    train_Real_Graph, train_Fake_Graph = nx.Graph(), nx.Graph()\n",
    "    test_Real_Graph, test_Fake_Graph = nx.Graph(), nx.Graph()\n",
    "\n",
    "    real_edges = np.array(dataReal.iloc[:, 0:3])\n",
    "    fake_edges = np.array(dataFake.iloc[:, 0:3])\n",
    "\n",
    "    dataNewType = [9, 8, 7, 6, 5, 4] if dataset.lower() == 'facebook' else [2]\n",
    "\n",
    "    for edge in real_edges:\n",
    "        relation = edge[2]\n",
    "        graph = test_Real_Graph if relation in dataNewType else train_Real_Graph\n",
    "        graph.add_edge(edge[0], edge[1], relationship=relation)\n",
    "\n",
    "    for edge in fake_edges:\n",
    "        relation = edge[2]\n",
    "        graph = test_Fake_Graph if relation in dataNewType else train_Fake_Graph\n",
    "        graph.add_edge(edge[0], edge[1], relationship=relation)\n",
    "\n",
    "    return train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph\n",
    "\n",
    "def get_train_validate_test(dataset):\n",
    "    # Load dataset and generate DataLoader objects\n",
    "    realFileName = f'Datasets/{dataset}/realData.csv'\n",
    "    fakeFileName = f'Datasets/{dataset}/fakeData.csv'\n",
    "    node2vecReFile = f'Datasets/node2vecFeature/{dataset}Feature.txt'\n",
    "\n",
    "    train_Real_Graph, train_Fake_Graph, test_Real_Graph, test_Fake_Graph = structuralGraph(\n",
    "        realFileName, fakeFileName, dataset\n",
    "    )\n",
    "    data = pd.read_csv(node2vecReFile, sep=' ', skiprows=1, header=None)\n",
    "    embeddings = np.array(data.iloc[:, 2:66])\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "\n",
    "    for i, (nodeL, nodeR) in enumerate(zip(data.iloc[:, 0], data.iloc[:, 1])):\n",
    "        nodel, noder = int(re.sub(\"\\D\", \"\", nodeL)), int(re.sub(\"\\D\", \"\", nodeR))\n",
    "        edgeFeature = edgeFeatures(embeddings=embeddings[i])\n",
    "\n",
    "        if train_Real_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 1, train_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            train_data.append(edgeFeature)\n",
    "        elif train_Fake_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 0, train_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            train_data.append(edgeFeature)\n",
    "        elif test_Real_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 1, test_Real_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            test_data.append(edgeFeature)\n",
    "        elif test_Fake_Graph.has_edge(nodel, noder):\n",
    "            edgeFeature.label, edgeFeature.type = 0, test_Fake_Graph.get_edge_data(nodel, noder)['relationship']\n",
    "            test_data.append(edgeFeature)\n",
    "\n",
    "    train, validate = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "    def create_dataset(data):\n",
    "        return [\n",
    "            [torch.tensor(f.embeddings, dtype=torch.float32),\n",
    "             torch.tensor(f.label, dtype=torch.long),\n",
    "             torch.tensor(f.type, dtype=torch.long)] for f in data\n",
    "        ]\n",
    "\n",
    "    return (\n",
    "        DataLoader(create_dataset(train), batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(create_dataset(validate), batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(create_dataset(test_data), batch_size=batch_size, shuffle=False),\n",
    "    )\n",
    "\n",
    "def to_var(x):\n",
    "    return Variable(x.cuda() if torch.cuda.is_available() else x)\n",
    "\n",
    "class re_shape(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view(len(x), -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.view(len(grad_output), 1, -1), None\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambd):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * -ctx.lambd, None\n",
    "\n",
    "# Encoder Module\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, 1, 10, stride=1, padding=0)\n",
    "        self.fc = nn.Linear(55, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return re_shape.apply(self.fc(x))\n",
    "\n",
    "# Decoder Module (Link Prediction)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32, 24), nn.ReLU(),\n",
    "            nn.Linear(24, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 2), nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Discriminator Module (Relationship Type Prediction)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, predicted_Type):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32, 16), nn.ReLU(),\n",
    "            nn.Linear(16, predicted_Type), nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Complete TDAN Model\n",
    "class TDAN(nn.Module):\n",
    "    def __init__(self, predicted_Type):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.discriminator = Discriminator(predicted_Type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        link_output = self.decoder(encoded)\n",
    "        reverse_embeddings = GradReverse.apply(encoded, 1.0)\n",
    "        type_output = self.discriminator(reverse_embeddings)\n",
    "        return link_output, type_output\n",
    "\n",
    "# Evaluation and Training Functions\n",
    "def evaluate_model(loader, model):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in loader:\n",
    "            outputs, _ = model(to_var(data).unsqueeze(1))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    auc = metrics.roc_auc_score(all_labels, all_preds)\n",
    "    precision = metrics.precision_score(all_labels, all_preds)\n",
    "    accuracy = metrics.accuracy_score(all_labels, all_preds)\n",
    "    cm = metrics.confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Final result: AUC -- {auc:.4f}, Precision -- {precision:.4f}, Accuracy -- {accuracy:.4f}\")\n",
    "    return auc, precision, cm\n",
    "\n",
    "def train_tdan(train_loader, validate_loader, model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss, val_accuracy = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for train_data, train_labels, type_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            link_outputs, type_outputs = model(to_var(train_data).unsqueeze(1))\n",
    "            loss = (criterion(link_outputs, to_var(train_labels)) +\n",
    "                    criterion(type_outputs, to_var(type_labels)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for val_data, val_labels, _ in validate_loader:\n",
    "                outputs, _ = model(to_var(val_data).unsqueeze(1))\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == val_labels).sum().item()\n",
    "                total += val_labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "        if epoch == 24:\n",
    "            print(\"\\nType-Specific Information at Epoch 25:\")\n",
    "            for _, (_, _, type_labels) in enumerate(train_loader):\n",
    "                print(f'Type Labels: {type_labels.tolist()}')\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return train_loss, val_accuracy\n",
    "\n",
    "def main(predicted_Type, dataset):\n",
    "    train_loader, validate_loader, test_loader = get_train_validate_test(dataset)\n",
    "    model = TDAN(predicted_Type)\n",
    "\n",
    "    # Train the model and collect training metrics\n",
    "    train_loss, val_accuracy = train_tdan(train_loader, validate_loader, model)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    auc, precision, cm = evaluate_model(test_loader, model)\n",
    "\n",
    "    # Generate plots\n",
    "    epoch_range = list(range(1, num_epochs + 1))\n",
    "    plot_metrics(epoch_range, train_loss, val_accuracy, auc, precision, cm)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(predicted_Type=4, dataset='Facebook')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbc274-c74b-4cb4-bdfd-c2daac0ca35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
